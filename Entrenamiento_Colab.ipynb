{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8WBImasmPk2"
      },
      "source": [
        "# Proyecto final Inteligencia Computacional\n",
        "## Style Transfer\n",
        "### [Ammi Beltrán Troppa, Fernanda Borja Muñoz]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y3kDcpVmUeK"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1se1aIUKbQJw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "#\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "#\n",
        "import zipfile\n",
        "#\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from natsort import natsorted\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS6wmmwomWYZ"
      },
      "source": [
        "# Encoder y Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4XLSo7UPb2Y_"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,layer = 3):\n",
        "    super().__init__()\n",
        "    fully_model = torchvision.models.vgg16(weights = torchvision.models.vgg.VGG16_Weights.DEFAULT).eval().features\n",
        "    self.model = fully_model[:layer + 1]\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "a62291f408204382a5a902c9ae2d266b",
            "911e526ca929470bbd49cf471ae1ddc6",
            "10a27d542d13425c8a3141ccf9a65210",
            "02991449ce9b48428a3d19f550da96c9",
            "ccb0f3ea7c1c46fea228a7ccda5dba43",
            "2c3427fb9b5843ef8233cf056ba9860a",
            "a550587140d24fe39a6aa6581bdd3ea8",
            "ec1c982aa9dc49ffac2ed79d26b898e6",
            "645904cd74304435bfc797c67ac572c5",
            "e43c28f81d4c416ca318b81f20de7266",
            "c7f5b073fdee49b3abd6049ded7dc1c7"
          ]
        },
        "id": "VuHJpqFIeGhu",
        "outputId": "18fa2c31-0dbd-4230-9de9-bec10c869a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a62291f408204382a5a902c9ae2d266b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "Encode = Encoder()\n",
        "Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dMVZVCTzgoDp"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,layer = 5):\n",
        "    super().__init__()\n",
        "    fully_decoder = nn.Sequential(\n",
        "        # Primera capa\n",
        "        nn.Upsample(scale_factor=(2,2), mode='nearest'),\n",
        "        nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        \n",
        "        # Segunda Capa\n",
        "        nn.Upsample(scale_factor=(2,2), mode='nearest'),\n",
        "        nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "\n",
        "        #Tercera Capa\n",
        "        nn.Upsample(scale_factor=(2,2), mode='nearest'),\n",
        "        nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        \n",
        "\n",
        "        #Cuarta Capa\n",
        "        nn.Upsample(scale_factor=(2,2), mode='nearest'),\n",
        "        nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        \n",
        "\n",
        "        #Quinta Capa\n",
        "        nn.Upsample(scale_factor=(2,2), mode='nearest'),\n",
        "        nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        nn.ReLU(inplace = True), \n",
        "    )\n",
        "    self.model = fully_decoder[layer:29 ]\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzPXAF9PmA5U",
        "outputId": "aabc455b-8975-4b71-b1e6-f2c94ab6c91b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (model): Sequential(\n",
              "    (26): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "Decode = Decoder(layer = 26)\n",
        "Decode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guEQ4Z01mhIo"
      },
      "source": [
        "# Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrKIfPwm6CSp"
      },
      "source": [
        "## Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvQ2tIKC6CSq",
        "outputId": "c0b388e6-7778-487c-c183-b63e9a0a017a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/drive/MyDrive/val2017.zip /content/\n",
        "# !unzip -qo /content/val2017.zip -d /content/\n",
        "# !rm /content/val2017.zip"
      ],
      "metadata": {
        "id": "_TieqYR4D0Qj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/test2017.zip /content/\n",
        "!unzip -qo /content/test2017.zip -d /content/\n",
        "!rm /content/test2017.zip"
      ],
      "metadata": {
        "id": "2fLj-9VzMPMb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yQlIJTZc6CSr"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/data/test2017\"\n",
        "obj = natsorted(os.listdir(dir))\n",
        "# #Entonces puedo armar una lista con todas las direcciones\n",
        "dirArray = []\n",
        "for i in range(0, len(obj)):\n",
        "   dirArray.append(os.path.join(dir, obj[i]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dirArray)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo6vZqaFn-ZZ",
        "outputId": "d3185c18-2878-4c10-bf75-00219feff073"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40670"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mYpfEsfba4S"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMb5HvsM6CSu"
      },
      "source": [
        "Transformaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Gz6fhhyJCgQJ"
      },
      "outputs": [],
      "source": [
        "#Transformación para imagenes \n",
        "transform1 = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "            ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnVTF6Ry6CSv"
      },
      "source": [
        "Dataset Antiguo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataSet(Dataset):\n",
        "    def __init__(self, imagelist, transform, encoder):\n",
        "        super().__init__()\n",
        "        self.imagepaths = imagelist\n",
        "        self.transform = transform\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imagepaths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imagepath = self.imagepaths[index]\n",
        "        image = Image.open(imagepath).convert('RGB')\n",
        "        \n",
        "        Norm_image = self.transform(image)\n",
        "        feature = self.encoder(Norm_image.to('cuda'))\n",
        "        \n",
        "        return feature.cpu(), Norm_image.cpu()"
      ],
      "metadata": {
        "id": "RQhQmQuC86mQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data = CustomDataSet(dirArray[:40600],transform = transform1, encoder = Encode)\n",
        "Val = CustomDataSet(dirArray[40600:40670],transform = transform1, encoder = Encode)"
      ],
      "metadata": {
        "id": "E2REaiqR84M5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD3EZOBJmMlZ"
      },
      "source": [
        "# Código entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6fa-JeZYmDmY"
      },
      "outputs": [],
      "source": [
        "class DecoderLoss(nn.Module):\n",
        "  def __init__(self,weight = 1):\n",
        "    super().__init__()\n",
        "    self.weight = weight\n",
        "  \n",
        "  def forward(self, ydif, fdif): \n",
        "    reconst = torch.norm(ydif)**2\n",
        "    features = self.weight * torch.norm(fdif)**2\n",
        "    return reconst + features\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2JGsDYS8nIlf"
      },
      "outputs": [],
      "source": [
        "def show_curves(curves):\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(13, 5))\n",
        "    fig.set_facecolor('white')\n",
        "\n",
        "    epochs = np.arange(len(curves[\"val_loss\"])) + 1\n",
        "\n",
        "    ax2.plot(epochs, curves['train_loss'], label='Training',color='r')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.set_title('Loss evolution during training (Training)')\n",
        "    ax2.legend()\n",
        "\n",
        "\n",
        "    ax1.plot(epochs, curves['val_loss'], label='Validation', color='b')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Loss evolution during training (Validation)')\n",
        "    ax1.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VeP3psM2nN3_"
      },
      "outputs": [],
      "source": [
        "def train_step(x_batch, y_batch, model, optimizer, criterion, deco,use_gpu = False ):\n",
        "    # Predicción\n",
        "    y_predicted = model(x_batch)\n",
        "\n",
        "    ydif = y_predicted - y_batch\n",
        "    fdif = deco(y_predicted) - x_batch\n",
        "\n",
        "    # Cálculo de loss\n",
        "    loss = criterion(ydif, fdif)\n",
        "\n",
        "    # Actualización de parámetros\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return y_predicted, loss\n",
        "\n",
        "def evaluate(val_loader, model, criterion, deco , use_gpu = False):\n",
        "    cumulative_loss = 0\n",
        "\n",
        "    for x_val, y_val in val_loader:\n",
        "\n",
        "        if use_gpu:\n",
        "            x_val = x_val.cuda()\n",
        "            y_val = y_val.cuda()\n",
        "\n",
        "        y_predicted = model(x_val)\n",
        "        ydif = y_predicted - y_val\n",
        "        fdif = deco(y_predicted) - x_val\n",
        "\n",
        "        \n",
        "        loss = criterion(ydif, fdif)\n",
        "\n",
        "        cumulative_loss += loss.item()\n",
        "        \n",
        "\n",
        "    val_loss = cumulative_loss / len(val_loader)\n",
        "    del x_val,y_val\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    epochs,\n",
        "    criterion,\n",
        "    batch_size,\n",
        "    lr,\n",
        "    n_evaluations_per_epoch=1,\n",
        "    use_gpu=False,\n",
        "    name = 'Deco',\n",
        "    Losses = 'Loss1L',\n",
        "    deco = Encode\n",
        "):\n",
        "    deco.cpu()\n",
        "    model.cpu()\n",
        "    if use_gpu:\n",
        "        model.cuda()\n",
        "        Encode.cuda()\n",
        "\n",
        "    # Definición de dataloader\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=use_gpu)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size= int(len(val_dataset)/2), shuffle=False, pin_memory=use_gpu) \n",
        "\n",
        "    # Listas para guardar curvas de entrenamiento\n",
        "    if  os.path.exists(Losses + \".pt\"):\n",
        "        curves = torch.load(f'{Losses}.pt')\n",
        "    else:\n",
        "        curves = {\n",
        "            \"train_loss\": [],\n",
        "            \"val_loss\": [],\n",
        "        }\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    n_batches = len(train_loader)\n",
        "\n",
        "    #best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
        "        cumulative_train_loss = 0\n",
        "        train_loss_count = 0\n",
        "\n",
        "        # Optimizador\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        # Entrenamiento del modelo\n",
        "        model.train()\n",
        "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "            if use_gpu:\n",
        "                x_batch = x_batch.cuda()\n",
        "                y_batch = y_batch.cuda()\n",
        "\n",
        "            y_predicted, loss = train_step(x_batch, y_batch, model, optimizer, criterion, deco,use_gpu )\n",
        "\n",
        "            cumulative_train_loss += loss.item()\n",
        "            train_loss_count += 1\n",
        "            \n",
        "\n",
        "\n",
        "            if (i % (n_batches // n_evaluations_per_epoch) == 0) and (i > 0):\n",
        "                train_loss = cumulative_train_loss / train_loss_count\n",
        "\n",
        "                print(f\"Iteration {iteration} - Batch {i}/{len(train_loader)} - Train loss: {train_loss}\")\n",
        "\n",
        "            iteration += 1\n",
        "        del x_batch,y_batch\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = evaluate(val_loader, model, criterion,deco, use_gpu)\n",
        "            \n",
        "\n",
        "        # if val_loss < best_valid_loss:\n",
        "        #     best_valid_loss = val_loss\n",
        "        torch.save(model.state_dict(), f'{name}{epoch + 1}epc.pt')\n",
        "\n",
        "        print(f\"Val loss: {val_loss}\")\n",
        "\n",
        "        train_loss = cumulative_train_loss / train_loss_count\n",
        "\n",
        "        curves[\"train_loss\"].append(train_loss)\n",
        "        curves[\"val_loss\"].append(val_loss)\n",
        "\n",
        "        torch.save(curves, f'{Losses}.pt')\n",
        "\n",
        "    print()\n",
        "    print(f\"Tiempo total de entrenamiento: {time.perf_counter() - t0:.4f} [s]\")\n",
        "\n",
        "    #model.load_state_dict(torch.load(f'{name}.pt'))\n",
        "    return show_curves(curves) #Descomentar esto para mostrar curvas de entrenamiento "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwgbdmWUgxQ_"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TufUoTXV6CS2"
      },
      "source": [
        "### Forma nueva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k-BMcAUK6CS2",
        "outputId": "dbf3b80d-ccf3-4603-b2f9-800d5f11b973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "Iteration 426 - Batch 426/903 - Train loss: 12892515.426229509\n",
            "Iteration 427 - Batch 427/903 - Train loss: 12874260.093457945\n",
            "Iteration 428 - Batch 428/903 - Train loss: 12855788.07925408\n",
            "Iteration 429 - Batch 429/903 - Train loss: 12837449.272093024\n",
            "Iteration 430 - Batch 430/903 - Train loss: 12818834.436194895\n",
            "Iteration 431 - Batch 431/903 - Train loss: 12799775.054398147\n",
            "Iteration 432 - Batch 432/903 - Train loss: 12782943.92147806\n",
            "Iteration 433 - Batch 433/903 - Train loss: 12765327.115207374\n",
            "Iteration 434 - Batch 434/903 - Train loss: 12748005.92183908\n",
            "Iteration 435 - Batch 435/903 - Train loss: 12729874.68119266\n",
            "Iteration 436 - Batch 436/903 - Train loss: 12711998.835240275\n",
            "Iteration 437 - Batch 437/903 - Train loss: 12695140.942922374\n",
            "Iteration 438 - Batch 438/903 - Train loss: 12678330.014806379\n",
            "Iteration 439 - Batch 439/903 - Train loss: 12661315.4375\n",
            "Iteration 440 - Batch 440/903 - Train loss: 12644106.8707483\n",
            "Iteration 441 - Batch 441/903 - Train loss: 12626457.145927602\n",
            "Iteration 442 - Batch 442/903 - Train loss: 12609572.524830699\n",
            "Iteration 443 - Batch 443/903 - Train loss: 12592850.21509009\n",
            "Iteration 444 - Batch 444/903 - Train loss: 12576092.704494381\n",
            "Iteration 445 - Batch 445/903 - Train loss: 12560466.704035874\n",
            "Iteration 446 - Batch 446/903 - Train loss: 12543690.864653245\n",
            "Iteration 447 - Batch 447/903 - Train loss: 12526878.887276785\n",
            "Iteration 448 - Batch 448/903 - Train loss: 12509598.417594654\n",
            "Iteration 449 - Batch 449/903 - Train loss: 12494318.443333333\n",
            "Iteration 450 - Batch 450/903 - Train loss: 12477890.114190686\n",
            "Iteration 451 - Batch 451/903 - Train loss: 12461566.653761063\n",
            "Iteration 452 - Batch 452/903 - Train loss: 12445027.625827815\n",
            "Iteration 453 - Batch 453/903 - Train loss: 12428028.440528635\n",
            "Iteration 454 - Batch 454/903 - Train loss: 12412360.07912088\n",
            "Iteration 455 - Batch 455/903 - Train loss: 12395323.87609649\n",
            "Iteration 456 - Batch 456/903 - Train loss: 12378576.015317287\n",
            "Iteration 457 - Batch 457/903 - Train loss: 12361725.94650655\n",
            "Iteration 458 - Batch 458/903 - Train loss: 12345183.501089325\n",
            "Iteration 459 - Batch 459/903 - Train loss: 12328707.856521739\n",
            "Iteration 460 - Batch 460/903 - Train loss: 12312204.137744036\n",
            "Iteration 461 - Batch 461/903 - Train loss: 12295154.18939394\n",
            "Iteration 462 - Batch 462/903 - Train loss: 12278698.85637149\n",
            "Iteration 463 - Batch 463/903 - Train loss: 12263077.46336207\n",
            "Iteration 464 - Batch 464/903 - Train loss: 12246705.521505376\n",
            "Iteration 465 - Batch 465/903 - Train loss: 12230048.912017167\n",
            "Iteration 466 - Batch 466/903 - Train loss: 12214289.615631692\n",
            "Iteration 467 - Batch 467/903 - Train loss: 12198308.197649572\n",
            "Iteration 468 - Batch 468/903 - Train loss: 12181988.689765459\n",
            "Iteration 469 - Batch 469/903 - Train loss: 12166183.44893617\n",
            "Iteration 470 - Batch 470/903 - Train loss: 12149880.973460723\n",
            "Iteration 471 - Batch 471/903 - Train loss: 12134138.908898305\n",
            "Iteration 472 - Batch 472/903 - Train loss: 12119012.227272727\n",
            "Iteration 473 - Batch 473/903 - Train loss: 12103641.374472573\n",
            "Iteration 474 - Batch 474/903 - Train loss: 12088230.303157894\n",
            "Iteration 475 - Batch 475/903 - Train loss: 12072619.460084034\n",
            "Iteration 476 - Batch 476/903 - Train loss: 12057608.7442348\n",
            "Iteration 477 - Batch 477/903 - Train loss: 12042670.734309623\n",
            "Iteration 478 - Batch 478/903 - Train loss: 12027923.092901878\n",
            "Iteration 479 - Batch 479/903 - Train loss: 12013326.733333332\n",
            "Iteration 480 - Batch 480/903 - Train loss: 11998656.48128898\n",
            "Iteration 481 - Batch 481/903 - Train loss: 11983553.651452282\n",
            "Iteration 482 - Batch 482/903 - Train loss: 11968930.81884058\n",
            "Iteration 483 - Batch 483/903 - Train loss: 11954332.017561983\n",
            "Iteration 484 - Batch 484/903 - Train loss: 11938449.221649485\n",
            "Iteration 485 - Batch 485/903 - Train loss: 11923478.031893004\n",
            "Iteration 486 - Batch 486/903 - Train loss: 11908381.864476386\n",
            "Iteration 487 - Batch 487/903 - Train loss: 11893878.652663935\n",
            "Iteration 488 - Batch 488/903 - Train loss: 11879338.125766871\n",
            "Iteration 489 - Batch 489/903 - Train loss: 11864400.695918368\n",
            "Iteration 490 - Batch 490/903 - Train loss: 11850003.028513238\n",
            "Iteration 491 - Batch 491/903 - Train loss: 11835492.848577235\n",
            "Iteration 492 - Batch 492/903 - Train loss: 11821134.231237322\n",
            "Iteration 493 - Batch 493/903 - Train loss: 11805842.719635628\n",
            "Iteration 494 - Batch 494/903 - Train loss: 11791173.443434343\n",
            "Iteration 495 - Batch 495/903 - Train loss: 11776651.09375\n",
            "Iteration 496 - Batch 496/903 - Train loss: 11762779.152917504\n",
            "Iteration 497 - Batch 497/903 - Train loss: 11749152.769076305\n",
            "Iteration 498 - Batch 498/903 - Train loss: 11735823.33266533\n",
            "Iteration 499 - Batch 499/903 - Train loss: 11723111.984\n",
            "Iteration 500 - Batch 500/903 - Train loss: 11711187.493013972\n",
            "Iteration 501 - Batch 501/903 - Train loss: 11697973.846613545\n",
            "Iteration 502 - Batch 502/903 - Train loss: 11683891.174950298\n",
            "Iteration 503 - Batch 503/903 - Train loss: 11669816.646825397\n",
            "Iteration 504 - Batch 504/903 - Train loss: 11656251.10990099\n",
            "Iteration 505 - Batch 505/903 - Train loss: 11643555.52964427\n",
            "Iteration 506 - Batch 506/903 - Train loss: 11630366.111439843\n",
            "Iteration 507 - Batch 507/903 - Train loss: 11616257.93503937\n",
            "Iteration 508 - Batch 508/903 - Train loss: 11602020.850687623\n",
            "Iteration 509 - Batch 509/903 - Train loss: 11588702.797058824\n",
            "Iteration 510 - Batch 510/903 - Train loss: 11575376.936399218\n",
            "Iteration 511 - Batch 511/903 - Train loss: 11561323.430664062\n",
            "Iteration 512 - Batch 512/903 - Train loss: 11547873.909356724\n",
            "Iteration 513 - Batch 513/903 - Train loss: 11534710.908560311\n",
            "Iteration 514 - Batch 514/903 - Train loss: 11520467.659223301\n",
            "Iteration 515 - Batch 515/903 - Train loss: 11506917.740310077\n",
            "Iteration 516 - Batch 516/903 - Train loss: 11493533.88007737\n",
            "Iteration 517 - Batch 517/903 - Train loss: 11480883.986486487\n",
            "Iteration 518 - Batch 518/903 - Train loss: 11467998.009633912\n",
            "Iteration 519 - Batch 519/903 - Train loss: 11454654.560576923\n",
            "Iteration 520 - Batch 520/903 - Train loss: 11441810.592130518\n",
            "Iteration 521 - Batch 521/903 - Train loss: 11427673.812739464\n",
            "Iteration 522 - Batch 522/903 - Train loss: 11414596.462237094\n",
            "Iteration 523 - Batch 523/903 - Train loss: 11402127.542461833\n",
            "Iteration 524 - Batch 524/903 - Train loss: 11390507.394761905\n",
            "Iteration 525 - Batch 525/903 - Train loss: 11378397.157319391\n",
            "Iteration 526 - Batch 526/903 - Train loss: 11366149.773719165\n",
            "Iteration 527 - Batch 527/903 - Train loss: 11352841.327178031\n",
            "Iteration 528 - Batch 528/903 - Train loss: 11339676.011814745\n",
            "Iteration 529 - Batch 529/903 - Train loss: 11326775.690094339\n",
            "Iteration 530 - Batch 530/903 - Train loss: 11313389.542843692\n",
            "Iteration 531 - Batch 531/903 - Train loss: 11300518.347274436\n",
            "Iteration 532 - Batch 532/903 - Train loss: 11288523.905722326\n",
            "Iteration 533 - Batch 533/903 - Train loss: 11275694.00983146\n",
            "Iteration 534 - Batch 534/903 - Train loss: 11263848.490186917\n",
            "Iteration 535 - Batch 535/903 - Train loss: 11250725.283115672\n",
            "Iteration 536 - Batch 536/903 - Train loss: 11238007.123370577\n",
            "Iteration 537 - Batch 537/903 - Train loss: 11225140.714219332\n",
            "Iteration 538 - Batch 538/903 - Train loss: 11212493.057977736\n",
            "Iteration 539 - Batch 539/903 - Train loss: 11199796.497685185\n",
            "Iteration 540 - Batch 540/903 - Train loss: 11187598.63077634\n",
            "Iteration 541 - Batch 541/903 - Train loss: 11175260.057656826\n",
            "Iteration 542 - Batch 542/903 - Train loss: 11164188.761970535\n",
            "Iteration 543 - Batch 543/903 - Train loss: 11151071.21875\n",
            "Iteration 544 - Batch 544/903 - Train loss: 11138305.841743119\n",
            "Iteration 545 - Batch 545/903 - Train loss: 11126171.757783882\n",
            "Iteration 546 - Batch 546/903 - Train loss: 11114486.076325411\n",
            "Iteration 547 - Batch 547/903 - Train loss: 11102085.079835767\n",
            "Iteration 548 - Batch 548/903 - Train loss: 11089807.356557377\n",
            "Iteration 549 - Batch 549/903 - Train loss: 11078058.733181817\n",
            "Iteration 550 - Batch 550/903 - Train loss: 11066022.014065336\n",
            "Iteration 551 - Batch 551/903 - Train loss: 11053338.804347826\n",
            "Iteration 552 - Batch 552/903 - Train loss: 11041800.484629294\n",
            "Iteration 553 - Batch 553/903 - Train loss: 11030287.565884477\n",
            "Iteration 554 - Batch 554/903 - Train loss: 11017886.898198199\n",
            "Iteration 555 - Batch 555/903 - Train loss: 11006153.022482015\n",
            "Iteration 556 - Batch 556/903 - Train loss: 10994188.54129264\n",
            "Iteration 557 - Batch 557/903 - Train loss: 10982009.049283154\n",
            "Iteration 558 - Batch 558/903 - Train loss: 10969420.934257602\n",
            "Iteration 559 - Batch 559/903 - Train loss: 10957984.441517858\n",
            "Iteration 560 - Batch 560/903 - Train loss: 10946028.261586452\n",
            "Iteration 561 - Batch 561/903 - Train loss: 10934118.542259786\n",
            "Iteration 562 - Batch 562/903 - Train loss: 10922461.372557726\n",
            "Iteration 563 - Batch 563/903 - Train loss: 10911839.104166666\n",
            "Iteration 564 - Batch 564/903 - Train loss: 10901272.947345132\n",
            "Iteration 565 - Batch 565/903 - Train loss: 10890314.596731449\n",
            "Iteration 566 - Batch 566/903 - Train loss: 10879125.189153438\n",
            "Iteration 567 - Batch 567/903 - Train loss: 10868633.52596831\n",
            "Iteration 568 - Batch 568/903 - Train loss: 10857658.265817223\n",
            "Iteration 569 - Batch 569/903 - Train loss: 10846553.526754387\n",
            "Iteration 570 - Batch 570/903 - Train loss: 10835401.346322242\n",
            "Iteration 571 - Batch 571/903 - Train loss: 10824854.561625874\n",
            "Iteration 572 - Batch 572/903 - Train loss: 10813453.69851658\n",
            "Iteration 573 - Batch 573/903 - Train loss: 10802052.861062719\n",
            "Iteration 574 - Batch 574/903 - Train loss: 10792676.383913044\n",
            "Iteration 575 - Batch 575/903 - Train loss: 10781735.655815972\n",
            "Iteration 576 - Batch 576/903 - Train loss: 10770893.65814558\n",
            "Iteration 577 - Batch 577/903 - Train loss: 10759406.162629757\n",
            "Iteration 578 - Batch 578/903 - Train loss: 10747854.715889465\n",
            "Iteration 579 - Batch 579/903 - Train loss: 10736430.680172414\n",
            "Iteration 580 - Batch 580/903 - Train loss: 10725617.886402754\n",
            "Iteration 581 - Batch 581/903 - Train loss: 10714310.475515464\n",
            "Iteration 582 - Batch 582/903 - Train loss: 10703251.64193825\n",
            "Iteration 583 - Batch 583/903 - Train loss: 10692204.969606165\n",
            "Iteration 584 - Batch 584/903 - Train loss: 10681282.597008547\n",
            "Iteration 585 - Batch 585/903 - Train loss: 10670358.270051194\n",
            "Iteration 586 - Batch 586/903 - Train loss: 10659271.99403748\n",
            "Iteration 587 - Batch 587/903 - Train loss: 10648770.974489795\n",
            "Iteration 588 - Batch 588/903 - Train loss: 10639103.519524617\n",
            "Iteration 589 - Batch 589/903 - Train loss: 10630272.145762712\n",
            "Iteration 590 - Batch 590/903 - Train loss: 10621169.92639594\n",
            "Iteration 591 - Batch 591/903 - Train loss: 10610926.342905406\n",
            "Iteration 592 - Batch 592/903 - Train loss: 10600611.668634064\n",
            "Iteration 593 - Batch 593/903 - Train loss: 10590699.081649832\n",
            "Iteration 594 - Batch 594/903 - Train loss: 10581662.686554622\n",
            "Iteration 595 - Batch 595/903 - Train loss: 10571638.743288592\n",
            "Iteration 596 - Batch 596/903 - Train loss: 10560682.868090453\n",
            "Iteration 597 - Batch 597/903 - Train loss: 10550833.823996656\n",
            "Iteration 598 - Batch 598/903 - Train loss: 10541266.05133556\n",
            "Iteration 599 - Batch 599/903 - Train loss: 10530941.15125\n",
            "Iteration 600 - Batch 600/903 - Train loss: 10521159.701747088\n",
            "Iteration 601 - Batch 601/903 - Train loss: 10511074.273671096\n",
            "Iteration 602 - Batch 602/903 - Train loss: 10501299.906716418\n",
            "Iteration 603 - Batch 603/903 - Train loss: 10491454.3977649\n",
            "Iteration 604 - Batch 604/903 - Train loss: 10481393.937603306\n",
            "Iteration 605 - Batch 605/903 - Train loss: 10471115.264438944\n",
            "Iteration 606 - Batch 606/903 - Train loss: 10460381.29365733\n",
            "Iteration 607 - Batch 607/903 - Train loss: 10450285.488898026\n",
            "Iteration 608 - Batch 608/903 - Train loss: 10440016.013546798\n",
            "Iteration 609 - Batch 609/903 - Train loss: 10429343.638114754\n",
            "Iteration 610 - Batch 610/903 - Train loss: 10419823.1698036\n",
            "Iteration 611 - Batch 611/903 - Train loss: 10410341.009395424\n",
            "Iteration 612 - Batch 612/903 - Train loss: 10400884.385399673\n",
            "Iteration 613 - Batch 613/903 - Train loss: 10391467.186889252\n",
            "Iteration 614 - Batch 614/903 - Train loss: 10381872.812601626\n",
            "Iteration 615 - Batch 615/903 - Train loss: 10372316.394886363\n",
            "Iteration 616 - Batch 616/903 - Train loss: 10362651.345623987\n",
            "Iteration 617 - Batch 617/903 - Train loss: 10353394.664644012\n",
            "Iteration 618 - Batch 618/903 - Train loss: 10344207.274232633\n",
            "Iteration 619 - Batch 619/903 - Train loss: 10334224.931854839\n",
            "Iteration 620 - Batch 620/903 - Train loss: 10324795.250805153\n",
            "Iteration 621 - Batch 621/903 - Train loss: 10315425.632234726\n",
            "Iteration 622 - Batch 622/903 - Train loss: 10305876.388041733\n",
            "Iteration 623 - Batch 623/903 - Train loss: 10296093.89863782\n",
            "Iteration 624 - Batch 624/903 - Train loss: 10285757.6268\n",
            "Iteration 625 - Batch 625/903 - Train loss: 10276289.588258786\n",
            "Iteration 626 - Batch 626/903 - Train loss: 10266189.12200957\n",
            "Iteration 627 - Batch 627/903 - Train loss: 10256465.021496816\n",
            "Iteration 628 - Batch 628/903 - Train loss: 10246831.458664548\n",
            "Iteration 629 - Batch 629/903 - Train loss: 10238002.407142857\n",
            "Iteration 630 - Batch 630/903 - Train loss: 10228401.622424724\n",
            "Iteration 631 - Batch 631/903 - Train loss: 10218769.023338608\n",
            "Iteration 632 - Batch 632/903 - Train loss: 10209383.382701421\n",
            "Iteration 633 - Batch 633/903 - Train loss: 10200128.142350158\n",
            "Iteration 634 - Batch 634/903 - Train loss: 10191002.072047245\n",
            "Iteration 635 - Batch 635/903 - Train loss: 10181146.4956761\n",
            "Iteration 636 - Batch 636/903 - Train loss: 10171831.093799058\n",
            "Iteration 637 - Batch 637/903 - Train loss: 10162443.119122257\n",
            "Iteration 638 - Batch 638/903 - Train loss: 10153078.295383412\n",
            "Iteration 639 - Batch 639/903 - Train loss: 10143677.730859375\n",
            "Iteration 640 - Batch 640/903 - Train loss: 10134079.238299532\n",
            "Iteration 641 - Batch 641/903 - Train loss: 10124495.772975078\n",
            "Iteration 642 - Batch 642/903 - Train loss: 10114895.906687403\n",
            "Iteration 643 - Batch 643/903 - Train loss: 10106265.161490683\n",
            "Iteration 644 - Batch 644/903 - Train loss: 10097195.478294574\n",
            "Iteration 645 - Batch 645/903 - Train loss: 10087559.322755419\n",
            "Iteration 646 - Batch 646/903 - Train loss: 10077949.882148378\n",
            "Iteration 647 - Batch 647/903 - Train loss: 10068828.317515433\n",
            "Iteration 648 - Batch 648/903 - Train loss: 10059811.555084746\n",
            "Iteration 649 - Batch 649/903 - Train loss: 10050994.793461539\n",
            "Iteration 650 - Batch 650/903 - Train loss: 10041986.217741935\n",
            "Iteration 651 - Batch 651/903 - Train loss: 10032772.952453988\n",
            "Iteration 652 - Batch 652/903 - Train loss: 10023576.972052068\n",
            "Iteration 653 - Batch 653/903 - Train loss: 10014538.17469419\n",
            "Iteration 654 - Batch 654/903 - Train loss: 10005644.042748092\n",
            "Iteration 655 - Batch 655/903 - Train loss: 9996776.71417683\n",
            "Iteration 656 - Batch 656/903 - Train loss: 9986998.038432268\n",
            "Iteration 657 - Batch 657/903 - Train loss: 9977656.192629179\n",
            "Iteration 658 - Batch 658/903 - Train loss: 9969231.579286799\n",
            "Iteration 659 - Batch 659/903 - Train loss: 9960551.84810606\n",
            "Iteration 660 - Batch 660/903 - Train loss: 9952401.31505295\n",
            "Iteration 661 - Batch 661/903 - Train loss: 9943806.236782478\n",
            "Iteration 662 - Batch 662/903 - Train loss: 9934567.410633484\n",
            "Iteration 663 - Batch 663/903 - Train loss: 9925798.79631024\n",
            "Iteration 664 - Batch 664/903 - Train loss: 9916783.54924812\n",
            "Iteration 665 - Batch 665/903 - Train loss: 9907787.831456456\n",
            "Iteration 666 - Batch 666/903 - Train loss: 9898597.16904048\n",
            "Iteration 667 - Batch 667/903 - Train loss: 9890135.70770958\n",
            "Iteration 668 - Batch 668/903 - Train loss: 9881271.869207773\n",
            "Iteration 669 - Batch 669/903 - Train loss: 9872609.915298507\n",
            "Iteration 670 - Batch 670/903 - Train loss: 9864184.881147541\n",
            "Iteration 671 - Batch 671/903 - Train loss: 9855525.188244049\n",
            "Iteration 672 - Batch 672/903 - Train loss: 9846854.198365528\n",
            "Iteration 673 - Batch 673/903 - Train loss: 9838121.37425816\n",
            "Iteration 674 - Batch 674/903 - Train loss: 9829766.68037037\n",
            "Iteration 675 - Batch 675/903 - Train loss: 9820696.549926035\n",
            "Iteration 676 - Batch 676/903 - Train loss: 9812696.682791729\n",
            "Iteration 677 - Batch 677/903 - Train loss: 9803714.967551623\n",
            "Iteration 678 - Batch 678/903 - Train loss: 9795515.52945508\n",
            "Iteration 679 - Batch 679/903 - Train loss: 9786823.156617647\n",
            "Iteration 680 - Batch 680/903 - Train loss: 9778071.859030837\n",
            "Iteration 681 - Batch 681/903 - Train loss: 9769648.222140763\n",
            "Iteration 682 - Batch 682/903 - Train loss: 9760840.238653002\n",
            "Iteration 683 - Batch 683/903 - Train loss: 9752862.407894736\n",
            "Iteration 684 - Batch 684/903 - Train loss: 9744343.055839416\n",
            "Iteration 685 - Batch 685/903 - Train loss: 9736064.630830904\n",
            "Iteration 686 - Batch 686/903 - Train loss: 9727929.219432315\n",
            "Iteration 687 - Batch 687/903 - Train loss: 9719729.996729651\n",
            "Iteration 688 - Batch 688/903 - Train loss: 9711710.963352686\n",
            "Iteration 689 - Batch 689/903 - Train loss: 9703118.017028986\n",
            "Iteration 690 - Batch 690/903 - Train loss: 9695144.445730824\n",
            "Iteration 691 - Batch 691/903 - Train loss: 9686896.75867052\n",
            "Iteration 692 - Batch 692/903 - Train loss: 9679133.720779222\n",
            "Iteration 693 - Batch 693/903 - Train loss: 9670921.765489914\n",
            "Iteration 694 - Batch 694/903 - Train loss: 9662852.145683452\n",
            "Iteration 695 - Batch 695/903 - Train loss: 9654637.271551725\n",
            "Iteration 696 - Batch 696/903 - Train loss: 9646709.656743186\n",
            "Iteration 697 - Batch 697/903 - Train loss: 9638754.892550142\n",
            "Iteration 698 - Batch 698/903 - Train loss: 9630740.63769671\n",
            "Iteration 699 - Batch 699/903 - Train loss: 9622588.329642856\n",
            "Iteration 700 - Batch 700/903 - Train loss: 9614499.097360913\n",
            "Iteration 701 - Batch 701/903 - Train loss: 9606336.382834759\n",
            "Iteration 702 - Batch 702/903 - Train loss: 9598286.012446657\n",
            "Iteration 703 - Batch 703/903 - Train loss: 9589767.686434658\n",
            "Iteration 704 - Batch 704/903 - Train loss: 9581157.726950355\n",
            "Iteration 705 - Batch 705/903 - Train loss: 9572509.283286119\n",
            "Iteration 706 - Batch 706/903 - Train loss: 9564305.85572843\n",
            "Iteration 707 - Batch 707/903 - Train loss: 9555924.942443503\n",
            "Iteration 708 - Batch 708/903 - Train loss: 9548872.49047955\n",
            "Iteration 709 - Batch 709/903 - Train loss: 9540710.534507042\n",
            "Iteration 710 - Batch 710/903 - Train loss: 9532756.804149086\n",
            "Iteration 711 - Batch 711/903 - Train loss: 9524832.365870787\n",
            "Iteration 712 - Batch 712/903 - Train loss: 9516925.650771389\n",
            "Iteration 713 - Batch 713/903 - Train loss: 9508830.831932774\n",
            "Iteration 714 - Batch 714/903 - Train loss: 9501081.37132867\n",
            "Iteration 715 - Batch 715/903 - Train loss: 9493018.722416202\n",
            "Iteration 716 - Batch 716/903 - Train loss: 9485614.46059972\n",
            "Iteration 717 - Batch 717/903 - Train loss: 9478348.599233983\n",
            "Iteration 718 - Batch 718/903 - Train loss: 9470576.586578581\n",
            "Iteration 719 - Batch 719/903 - Train loss: 9462916.428472223\n",
            "Iteration 720 - Batch 720/903 - Train loss: 9454554.865117893\n",
            "Iteration 721 - Batch 721/903 - Train loss: 9446997.240304708\n",
            "Iteration 722 - Batch 722/903 - Train loss: 9439578.975103734\n",
            "Iteration 723 - Batch 723/903 - Train loss: 9432602.73135359\n",
            "Iteration 724 - Batch 724/903 - Train loss: 9426051.7\n",
            "Iteration 725 - Batch 725/903 - Train loss: 9419988.918732783\n",
            "Iteration 726 - Batch 726/903 - Train loss: 9413179.309491059\n",
            "Iteration 727 - Batch 727/903 - Train loss: 9405997.788461538\n",
            "Iteration 728 - Batch 728/903 - Train loss: 9398610.845679013\n",
            "Iteration 729 - Batch 729/903 - Train loss: 9391683.176027397\n",
            "Iteration 730 - Batch 730/903 - Train loss: 9385114.970588235\n",
            "Iteration 731 - Batch 731/903 - Train loss: 9378154.296448087\n",
            "Iteration 732 - Batch 732/903 - Train loss: 9371002.906548431\n",
            "Iteration 733 - Batch 733/903 - Train loss: 9363072.007152589\n",
            "Iteration 734 - Batch 734/903 - Train loss: 9356896.660884354\n",
            "Iteration 735 - Batch 735/903 - Train loss: 9350839.492866848\n",
            "Iteration 736 - Batch 736/903 - Train loss: 9344386.545793759\n",
            "Iteration 737 - Batch 737/903 - Train loss: 9337382.867886178\n",
            "Iteration 738 - Batch 738/903 - Train loss: 9330093.605548037\n",
            "Iteration 739 - Batch 739/903 - Train loss: 9323191.173648648\n",
            "Iteration 740 - Batch 740/903 - Train loss: 9316817.79217274\n",
            "Iteration 741 - Batch 741/903 - Train loss: 9309774.571091644\n",
            "Iteration 742 - Batch 742/903 - Train loss: 9302980.649057873\n",
            "Iteration 743 - Batch 743/903 - Train loss: 9296347.34375\n",
            "Iteration 744 - Batch 744/903 - Train loss: 9289390.055033557\n",
            "Iteration 745 - Batch 745/903 - Train loss: 9282338.143096514\n",
            "Iteration 746 - Batch 746/903 - Train loss: 9275188.943440428\n",
            "Iteration 747 - Batch 747/903 - Train loss: 9268191.340909092\n",
            "Iteration 748 - Batch 748/903 - Train loss: 9260942.746662216\n",
            "Iteration 749 - Batch 749/903 - Train loss: 9253743.228\n",
            "Iteration 750 - Batch 750/903 - Train loss: 9246687.821904128\n",
            "Iteration 751 - Batch 751/903 - Train loss: 9239563.453789894\n",
            "Iteration 752 - Batch 752/903 - Train loss: 9232881.061420983\n",
            "Iteration 753 - Batch 753/903 - Train loss: 9226233.769562334\n",
            "Iteration 754 - Batch 754/903 - Train loss: 9218895.495695364\n",
            "Iteration 755 - Batch 755/903 - Train loss: 9212184.01984127\n",
            "Iteration 756 - Batch 756/903 - Train loss: 9204832.60997358\n",
            "Iteration 757 - Batch 757/903 - Train loss: 9197791.636543535\n",
            "Iteration 758 - Batch 758/903 - Train loss: 9190667.808629775\n",
            "Iteration 759 - Batch 759/903 - Train loss: 9183764.058552632\n",
            "Iteration 760 - Batch 760/903 - Train loss: 9176489.526609724\n",
            "Iteration 761 - Batch 761/903 - Train loss: 9169444.045603674\n",
            "Iteration 762 - Batch 762/903 - Train loss: 9162198.767038008\n",
            "Iteration 763 - Batch 763/903 - Train loss: 9155476.509816755\n",
            "Iteration 764 - Batch 764/903 - Train loss: 9148470.736601308\n",
            "Iteration 765 - Batch 765/903 - Train loss: 9141843.891971279\n",
            "Iteration 766 - Batch 766/903 - Train loss: 9134607.185462842\n",
            "Iteration 767 - Batch 767/903 - Train loss: 9127610.343098959\n",
            "Iteration 768 - Batch 768/903 - Train loss: 9120675.922626788\n",
            "Iteration 769 - Batch 769/903 - Train loss: 9113717.476298701\n",
            "Iteration 770 - Batch 770/903 - Train loss: 9106868.934176395\n",
            "Iteration 771 - Batch 771/903 - Train loss: 9099978.157059586\n",
            "Iteration 772 - Batch 772/903 - Train loss: 9093017.089262614\n",
            "Iteration 773 - Batch 773/903 - Train loss: 9086476.563630491\n",
            "Iteration 774 - Batch 774/903 - Train loss: 9079732.686774194\n",
            "Iteration 775 - Batch 775/903 - Train loss: 9072902.167525774\n",
            "Iteration 776 - Batch 776/903 - Train loss: 9065875.48037323\n",
            "Iteration 777 - Batch 777/903 - Train loss: 9059484.304948587\n",
            "Iteration 778 - Batch 778/903 - Train loss: 9052700.146341464\n",
            "Iteration 779 - Batch 779/903 - Train loss: 9046176.359615386\n",
            "Iteration 780 - Batch 780/903 - Train loss: 9039691.160691421\n",
            "Iteration 781 - Batch 781/903 - Train loss: 9033306.154092072\n",
            "Iteration 782 - Batch 782/903 - Train loss: 9026434.410600256\n",
            "Iteration 783 - Batch 783/903 - Train loss: 9019502.26945153\n",
            "Iteration 784 - Batch 784/903 - Train loss: 9012728.839171974\n",
            "Iteration 785 - Batch 785/903 - Train loss: 9005977.82951654\n",
            "Iteration 786 - Batch 786/903 - Train loss: 8999103.668996189\n",
            "Iteration 787 - Batch 787/903 - Train loss: 8992492.89181472\n",
            "Iteration 788 - Batch 788/903 - Train loss: 8985938.866603294\n",
            "Iteration 789 - Batch 789/903 - Train loss: 8979442.317405064\n",
            "Iteration 790 - Batch 790/903 - Train loss: 8972961.678887485\n",
            "Iteration 791 - Batch 791/903 - Train loss: 8966306.247474747\n",
            "Iteration 792 - Batch 792/903 - Train loss: 8960056.39186633\n",
            "Iteration 793 - Batch 793/903 - Train loss: 8953471.95906801\n",
            "Iteration 794 - Batch 794/903 - Train loss: 8947596.422012579\n",
            "Iteration 795 - Batch 795/903 - Train loss: 8941489.425565327\n",
            "Iteration 796 - Batch 796/903 - Train loss: 8935426.691028858\n",
            "Iteration 797 - Batch 797/903 - Train loss: 8929003.104323309\n",
            "Iteration 798 - Batch 798/903 - Train loss: 8922478.444931164\n",
            "Iteration 799 - Batch 799/903 - Train loss: 8915932.614375\n",
            "Iteration 800 - Batch 800/903 - Train loss: 8909528.463795256\n",
            "Iteration 801 - Batch 801/903 - Train loss: 8902853.412718205\n",
            "Iteration 802 - Batch 802/903 - Train loss: 8896343.64445828\n",
            "Iteration 803 - Batch 803/903 - Train loss: 8890240.008084577\n",
            "Iteration 804 - Batch 804/903 - Train loss: 8883832.391925465\n",
            "Iteration 805 - Batch 805/903 - Train loss: 8877429.445099255\n",
            "Iteration 806 - Batch 806/903 - Train loss: 8870848.609355638\n",
            "Iteration 807 - Batch 807/903 - Train loss: 8864574.505259901\n",
            "Iteration 808 - Batch 808/903 - Train loss: 8857713.767614339\n",
            "Iteration 809 - Batch 809/903 - Train loss: 8850904.580555556\n",
            "Iteration 810 - Batch 810/903 - Train loss: 8844682.777435265\n",
            "Iteration 811 - Batch 811/903 - Train loss: 8838318.557573892\n",
            "Iteration 812 - Batch 812/903 - Train loss: 8832015.128228782\n",
            "Iteration 813 - Batch 813/903 - Train loss: 8826121.044840295\n",
            "Iteration 814 - Batch 814/903 - Train loss: 8819947.358282208\n",
            "Iteration 815 - Batch 815/903 - Train loss: 8813892.590992646\n",
            "Iteration 816 - Batch 816/903 - Train loss: 8807542.144430844\n",
            "Iteration 817 - Batch 817/903 - Train loss: 8801267.732579462\n",
            "Iteration 818 - Batch 818/903 - Train loss: 8794717.985653237\n",
            "Iteration 819 - Batch 819/903 - Train loss: 8788876.035975609\n",
            "Iteration 820 - Batch 820/903 - Train loss: 8782615.092874544\n",
            "Iteration 821 - Batch 821/903 - Train loss: 8776056.911192214\n",
            "Iteration 822 - Batch 822/903 - Train loss: 8769609.256986635\n",
            "Iteration 823 - Batch 823/903 - Train loss: 8763400.242718447\n",
            "Iteration 824 - Batch 824/903 - Train loss: 8757752.143636364\n",
            "Iteration 825 - Batch 825/903 - Train loss: 8751501.410411622\n",
            "Iteration 826 - Batch 826/903 - Train loss: 8744965.625151148\n",
            "Iteration 827 - Batch 827/903 - Train loss: 8738669.657910628\n",
            "Iteration 828 - Batch 828/903 - Train loss: 8732571.998492159\n",
            "Iteration 829 - Batch 829/903 - Train loss: 8726116.95813253\n",
            "Iteration 830 - Batch 830/903 - Train loss: 8719905.228941035\n",
            "Iteration 831 - Batch 831/903 - Train loss: 8713971.577524038\n",
            "Iteration 832 - Batch 832/903 - Train loss: 8708429.68607443\n",
            "Iteration 833 - Batch 833/903 - Train loss: 8702187.199340528\n",
            "Iteration 834 - Batch 834/903 - Train loss: 8696185.82005988\n",
            "Iteration 835 - Batch 835/903 - Train loss: 8689910.989533493\n",
            "Iteration 836 - Batch 836/903 - Train loss: 8683633.64934289\n",
            "Iteration 837 - Batch 837/903 - Train loss: 8677234.957040573\n",
            "Iteration 838 - Batch 838/903 - Train loss: 8670763.010131108\n",
            "Iteration 839 - Batch 839/903 - Train loss: 8664852.881845238\n",
            "Iteration 840 - Batch 840/903 - Train loss: 8658785.052913198\n",
            "Iteration 841 - Batch 841/903 - Train loss: 8652724.270783847\n",
            "Iteration 842 - Batch 842/903 - Train loss: 8646694.738137605\n",
            "Iteration 843 - Batch 843/903 - Train loss: 8641067.713862559\n",
            "Iteration 844 - Batch 844/903 - Train loss: 8635204.107100591\n",
            "Iteration 845 - Batch 845/903 - Train loss: 8629767.366725769\n",
            "Iteration 846 - Batch 846/903 - Train loss: 8624276.036599765\n",
            "Iteration 847 - Batch 847/903 - Train loss: 8619490.248231132\n",
            "Iteration 848 - Batch 848/903 - Train loss: 8614729.912838634\n",
            "Iteration 849 - Batch 849/903 - Train loss: 8609740.232941177\n",
            "Iteration 850 - Batch 850/903 - Train loss: 8603932.052585194\n",
            "Iteration 851 - Batch 851/903 - Train loss: 8598012.58362676\n",
            "Iteration 852 - Batch 852/903 - Train loss: 8592256.800703399\n",
            "Iteration 853 - Batch 853/903 - Train loss: 8587021.293911006\n",
            "Iteration 854 - Batch 854/903 - Train loss: 8581837.719298245\n",
            "Iteration 855 - Batch 855/903 - Train loss: 8576592.575642524\n",
            "Iteration 856 - Batch 856/903 - Train loss: 8570726.957117854\n",
            "Iteration 857 - Batch 857/903 - Train loss: 8564991.561188811\n",
            "Iteration 858 - Batch 858/903 - Train loss: 8559512.678696157\n",
            "Iteration 859 - Batch 859/903 - Train loss: 8554293.413372094\n",
            "Iteration 860 - Batch 860/903 - Train loss: 8548862.033681765\n",
            "Iteration 861 - Batch 861/903 - Train loss: 8543023.342807425\n",
            "Iteration 862 - Batch 862/903 - Train loss: 8537414.020567786\n",
            "Iteration 863 - Batch 863/903 - Train loss: 8531893.214988425\n",
            "Iteration 864 - Batch 864/903 - Train loss: 8526999.608381502\n",
            "Iteration 865 - Batch 865/903 - Train loss: 8522160.401558891\n",
            "Iteration 866 - Batch 866/903 - Train loss: 8516132.920703575\n",
            "Iteration 867 - Batch 867/903 - Train loss: 8510833.424827188\n",
            "Iteration 868 - Batch 868/903 - Train loss: 8505903.916283084\n",
            "Iteration 869 - Batch 869/903 - Train loss: 8501249.58591954\n",
            "Iteration 870 - Batch 870/903 - Train loss: 8495688.730195178\n",
            "Iteration 871 - Batch 871/903 - Train loss: 8490243.637614679\n",
            "Iteration 872 - Batch 872/903 - Train loss: 8484945.193871707\n",
            "Iteration 873 - Batch 873/903 - Train loss: 8479430.792334097\n",
            "Iteration 874 - Batch 874/903 - Train loss: 8474116.870571429\n",
            "Iteration 875 - Batch 875/903 - Train loss: 8469158.060502283\n",
            "Iteration 876 - Batch 876/903 - Train loss: 8464141.268529076\n",
            "Iteration 877 - Batch 877/903 - Train loss: 8458865.460136674\n",
            "Iteration 878 - Batch 878/903 - Train loss: 8453283.531001138\n",
            "Iteration 879 - Batch 879/903 - Train loss: 8449047.157670455\n",
            "Iteration 880 - Batch 880/903 - Train loss: 8445143.410045402\n",
            "Iteration 881 - Batch 881/903 - Train loss: 8439898.838435374\n",
            "Iteration 882 - Batch 882/903 - Train loss: 8434472.261041902\n",
            "Iteration 883 - Batch 883/903 - Train loss: 8429638.156391403\n",
            "Iteration 884 - Batch 884/903 - Train loss: 8424298.51158192\n",
            "Iteration 885 - Batch 885/903 - Train loss: 8419589.941027088\n",
            "Iteration 886 - Batch 886/903 - Train loss: 8415393.329481399\n",
            "Iteration 887 - Batch 887/903 - Train loss: 8409921.992117116\n",
            "Iteration 888 - Batch 888/903 - Train loss: 8404703.899606299\n",
            "Iteration 889 - Batch 889/903 - Train loss: 8400296.550842697\n",
            "Iteration 890 - Batch 890/903 - Train loss: 8395860.120370371\n",
            "Iteration 891 - Batch 891/903 - Train loss: 8390567.781670403\n",
            "Iteration 892 - Batch 892/903 - Train loss: 8386047.337905935\n",
            "Iteration 893 - Batch 893/903 - Train loss: 8381079.133389262\n",
            "Iteration 894 - Batch 894/903 - Train loss: 8376415.166759777\n",
            "Iteration 895 - Batch 895/903 - Train loss: 8371563.95703125\n",
            "Iteration 896 - Batch 896/903 - Train loss: 8366617.198439242\n",
            "Iteration 897 - Batch 897/903 - Train loss: 8361173.462138085\n",
            "Iteration 898 - Batch 898/903 - Train loss: 8356393.33676307\n",
            "Iteration 899 - Batch 899/903 - Train loss: 8351307.725555556\n",
            "Iteration 900 - Batch 900/903 - Train loss: 8345854.317425083\n",
            "Iteration 901 - Batch 901/903 - Train loss: 8340652.867516629\n",
            "Iteration 902 - Batch 902/903 - Train loss: 8332321.9056616835\n",
            "Val loss: 3143309.5\n",
            "Epoch 2/6\n",
            "Iteration 904 - Batch 1/903 - Train loss: 4067958.875\n",
            "Iteration 905 - Batch 2/903 - Train loss: 3892029.5833333335\n",
            "Iteration 906 - Batch 3/903 - Train loss: 3937214.1875\n",
            "Iteration 907 - Batch 4/903 - Train loss: 3950342.75\n",
            "Iteration 908 - Batch 5/903 - Train loss: 3956635.4583333335\n",
            "Iteration 909 - Batch 6/903 - Train loss: 3937745.5714285714\n",
            "Iteration 910 - Batch 7/903 - Train loss: 3968019.78125\n",
            "Iteration 911 - Batch 8/903 - Train loss: 3940414.222222222\n",
            "Iteration 912 - Batch 9/903 - Train loss: 3982374.6\n",
            "Iteration 913 - Batch 10/903 - Train loss: 4002700.272727273\n",
            "Iteration 914 - Batch 11/903 - Train loss: 3954340.75\n",
            "Iteration 915 - Batch 12/903 - Train loss: 3945805.0576923075\n",
            "Iteration 916 - Batch 13/903 - Train loss: 3928841.0\n",
            "Iteration 917 - Batch 14/903 - Train loss: 3895214.3666666667\n",
            "Iteration 918 - Batch 15/903 - Train loss: 3909627.71875\n",
            "Iteration 919 - Batch 16/903 - Train loss: 3921289.3235294116\n",
            "Iteration 920 - Batch 17/903 - Train loss: 3903308.472222222\n",
            "Iteration 921 - Batch 18/903 - Train loss: 3883939.7236842103\n",
            "Iteration 922 - Batch 19/903 - Train loss: 3880823.2375\n",
            "Iteration 923 - Batch 20/903 - Train loss: 3872330.0\n",
            "Iteration 924 - Batch 21/903 - Train loss: 3875668.6136363638\n",
            "Iteration 925 - Batch 22/903 - Train loss: 3870832.782608696\n",
            "Iteration 926 - Batch 23/903 - Train loss: 3857075.0104166665\n",
            "Iteration 927 - Batch 24/903 - Train loss: 3850111.73\n",
            "Iteration 928 - Batch 25/903 - Train loss: 3833541.173076923\n",
            "Iteration 929 - Batch 26/903 - Train loss: 3818707.611111111\n",
            "Iteration 930 - Batch 27/903 - Train loss: 3811135.410714286\n",
            "Iteration 931 - Batch 28/903 - Train loss: 3798682.9051724137\n",
            "Iteration 932 - Batch 29/903 - Train loss: 3792952.475\n",
            "Iteration 933 - Batch 30/903 - Train loss: 3798180.4838709678\n",
            "Iteration 934 - Batch 31/903 - Train loss: 3785890.1953125\n",
            "Iteration 935 - Batch 32/903 - Train loss: 3770371.2045454546\n",
            "Iteration 936 - Batch 33/903 - Train loss: 3767978.213235294\n",
            "Iteration 937 - Batch 34/903 - Train loss: 3759621.842857143\n",
            "Iteration 938 - Batch 35/903 - Train loss: 3753204.3194444445\n",
            "Iteration 939 - Batch 36/903 - Train loss: 3745092.608108108\n",
            "Iteration 940 - Batch 37/903 - Train loss: 3748674.072368421\n",
            "Iteration 941 - Batch 38/903 - Train loss: 3749253.846153846\n",
            "Iteration 942 - Batch 39/903 - Train loss: 3742774.6125\n",
            "Iteration 943 - Batch 40/903 - Train loss: 3736613.225609756\n",
            "Iteration 944 - Batch 41/903 - Train loss: 3729919.511904762\n",
            "Iteration 945 - Batch 42/903 - Train loss: 3721487.7906976743\n",
            "Iteration 946 - Batch 43/903 - Train loss: 3711740.7045454546\n",
            "Iteration 947 - Batch 44/903 - Train loss: 3710457.6555555556\n",
            "Iteration 948 - Batch 45/903 - Train loss: 3710832.6413043477\n",
            "Iteration 949 - Batch 46/903 - Train loss: 3706745.537234043\n",
            "Iteration 950 - Batch 47/903 - Train loss: 3707616.8802083335\n",
            "Iteration 951 - Batch 48/903 - Train loss: 3709612.1836734693\n",
            "Iteration 952 - Batch 49/903 - Train loss: 3697228.355\n",
            "Iteration 953 - Batch 50/903 - Train loss: 3694296.475490196\n",
            "Iteration 954 - Batch 51/903 - Train loss: 3689238.1875\n",
            "Iteration 955 - Batch 52/903 - Train loss: 3686281.9386792453\n",
            "Iteration 956 - Batch 53/903 - Train loss: 3682734.8333333335\n",
            "Iteration 957 - Batch 54/903 - Train loss: 3682440.154545455\n",
            "Iteration 958 - Batch 55/903 - Train loss: 3675934.7366071427\n",
            "Iteration 959 - Batch 56/903 - Train loss: 3672870.9210526315\n",
            "Iteration 960 - Batch 57/903 - Train loss: 3671263.2672413792\n",
            "Iteration 961 - Batch 58/903 - Train loss: 3672432.779661017\n",
            "Iteration 962 - Batch 59/903 - Train loss: 3665180.9791666665\n",
            "Iteration 963 - Batch 60/903 - Train loss: 3661444.799180328\n",
            "Iteration 964 - Batch 61/903 - Train loss: 3655938.3588709678\n",
            "Iteration 965 - Batch 62/903 - Train loss: 3650786.4285714286\n",
            "Iteration 966 - Batch 63/903 - Train loss: 3647990.91796875\n",
            "Iteration 967 - Batch 64/903 - Train loss: 3641952.9076923076\n",
            "Iteration 968 - Batch 65/903 - Train loss: 3640240.784090909\n",
            "Iteration 969 - Batch 66/903 - Train loss: 3645365.7649253733\n",
            "Iteration 970 - Batch 67/903 - Train loss: 3642217.4852941176\n",
            "Iteration 971 - Batch 68/903 - Train loss: 3640973.2101449277\n",
            "Iteration 972 - Batch 69/903 - Train loss: 3638558.0535714286\n",
            "Iteration 973 - Batch 70/903 - Train loss: 3632417.7112676054\n",
            "Iteration 974 - Batch 71/903 - Train loss: 3628830.5729166665\n",
            "Iteration 975 - Batch 72/903 - Train loss: 3623127.8493150687\n",
            "Iteration 976 - Batch 73/903 - Train loss: 3619823.010135135\n",
            "Iteration 977 - Batch 74/903 - Train loss: 3617653.756666667\n",
            "Iteration 978 - Batch 75/903 - Train loss: 3614445.1710526315\n",
            "Iteration 979 - Batch 76/903 - Train loss: 3612572.3766233767\n",
            "Iteration 980 - Batch 77/903 - Train loss: 3610846.5576923075\n",
            "Iteration 981 - Batch 78/903 - Train loss: 3612363.398734177\n",
            "Iteration 982 - Batch 79/903 - Train loss: 3609597.871875\n",
            "Iteration 983 - Batch 80/903 - Train loss: 3606642.0185185187\n",
            "Iteration 984 - Batch 81/903 - Train loss: 3604488.426829268\n",
            "Iteration 985 - Batch 82/903 - Train loss: 3603499.768072289\n",
            "Iteration 986 - Batch 83/903 - Train loss: 3604822.3958333335\n",
            "Iteration 987 - Batch 84/903 - Train loss: 3599066.6088235294\n",
            "Iteration 988 - Batch 85/903 - Train loss: 3596926.0872093025\n",
            "Iteration 989 - Batch 86/903 - Train loss: 3596991.692528736\n",
            "Iteration 990 - Batch 87/903 - Train loss: 3593487.877840909\n",
            "Iteration 991 - Batch 88/903 - Train loss: 3591992.808988764\n",
            "Iteration 992 - Batch 89/903 - Train loss: 3589173.3805555557\n",
            "Iteration 993 - Batch 90/903 - Train loss: 3590404.5027472526\n",
            "Iteration 994 - Batch 91/903 - Train loss: 3588678.8614130435\n",
            "Iteration 995 - Batch 92/903 - Train loss: 3583385.744623656\n",
            "Iteration 996 - Batch 93/903 - Train loss: 3582345.284574468\n",
            "Iteration 997 - Batch 94/903 - Train loss: 3584923.415789474\n",
            "Iteration 998 - Batch 95/903 - Train loss: 3583818.3645833335\n",
            "Iteration 999 - Batch 96/903 - Train loss: 3583647.5077319588\n",
            "Iteration 1000 - Batch 97/903 - Train loss: 3580408.5\n",
            "Iteration 1001 - Batch 98/903 - Train loss: 3577286.984848485\n",
            "Iteration 1002 - Batch 99/903 - Train loss: 3576440.575\n",
            "Iteration 1003 - Batch 100/903 - Train loss: 3574539.217821782\n",
            "Iteration 1004 - Batch 101/903 - Train loss: 3571522.3553921566\n",
            "Iteration 1005 - Batch 102/903 - Train loss: 3569225.0582524273\n",
            "Iteration 1006 - Batch 103/903 - Train loss: 3567738.435096154\n",
            "Iteration 1007 - Batch 104/903 - Train loss: 3567391.095238095\n",
            "Iteration 1008 - Batch 105/903 - Train loss: 3565575.1391509436\n",
            "Iteration 1009 - Batch 106/903 - Train loss: 3565271.918224299\n",
            "Iteration 1010 - Batch 107/903 - Train loss: 3564779.525462963\n",
            "Iteration 1011 - Batch 108/903 - Train loss: 3562417.268348624\n",
            "Iteration 1012 - Batch 109/903 - Train loss: 3562453.5704545453\n",
            "Iteration 1013 - Batch 110/903 - Train loss: 3559864.7725225226\n",
            "Iteration 1014 - Batch 111/903 - Train loss: 3556294.0\n",
            "Iteration 1015 - Batch 112/903 - Train loss: 3554685.8207964604\n",
            "Iteration 1016 - Batch 113/903 - Train loss: 3554055.85745614\n",
            "Iteration 1017 - Batch 114/903 - Train loss: 3550749.05\n",
            "Iteration 1018 - Batch 115/903 - Train loss: 3549831.3038793104\n",
            "Iteration 1019 - Batch 116/903 - Train loss: 3548557.9273504275\n",
            "Iteration 1020 - Batch 117/903 - Train loss: 3546654.48940678\n",
            "Iteration 1021 - Batch 118/903 - Train loss: 3546495.31302521\n",
            "Iteration 1022 - Batch 119/903 - Train loss: 3544150.7875\n",
            "Iteration 1023 - Batch 120/903 - Train loss: 3543715.3202479337\n",
            "Iteration 1024 - Batch 121/903 - Train loss: 3543882.725409836\n",
            "Iteration 1025 - Batch 122/903 - Train loss: 3540874.9837398375\n",
            "Iteration 1026 - Batch 123/903 - Train loss: 3538877.9112903224\n",
            "Iteration 1027 - Batch 124/903 - Train loss: 3537318.126\n",
            "Iteration 1028 - Batch 125/903 - Train loss: 3535825.3055555555\n",
            "Iteration 1029 - Batch 126/903 - Train loss: 3537241.929133858\n",
            "Iteration 1030 - Batch 127/903 - Train loss: 3535280.275390625\n",
            "Iteration 1031 - Batch 128/903 - Train loss: 3534146.8372093025\n",
            "Iteration 1032 - Batch 129/903 - Train loss: 3532251.8615384614\n",
            "Iteration 1033 - Batch 130/903 - Train loss: 3528905.3721374045\n",
            "Iteration 1034 - Batch 131/903 - Train loss: 3530647.0738636362\n",
            "Iteration 1035 - Batch 132/903 - Train loss: 3528392.7781954887\n",
            "Iteration 1036 - Batch 133/903 - Train loss: 3527285.817164179\n",
            "Iteration 1037 - Batch 134/903 - Train loss: 3525327.059259259\n",
            "Iteration 1038 - Batch 135/903 - Train loss: 3523460.9908088236\n",
            "Iteration 1039 - Batch 136/903 - Train loss: 3522013.0802919706\n",
            "Iteration 1040 - Batch 137/903 - Train loss: 3519557.0398550723\n",
            "Iteration 1041 - Batch 138/903 - Train loss: 3518300.5791366906\n",
            "Iteration 1042 - Batch 139/903 - Train loss: 3518440.0875\n",
            "Iteration 1043 - Batch 140/903 - Train loss: 3518084.969858156\n",
            "Iteration 1044 - Batch 141/903 - Train loss: 3517064.7658450706\n",
            "Iteration 1045 - Batch 142/903 - Train loss: 3515625.160839161\n",
            "Iteration 1046 - Batch 143/903 - Train loss: 3514567.434027778\n",
            "Iteration 1047 - Batch 144/903 - Train loss: 3516353.3689655173\n",
            "Iteration 1048 - Batch 145/903 - Train loss: 3515529.970890411\n",
            "Iteration 1049 - Batch 146/903 - Train loss: 3512810.569727891\n",
            "Iteration 1050 - Batch 147/903 - Train loss: 3511917.964527027\n",
            "Iteration 1051 - Batch 148/903 - Train loss: 3512849.716442953\n",
            "Iteration 1052 - Batch 149/903 - Train loss: 3513532.3066666666\n",
            "Iteration 1053 - Batch 150/903 - Train loss: 3514372.2880794704\n",
            "Iteration 1054 - Batch 151/903 - Train loss: 3513659.9226973685\n",
            "Iteration 1055 - Batch 152/903 - Train loss: 3513785.2614379083\n",
            "Iteration 1056 - Batch 153/903 - Train loss: 3514378.7662337665\n",
            "Iteration 1057 - Batch 154/903 - Train loss: 3516259.877419355\n",
            "Iteration 1058 - Batch 155/903 - Train loss: 3514761.5240384615\n",
            "Iteration 1059 - Batch 156/903 - Train loss: 3514322.243630573\n",
            "Iteration 1060 - Batch 157/903 - Train loss: 3516343.0221518986\n",
            "Iteration 1061 - Batch 158/903 - Train loss: 3514723.8317610063\n",
            "Iteration 1062 - Batch 159/903 - Train loss: 3513964.7328125\n",
            "Iteration 1063 - Batch 160/903 - Train loss: 3511498.3913043477\n",
            "Iteration 1064 - Batch 161/903 - Train loss: 3510519.575617284\n",
            "Iteration 1065 - Batch 162/903 - Train loss: 3509488.243865031\n",
            "Iteration 1066 - Batch 163/903 - Train loss: 3507707.349085366\n",
            "Iteration 1067 - Batch 164/903 - Train loss: 3506848.360606061\n",
            "Iteration 1068 - Batch 165/903 - Train loss: 3505607.6807228914\n",
            "Iteration 1069 - Batch 166/903 - Train loss: 3508529.332335329\n",
            "Iteration 1070 - Batch 167/903 - Train loss: 3509610.1696428573\n",
            "Iteration 1071 - Batch 168/903 - Train loss: 3508480.6819526628\n",
            "Iteration 1072 - Batch 169/903 - Train loss: 3508428.3779411763\n",
            "Iteration 1073 - Batch 170/903 - Train loss: 3506854.0292397663\n",
            "Iteration 1074 - Batch 171/903 - Train loss: 3507001.8895348837\n",
            "Iteration 1075 - Batch 172/903 - Train loss: 3507418.4104046244\n",
            "Iteration 1076 - Batch 173/903 - Train loss: 3506940.1408045976\n",
            "Iteration 1077 - Batch 174/903 - Train loss: 3506459.137142857\n",
            "Iteration 1078 - Batch 175/903 - Train loss: 3506142.25\n",
            "Iteration 1079 - Batch 176/903 - Train loss: 3505693.2556497175\n",
            "Iteration 1080 - Batch 177/903 - Train loss: 3505785.3146067415\n",
            "Iteration 1081 - Batch 178/903 - Train loss: 3508214.36452514\n",
            "Iteration 1082 - Batch 179/903 - Train loss: 3507871.0805555554\n",
            "Iteration 1083 - Batch 180/903 - Train loss: 3507546.408839779\n",
            "Iteration 1084 - Batch 181/903 - Train loss: 3505617.943681319\n",
            "Iteration 1085 - Batch 182/903 - Train loss: 3505068.4344262294\n",
            "Iteration 1086 - Batch 183/903 - Train loss: 3504653.0720108696\n",
            "Iteration 1087 - Batch 184/903 - Train loss: 3505135.6256756755\n",
            "Iteration 1088 - Batch 185/903 - Train loss: 3504620.4112903224\n",
            "Iteration 1089 - Batch 186/903 - Train loss: 3503497.7633689838\n",
            "Iteration 1090 - Batch 187/903 - Train loss: 3504278.287234043\n",
            "Iteration 1091 - Batch 188/903 - Train loss: 3503484.8597883596\n",
            "Iteration 1092 - Batch 189/903 - Train loss: 3502797.602631579\n",
            "Iteration 1093 - Batch 190/903 - Train loss: 3502025.467277487\n",
            "Iteration 1094 - Batch 191/903 - Train loss: 3501414.2421875\n",
            "Iteration 1095 - Batch 192/903 - Train loss: 3501270.9248704663\n",
            "Iteration 1096 - Batch 193/903 - Train loss: 3500515.1108247424\n",
            "Iteration 1097 - Batch 194/903 - Train loss: 3498709.187179487\n",
            "Iteration 1098 - Batch 195/903 - Train loss: 3498256.224489796\n",
            "Iteration 1099 - Batch 196/903 - Train loss: 3500020.6878172588\n",
            "Iteration 1100 - Batch 197/903 - Train loss: 3502988.773989899\n",
            "Iteration 1101 - Batch 198/903 - Train loss: 3502691.75\n",
            "Iteration 1102 - Batch 199/903 - Train loss: 3502926.16\n",
            "Iteration 1103 - Batch 200/903 - Train loss: 3502119.5049751243\n",
            "Iteration 1104 - Batch 201/903 - Train loss: 3500575.1584158414\n",
            "Iteration 1105 - Batch 202/903 - Train loss: 3500352.7721674875\n",
            "Iteration 1106 - Batch 203/903 - Train loss: 3498705.4595588236\n",
            "Iteration 1107 - Batch 204/903 - Train loss: 3500013.0865853657\n",
            "Iteration 1108 - Batch 205/903 - Train loss: 3501538.100728155\n",
            "Iteration 1109 - Batch 206/903 - Train loss: 3501568.579710145\n",
            "Iteration 1110 - Batch 207/903 - Train loss: 3500929.4170673075\n",
            "Iteration 1111 - Batch 208/903 - Train loss: 3500954.534688995\n",
            "Iteration 1112 - Batch 209/903 - Train loss: 3503631.473809524\n",
            "Iteration 1113 - Batch 210/903 - Train loss: 3503133.261848341\n",
            "Iteration 1114 - Batch 211/903 - Train loss: 3504137.3101415094\n",
            "Iteration 1115 - Batch 212/903 - Train loss: 3504453.186619718\n",
            "Iteration 1116 - Batch 213/903 - Train loss: 3504474.3528037383\n",
            "Iteration 1117 - Batch 214/903 - Train loss: 3506888.5616279067\n",
            "Iteration 1118 - Batch 215/903 - Train loss: 3506141.9791666665\n",
            "Iteration 1119 - Batch 216/903 - Train loss: 3507179.3087557605\n",
            "Iteration 1120 - Batch 217/903 - Train loss: 3507327.9116972475\n",
            "Iteration 1121 - Batch 218/903 - Train loss: 3506327.7910958906\n",
            "Iteration 1122 - Batch 219/903 - Train loss: 3504274.894318182\n",
            "Iteration 1123 - Batch 220/903 - Train loss: 3503509.25\n",
            "Iteration 1124 - Batch 221/903 - Train loss: 3502237.993243243\n",
            "Iteration 1125 - Batch 222/903 - Train loss: 3502844.1860986548\n",
            "Iteration 1126 - Batch 223/903 - Train loss: 3501920.246651786\n",
            "Iteration 1127 - Batch 224/903 - Train loss: 3500256.461111111\n",
            "Iteration 1128 - Batch 225/903 - Train loss: 3498208.896017699\n",
            "Iteration 1129 - Batch 226/903 - Train loss: 3497561.5914096916\n",
            "Iteration 1130 - Batch 227/903 - Train loss: 3496056.0197368423\n",
            "Iteration 1131 - Batch 228/903 - Train loss: 3493361.2576419213\n",
            "Iteration 1132 - Batch 229/903 - Train loss: 3493605.519565217\n",
            "Iteration 1133 - Batch 230/903 - Train loss: 3492420.645021645\n",
            "Iteration 1134 - Batch 231/903 - Train loss: 3492466.3814655175\n",
            "Iteration 1135 - Batch 232/903 - Train loss: 3492106.839055794\n",
            "Iteration 1136 - Batch 233/903 - Train loss: 3491046.0844017095\n",
            "Iteration 1137 - Batch 234/903 - Train loss: 3489576.6776595744\n",
            "Iteration 1138 - Batch 235/903 - Train loss: 3488451.3358050846\n",
            "Iteration 1139 - Batch 236/903 - Train loss: 3489037.5717299576\n",
            "Iteration 1140 - Batch 237/903 - Train loss: 3487556.5462184874\n",
            "Iteration 1141 - Batch 238/903 - Train loss: 3487081.4027196653\n",
            "Iteration 1142 - Batch 239/903 - Train loss: 3485612.629166667\n",
            "Iteration 1143 - Batch 240/903 - Train loss: 3485343.5663900413\n",
            "Iteration 1144 - Batch 241/903 - Train loss: 3485426.1880165287\n",
            "Iteration 1145 - Batch 242/903 - Train loss: 3484530.5802469137\n",
            "Iteration 1146 - Batch 243/903 - Train loss: 3484398.643442623\n",
            "Iteration 1147 - Batch 244/903 - Train loss: 3483143.2714285715\n",
            "Iteration 1148 - Batch 245/903 - Train loss: 3482069.0701219514\n",
            "Iteration 1149 - Batch 246/903 - Train loss: 3481682.7155870446\n",
            "Iteration 1150 - Batch 247/903 - Train loss: 3480590.7933467743\n",
            "Iteration 1151 - Batch 248/903 - Train loss: 3480079.201807229\n",
            "Iteration 1152 - Batch 249/903 - Train loss: 3481484.16\n",
            "Iteration 1153 - Batch 250/903 - Train loss: 3483177.626494024\n",
            "Iteration 1154 - Batch 251/903 - Train loss: 3483010.050595238\n",
            "Iteration 1155 - Batch 252/903 - Train loss: 3482328.5039525693\n",
            "Iteration 1156 - Batch 253/903 - Train loss: 3481874.6673228345\n",
            "Iteration 1157 - Batch 254/903 - Train loss: 3481538.317647059\n",
            "Iteration 1158 - Batch 255/903 - Train loss: 3481529.6064453125\n",
            "Iteration 1159 - Batch 256/903 - Train loss: 3480870.212062257\n",
            "Iteration 1160 - Batch 257/903 - Train loss: 3480173.113372093\n",
            "Iteration 1161 - Batch 258/903 - Train loss: 3480000.784749035\n",
            "Iteration 1162 - Batch 259/903 - Train loss: 3480022.875\n",
            "Iteration 1163 - Batch 260/903 - Train loss: 3480512.6379310344\n",
            "Iteration 1164 - Batch 261/903 - Train loss: 3480390.869274809\n",
            "Iteration 1165 - Batch 262/903 - Train loss: 3480598.799429658\n",
            "Iteration 1166 - Batch 263/903 - Train loss: 3479787.1979166665\n",
            "Iteration 1167 - Batch 264/903 - Train loss: 3479147.6235849056\n",
            "Iteration 1168 - Batch 265/903 - Train loss: 3478396.285714286\n",
            "Iteration 1169 - Batch 266/903 - Train loss: 3478029.7191011235\n",
            "Iteration 1170 - Batch 267/903 - Train loss: 3477707.8041044776\n",
            "Iteration 1171 - Batch 268/903 - Train loss: 3477635.2388475835\n",
            "Iteration 1172 - Batch 269/903 - Train loss: 3477068.2805555556\n",
            "Iteration 1173 - Batch 270/903 - Train loss: 3476603.114391144\n",
            "Iteration 1174 - Batch 271/903 - Train loss: 3476555.6810661764\n",
            "Iteration 1175 - Batch 272/903 - Train loss: 3475279.391025641\n",
            "Iteration 1176 - Batch 273/903 - Train loss: 3475917.7290145988\n",
            "Iteration 1177 - Batch 274/903 - Train loss: 3475069.558181818\n",
            "Iteration 1178 - Batch 275/903 - Train loss: 3474231.4664855073\n",
            "Iteration 1179 - Batch 276/903 - Train loss: 3473376.944043321\n",
            "Iteration 1180 - Batch 277/903 - Train loss: 3472318.525179856\n",
            "Iteration 1181 - Batch 278/903 - Train loss: 3471822.814516129\n",
            "Iteration 1182 - Batch 279/903 - Train loss: 3469691.5660714284\n",
            "Iteration 1183 - Batch 280/903 - Train loss: 3469656.981316726\n",
            "Iteration 1184 - Batch 281/903 - Train loss: 3469568.983156028\n",
            "Iteration 1185 - Batch 282/903 - Train loss: 3470453.585689046\n",
            "Iteration 1186 - Batch 283/903 - Train loss: 3470577.9445422534\n",
            "Iteration 1187 - Batch 284/903 - Train loss: 3469597.7219298244\n",
            "Iteration 1188 - Batch 285/903 - Train loss: 3470508.9405594408\n",
            "Iteration 1189 - Batch 286/903 - Train loss: 3468608.43641115\n",
            "Iteration 1190 - Batch 287/903 - Train loss: 3467353.3368055555\n",
            "Iteration 1191 - Batch 288/903 - Train loss: 3466488.101211073\n",
            "Iteration 1192 - Batch 289/903 - Train loss: 3465260.651724138\n",
            "Iteration 1193 - Batch 290/903 - Train loss: 3463851.8951890036\n",
            "Iteration 1194 - Batch 291/903 - Train loss: 3464395.4297945206\n",
            "Iteration 1195 - Batch 292/903 - Train loss: 3464287.0230375426\n",
            "Iteration 1196 - Batch 293/903 - Train loss: 3464098.7576530613\n",
            "Iteration 1197 - Batch 294/903 - Train loss: 3463045.0093220337\n",
            "Iteration 1198 - Batch 295/903 - Train loss: 3462253.6131756757\n",
            "Iteration 1199 - Batch 296/903 - Train loss: 3462600.777777778\n",
            "Iteration 1200 - Batch 297/903 - Train loss: 3461996.072986577\n",
            "Iteration 1201 - Batch 298/903 - Train loss: 3461417.83361204\n",
            "Iteration 1202 - Batch 299/903 - Train loss: 3461094.8741666665\n",
            "Iteration 1203 - Batch 300/903 - Train loss: 3461579.161960133\n",
            "Iteration 1204 - Batch 301/903 - Train loss: 3462268.419701987\n",
            "Iteration 1205 - Batch 302/903 - Train loss: 3461250.989273927\n",
            "Iteration 1206 - Batch 303/903 - Train loss: 3460930.7039473685\n",
            "Iteration 1207 - Batch 304/903 - Train loss: 3461470.4286885248\n",
            "Iteration 1208 - Batch 305/903 - Train loss: 3460698.6601307187\n",
            "Iteration 1209 - Batch 306/903 - Train loss: 3460155.32980456\n",
            "Iteration 1210 - Batch 307/903 - Train loss: 3459468.8181818184\n",
            "Iteration 1211 - Batch 308/903 - Train loss: 3459184.377831715\n",
            "Iteration 1212 - Batch 309/903 - Train loss: 3459532.561290323\n",
            "Iteration 1213 - Batch 310/903 - Train loss: 3459047.8754019295\n",
            "Iteration 1214 - Batch 311/903 - Train loss: 3457770.592147436\n",
            "Iteration 1215 - Batch 312/903 - Train loss: 3457623.126198083\n",
            "Iteration 1216 - Batch 313/903 - Train loss: 3457332.803343949\n",
            "Iteration 1217 - Batch 314/903 - Train loss: 3457194.214285714\n",
            "Iteration 1218 - Batch 315/903 - Train loss: 3456324.6210443038\n",
            "Iteration 1219 - Batch 316/903 - Train loss: 3456140.2831230285\n",
            "Iteration 1220 - Batch 317/903 - Train loss: 3456316.8081761007\n",
            "Iteration 1221 - Batch 318/903 - Train loss: 3456051.8753918493\n",
            "Iteration 1222 - Batch 319/903 - Train loss: 3455776.14453125\n",
            "Iteration 1223 - Batch 320/903 - Train loss: 3456199.3325545173\n",
            "Iteration 1224 - Batch 321/903 - Train loss: 3455803.3967391304\n",
            "Iteration 1225 - Batch 322/903 - Train loss: 3455917.8235294116\n",
            "Iteration 1226 - Batch 323/903 - Train loss: 3456084.6875\n",
            "Iteration 1227 - Batch 324/903 - Train loss: 3454987.150769231\n",
            "Iteration 1228 - Batch 325/903 - Train loss: 3454207.444785276\n",
            "Iteration 1229 - Batch 326/903 - Train loss: 3452942.8379204893\n",
            "Iteration 1230 - Batch 327/903 - Train loss: 3452358.6608231706\n",
            "Iteration 1231 - Batch 328/903 - Train loss: 3451653.679331307\n",
            "Iteration 1232 - Batch 329/903 - Train loss: 3450690.70530303\n",
            "Iteration 1233 - Batch 330/903 - Train loss: 3449637.920694864\n",
            "Iteration 1234 - Batch 331/903 - Train loss: 3449556.9600903615\n",
            "Iteration 1235 - Batch 332/903 - Train loss: 3450794.9984984985\n",
            "Iteration 1236 - Batch 333/903 - Train loss: 3450028.233532934\n",
            "Iteration 1237 - Batch 334/903 - Train loss: 3448978.21119403\n",
            "Iteration 1238 - Batch 335/903 - Train loss: 3447691.0729166665\n",
            "Iteration 1239 - Batch 336/903 - Train loss: 3447734.915430267\n",
            "Iteration 1240 - Batch 337/903 - Train loss: 3447290.4223372783\n",
            "Iteration 1241 - Batch 338/903 - Train loss: 3447277.883480826\n",
            "Iteration 1242 - Batch 339/903 - Train loss: 3447076.470588235\n",
            "Iteration 1243 - Batch 340/903 - Train loss: 3445784.5535190618\n",
            "Iteration 1244 - Batch 341/903 - Train loss: 3444537.989766082\n",
            "Iteration 1245 - Batch 342/903 - Train loss: 3444208.2193877553\n",
            "Iteration 1246 - Batch 343/903 - Train loss: 3443996.835755814\n",
            "Iteration 1247 - Batch 344/903 - Train loss: 3442963.1934782607\n",
            "Iteration 1248 - Batch 345/903 - Train loss: 3442160.386560694\n",
            "Iteration 1249 - Batch 346/903 - Train loss: 3441482.136887608\n",
            "Iteration 1250 - Batch 347/903 - Train loss: 3440636.047413793\n",
            "Iteration 1251 - Batch 348/903 - Train loss: 3439609.548710602\n",
            "Iteration 1252 - Batch 349/903 - Train loss: 3438968.007142857\n",
            "Iteration 1253 - Batch 350/903 - Train loss: 3437633.9344729343\n",
            "Iteration 1254 - Batch 351/903 - Train loss: 3436894.0056818184\n",
            "Iteration 1255 - Batch 352/903 - Train loss: 3436921.2797450423\n",
            "Iteration 1256 - Batch 353/903 - Train loss: 3436844.890536723\n",
            "Iteration 1257 - Batch 354/903 - Train loss: 3435923.0584507044\n",
            "Iteration 1258 - Batch 355/903 - Train loss: 3434207.51755618\n",
            "Iteration 1259 - Batch 356/903 - Train loss: 3433408.2535014004\n",
            "Iteration 1260 - Batch 357/903 - Train loss: 3432484.3303072625\n",
            "Iteration 1261 - Batch 358/903 - Train loss: 3432149.266713092\n",
            "Iteration 1262 - Batch 359/903 - Train loss: 3430688.206944444\n",
            "Iteration 1263 - Batch 360/903 - Train loss: 3429920.1551246536\n",
            "Iteration 1264 - Batch 361/903 - Train loss: 3429062.664364641\n",
            "Iteration 1265 - Batch 362/903 - Train loss: 3428305.1756198346\n",
            "Iteration 1266 - Batch 363/903 - Train loss: 3427492.6978021977\n",
            "Iteration 1267 - Batch 364/903 - Train loss: 3426644.184931507\n",
            "Iteration 1268 - Batch 365/903 - Train loss: 3425457.2540983604\n",
            "Iteration 1269 - Batch 366/903 - Train loss: 3424257.851498638\n",
            "Iteration 1270 - Batch 367/903 - Train loss: 3423802.2934782607\n",
            "Iteration 1271 - Batch 368/903 - Train loss: 3422989.4017615174\n",
            "Iteration 1272 - Batch 369/903 - Train loss: 3422444.1736486484\n",
            "Iteration 1273 - Batch 370/903 - Train loss: 3421993.802560647\n",
            "Iteration 1274 - Batch 371/903 - Train loss: 3420479.0907258065\n",
            "Iteration 1275 - Batch 372/903 - Train loss: 3419378.3820375334\n",
            "Iteration 1276 - Batch 373/903 - Train loss: 3419392.862299465\n",
            "Iteration 1277 - Batch 374/903 - Train loss: 3418407.2586666667\n",
            "Iteration 1278 - Batch 375/903 - Train loss: 3418344.366356383\n",
            "Iteration 1279 - Batch 376/903 - Train loss: 3418486.5344827585\n",
            "Iteration 1280 - Batch 377/903 - Train loss: 3418119.787037037\n",
            "Iteration 1281 - Batch 378/903 - Train loss: 3417302.8331134566\n",
            "Iteration 1282 - Batch 379/903 - Train loss: 3416567.6236842107\n",
            "Iteration 1283 - Batch 380/903 - Train loss: 3416185.677821522\n",
            "Iteration 1284 - Batch 381/903 - Train loss: 3415233.17539267\n",
            "Iteration 1285 - Batch 382/903 - Train loss: 3415213.2734986944\n",
            "Iteration 1286 - Batch 383/903 - Train loss: 3415680.8470052085\n",
            "Iteration 1287 - Batch 384/903 - Train loss: 3416314.837662338\n",
            "Iteration 1288 - Batch 385/903 - Train loss: 3416067.6988341967\n",
            "Iteration 1289 - Batch 386/903 - Train loss: 3415267.5568475453\n",
            "Iteration 1290 - Batch 387/903 - Train loss: 3414682.132087629\n",
            "Iteration 1291 - Batch 388/903 - Train loss: 3414830.9832904884\n",
            "Iteration 1292 - Batch 389/903 - Train loss: 3414539.941666667\n",
            "Iteration 1293 - Batch 390/903 - Train loss: 3414355.9040920716\n",
            "Iteration 1294 - Batch 391/903 - Train loss: 3413855.8730867347\n",
            "Iteration 1295 - Batch 392/903 - Train loss: 3412670.411577608\n",
            "Iteration 1296 - Batch 393/903 - Train loss: 3412094.54822335\n",
            "Iteration 1297 - Batch 394/903 - Train loss: 3411201.4632911393\n",
            "Iteration 1298 - Batch 395/903 - Train loss: 3411438.860479798\n",
            "Iteration 1299 - Batch 396/903 - Train loss: 3410250.659949622\n",
            "Iteration 1300 - Batch 397/903 - Train loss: 3409756.0395728643\n",
            "Iteration 1301 - Batch 398/903 - Train loss: 3409645.1365914787\n",
            "Iteration 1302 - Batch 399/903 - Train loss: 3409370.618125\n",
            "Iteration 1303 - Batch 400/903 - Train loss: 3408682.974438903\n",
            "Iteration 1304 - Batch 401/903 - Train loss: 3408001.4589552237\n",
            "Iteration 1305 - Batch 402/903 - Train loss: 3407242.0359801487\n",
            "Iteration 1306 - Batch 403/903 - Train loss: 3406941.6800742573\n",
            "Iteration 1307 - Batch 404/903 - Train loss: 3405692.2098765434\n",
            "Iteration 1308 - Batch 405/903 - Train loss: 3404781.578817734\n",
            "Iteration 1309 - Batch 406/903 - Train loss: 3404983.528869779\n",
            "Iteration 1310 - Batch 407/903 - Train loss: 3404668.5226715687\n",
            "Iteration 1311 - Batch 408/903 - Train loss: 3404267.19804401\n",
            "Iteration 1312 - Batch 409/903 - Train loss: 3403854.5347560975\n",
            "Iteration 1313 - Batch 410/903 - Train loss: 3402492.865571776\n",
            "Iteration 1314 - Batch 411/903 - Train loss: 3402378.6674757283\n",
            "Iteration 1315 - Batch 412/903 - Train loss: 3401887.204600484\n",
            "Iteration 1316 - Batch 413/903 - Train loss: 3401925.8266908214\n",
            "Iteration 1317 - Batch 414/903 - Train loss: 3401474.4240963855\n",
            "Iteration 1318 - Batch 415/903 - Train loss: 3400068.418269231\n",
            "Iteration 1319 - Batch 416/903 - Train loss: 3399090.8776978417\n",
            "Iteration 1320 - Batch 417/903 - Train loss: 3399497.8157894737\n",
            "Iteration 1321 - Batch 418/903 - Train loss: 3399265.512529833\n",
            "Iteration 1322 - Batch 419/903 - Train loss: 3398463.714285714\n",
            "Iteration 1323 - Batch 420/903 - Train loss: 3396879.2203087886\n",
            "Iteration 1324 - Batch 421/903 - Train loss: 3395869.489336493\n",
            "Iteration 1325 - Batch 422/903 - Train loss: 3395234.862293144\n",
            "Iteration 1326 - Batch 423/903 - Train loss: 3394705.453419811\n",
            "Iteration 1327 - Batch 424/903 - Train loss: 3393238.798235294\n",
            "Iteration 1328 - Batch 425/903 - Train loss: 3392622.8526995303\n",
            "Iteration 1329 - Batch 426/903 - Train loss: 3391831.4098360655\n",
            "Iteration 1330 - Batch 427/903 - Train loss: 3391323.287383178\n",
            "Iteration 1331 - Batch 428/903 - Train loss: 3391305.53962704\n",
            "Iteration 1332 - Batch 429/903 - Train loss: 3391076.3110465114\n",
            "Iteration 1333 - Batch 430/903 - Train loss: 3390271.5725058005\n",
            "Iteration 1334 - Batch 431/903 - Train loss: 3389313.030671296\n",
            "Iteration 1335 - Batch 432/903 - Train loss: 3388355.8140877597\n",
            "Iteration 1336 - Batch 433/903 - Train loss: 3388144.318548387\n",
            "Iteration 1337 - Batch 434/903 - Train loss: 3387378.5166666666\n",
            "Iteration 1338 - Batch 435/903 - Train loss: 3386908.5086009176\n",
            "Iteration 1339 - Batch 436/903 - Train loss: 3386052.4891304346\n",
            "Iteration 1340 - Batch 437/903 - Train loss: 3385659.2928082193\n",
            "Iteration 1341 - Batch 438/903 - Train loss: 3384377.3354214123\n",
            "Iteration 1342 - Batch 439/903 - Train loss: 3384136.3397727273\n",
            "Iteration 1343 - Batch 440/903 - Train loss: 3384551.7494331067\n",
            "Iteration 1344 - Batch 441/903 - Train loss: 3385114.3178733033\n",
            "Iteration 1345 - Batch 442/903 - Train loss: 3385476.7945823926\n",
            "Iteration 1346 - Batch 443/903 - Train loss: 3385239.953828829\n",
            "Iteration 1347 - Batch 444/903 - Train loss: 3384868.4421348316\n",
            "Iteration 1348 - Batch 445/903 - Train loss: 3385393.8621076234\n",
            "Iteration 1349 - Batch 446/903 - Train loss: 3385972.2315436243\n",
            "Iteration 1350 - Batch 447/903 - Train loss: 3385782.5206473214\n",
            "Iteration 1351 - Batch 448/903 - Train loss: 3385770.574053452\n",
            "Iteration 1352 - Batch 449/903 - Train loss: 3385844.9744444443\n",
            "Iteration 1353 - Batch 450/903 - Train loss: 3385837.9035476716\n",
            "Iteration 1354 - Batch 451/903 - Train loss: 3385947.6686946903\n",
            "Iteration 1355 - Batch 452/903 - Train loss: 3385825.5298013245\n",
            "Iteration 1356 - Batch 453/903 - Train loss: 3386422.2230176213\n",
            "Iteration 1357 - Batch 454/903 - Train loss: 3386695.6587912086\n",
            "Iteration 1358 - Batch 455/903 - Train loss: 3385758.5888157897\n",
            "Iteration 1359 - Batch 456/903 - Train loss: 3385956.612691466\n",
            "Iteration 1360 - Batch 457/903 - Train loss: 3386579.6517467247\n",
            "Iteration 1361 - Batch 458/903 - Train loss: 3387046.6786492374\n",
            "Iteration 1362 - Batch 459/903 - Train loss: 3386201.3679347825\n",
            "Iteration 1363 - Batch 460/903 - Train loss: 3386167.867678959\n",
            "Iteration 1364 - Batch 461/903 - Train loss: 3386382.6233766233\n",
            "Iteration 1365 - Batch 462/903 - Train loss: 3385798.3185745142\n",
            "Iteration 1366 - Batch 463/903 - Train loss: 3385349.646012931\n",
            "Iteration 1367 - Batch 464/903 - Train loss: 3384720.3715053764\n",
            "Iteration 1368 - Batch 465/903 - Train loss: 3384389.8519313303\n",
            "Iteration 1369 - Batch 466/903 - Train loss: 3384245.408993576\n",
            "Iteration 1370 - Batch 467/903 - Train loss: 3383273.5865384615\n",
            "Iteration 1371 - Batch 468/903 - Train loss: 3383641.2995735607\n",
            "Iteration 1372 - Batch 469/903 - Train loss: 3383127.870744681\n",
            "Iteration 1373 - Batch 470/903 - Train loss: 3382445.3874734608\n",
            "Iteration 1374 - Batch 471/903 - Train loss: 3381622.696504237\n",
            "Iteration 1375 - Batch 472/903 - Train loss: 3381193.2071881606\n",
            "Iteration 1376 - Batch 473/903 - Train loss: 3380603.4161392404\n",
            "Iteration 1377 - Batch 474/903 - Train loss: 3379902.0415789476\n",
            "Iteration 1378 - Batch 475/903 - Train loss: 3380099.669117647\n",
            "Iteration 1379 - Batch 476/903 - Train loss: 3379348.7348008384\n",
            "Iteration 1380 - Batch 477/903 - Train loss: 3378891.4361924687\n",
            "Iteration 1381 - Batch 478/903 - Train loss: 3378288.0657620043\n",
            "Iteration 1382 - Batch 479/903 - Train loss: 3377419.03125\n",
            "Iteration 1383 - Batch 480/903 - Train loss: 3376983.581600832\n",
            "Iteration 1384 - Batch 481/903 - Train loss: 3376461.768672199\n",
            "Iteration 1385 - Batch 482/903 - Train loss: 3375874.975672878\n",
            "Iteration 1386 - Batch 483/903 - Train loss: 3375096.1420454546\n",
            "Iteration 1387 - Batch 484/903 - Train loss: 3374539.4422680414\n",
            "Iteration 1388 - Batch 485/903 - Train loss: 3373562.8986625513\n",
            "Iteration 1389 - Batch 486/903 - Train loss: 3373144.274640657\n",
            "Iteration 1390 - Batch 487/903 - Train loss: 3372144.7269467213\n",
            "Iteration 1391 - Batch 488/903 - Train loss: 3371638.2101226994\n",
            "Iteration 1392 - Batch 489/903 - Train loss: 3370734.388265306\n",
            "Iteration 1393 - Batch 490/903 - Train loss: 3369495.334521385\n",
            "Iteration 1394 - Batch 491/903 - Train loss: 3369146.1092479676\n",
            "Iteration 1395 - Batch 492/903 - Train loss: 3368661.5922920895\n",
            "Iteration 1396 - Batch 493/903 - Train loss: 3367751.3071862347\n",
            "Iteration 1397 - Batch 494/903 - Train loss: 3366943.403030303\n",
            "Iteration 1398 - Batch 495/903 - Train loss: 3366215.4430443547\n",
            "Iteration 1399 - Batch 496/903 - Train loss: 3365883.3938631793\n",
            "Iteration 1400 - Batch 497/903 - Train loss: 3365383.449799197\n",
            "Iteration 1401 - Batch 498/903 - Train loss: 3364801.2454909817\n",
            "Iteration 1402 - Batch 499/903 - Train loss: 3364097.894\n",
            "Iteration 1403 - Batch 500/903 - Train loss: 3363046.3468063874\n",
            "Iteration 1404 - Batch 501/903 - Train loss: 3362093.425796813\n",
            "Iteration 1405 - Batch 502/903 - Train loss: 3361225.7952286284\n",
            "Iteration 1406 - Batch 503/903 - Train loss: 3359881.7822420634\n",
            "Iteration 1407 - Batch 504/903 - Train loss: 3359064.3539603963\n",
            "Iteration 1408 - Batch 505/903 - Train loss: 3358357.4061264824\n",
            "Iteration 1409 - Batch 506/903 - Train loss: 3358098.680473373\n",
            "Iteration 1410 - Batch 507/903 - Train loss: 3357674.43257874\n",
            "Iteration 1411 - Batch 508/903 - Train loss: 3356819.007367387\n",
            "Iteration 1412 - Batch 509/903 - Train loss: 3356058.6715686275\n",
            "Iteration 1413 - Batch 510/903 - Train loss: 3355130.9134050882\n",
            "Iteration 1414 - Batch 511/903 - Train loss: 3354515.1020507812\n",
            "Iteration 1415 - Batch 512/903 - Train loss: 3353678.921539961\n",
            "Iteration 1416 - Batch 513/903 - Train loss: 3352693.8321984434\n",
            "Iteration 1417 - Batch 514/903 - Train loss: 3351714.7053398057\n",
            "Iteration 1418 - Batch 515/903 - Train loss: 3350999.551356589\n",
            "Iteration 1419 - Batch 516/903 - Train loss: 3350189.7572533847\n",
            "Iteration 1420 - Batch 517/903 - Train loss: 3350110.570945946\n",
            "Iteration 1421 - Batch 518/903 - Train loss: 3349305.554431599\n",
            "Iteration 1422 - Batch 519/903 - Train loss: 3348442.394230769\n",
            "Iteration 1423 - Batch 520/903 - Train loss: 3347379.917466411\n",
            "Iteration 1424 - Batch 521/903 - Train loss: 3347752.014367816\n",
            "Iteration 1425 - Batch 522/903 - Train loss: 3347875.5157743786\n",
            "Iteration 1426 - Batch 523/903 - Train loss: 3348246.443225191\n",
            "Iteration 1427 - Batch 524/903 - Train loss: 3347888.2585714287\n",
            "Iteration 1428 - Batch 525/903 - Train loss: 3346772.016634981\n",
            "Iteration 1429 - Batch 526/903 - Train loss: 3346907.3562618596\n",
            "Iteration 1430 - Batch 527/903 - Train loss: 3346806.3650568184\n",
            "Iteration 1431 - Batch 528/903 - Train loss: 3346443.9735349715\n",
            "Iteration 1432 - Batch 529/903 - Train loss: 3345948.000471698\n",
            "Iteration 1433 - Batch 530/903 - Train loss: 3345682.006120527\n",
            "Iteration 1434 - Batch 531/903 - Train loss: 3345372.67481203\n",
            "Iteration 1435 - Batch 532/903 - Train loss: 3344433.340525328\n",
            "Iteration 1436 - Batch 533/903 - Train loss: 3344386.066011236\n",
            "Iteration 1437 - Batch 534/903 - Train loss: 3343918.81728972\n",
            "Iteration 1438 - Batch 535/903 - Train loss: 3344167.5083955224\n",
            "Iteration 1439 - Batch 536/903 - Train loss: 3343712.4250465548\n",
            "Iteration 1440 - Batch 537/903 - Train loss: 3342790.270910781\n",
            "Iteration 1441 - Batch 538/903 - Train loss: 3341959.9935064935\n",
            "Iteration 1442 - Batch 539/903 - Train loss: 3342276.9625\n",
            "Iteration 1443 - Batch 540/903 - Train loss: 3341785.2994454713\n",
            "Iteration 1444 - Batch 541/903 - Train loss: 3341208.0922509227\n",
            "Iteration 1445 - Batch 542/903 - Train loss: 3340639.859116022\n",
            "Iteration 1446 - Batch 543/903 - Train loss: 3339642.3970588236\n",
            "Iteration 1447 - Batch 544/903 - Train loss: 3339196.185321101\n",
            "Iteration 1448 - Batch 545/903 - Train loss: 3338695.622710623\n",
            "Iteration 1449 - Batch 546/903 - Train loss: 3338438.8756855577\n",
            "Iteration 1450 - Batch 547/903 - Train loss: 3338037.1564781023\n",
            "Iteration 1451 - Batch 548/903 - Train loss: 3338002.1443533697\n",
            "Iteration 1452 - Batch 549/903 - Train loss: 3337266.6945454543\n",
            "Iteration 1453 - Batch 550/903 - Train loss: 3336947.9287658804\n",
            "Iteration 1454 - Batch 551/903 - Train loss: 3336651.357789855\n",
            "Iteration 1455 - Batch 552/903 - Train loss: 3336730.5316455695\n",
            "Iteration 1456 - Batch 553/903 - Train loss: 3335800.3533393503\n",
            "Iteration 1457 - Batch 554/903 - Train loss: 3335397.9963963963\n",
            "Iteration 1458 - Batch 555/903 - Train loss: 3334541.195143885\n",
            "Iteration 1459 - Batch 556/903 - Train loss: 3333752.31956912\n",
            "Iteration 1460 - Batch 557/903 - Train loss: 3333730.3086917563\n",
            "Iteration 1461 - Batch 558/903 - Train loss: 3332949.0424865833\n",
            "Iteration 1462 - Batch 559/903 - Train loss: 3332043.600892857\n",
            "Iteration 1463 - Batch 560/903 - Train loss: 3331864.688502674\n",
            "Iteration 1464 - Batch 561/903 - Train loss: 3331398.149021352\n",
            "Iteration 1465 - Batch 562/903 - Train loss: 3330632.6940497337\n",
            "Iteration 1466 - Batch 563/903 - Train loss: 3329988.0975177307\n",
            "Iteration 1467 - Batch 564/903 - Train loss: 3329549.319026549\n",
            "Iteration 1468 - Batch 565/903 - Train loss: 3329016.008392226\n",
            "Iteration 1469 - Batch 566/903 - Train loss: 3328916.7874779543\n",
            "Iteration 1470 - Batch 567/903 - Train loss: 3328259.3890845072\n",
            "Iteration 1471 - Batch 568/903 - Train loss: 3327810.0645869947\n",
            "Iteration 1472 - Batch 569/903 - Train loss: 3327625.1978070177\n",
            "Iteration 1473 - Batch 570/903 - Train loss: 3327025.7767075305\n",
            "Iteration 1474 - Batch 571/903 - Train loss: 3326837.7036713287\n",
            "Iteration 1475 - Batch 572/903 - Train loss: 3326434.5990401395\n",
            "Iteration 1476 - Batch 573/903 - Train loss: 3325811.0910278745\n",
            "Iteration 1477 - Batch 574/903 - Train loss: 3324845.522173913\n",
            "Iteration 1478 - Batch 575/903 - Train loss: 3324305.5633680555\n",
            "Iteration 1479 - Batch 576/903 - Train loss: 3323614.6702772966\n",
            "Iteration 1480 - Batch 577/903 - Train loss: 3322463.8434256054\n",
            "Iteration 1481 - Batch 578/903 - Train loss: 3321528.262521589\n",
            "Iteration 1482 - Batch 579/903 - Train loss: 3320615.9685344826\n",
            "Iteration 1483 - Batch 580/903 - Train loss: 3320340.739242685\n",
            "Iteration 1484 - Batch 581/903 - Train loss: 3319529.4720790377\n",
            "Iteration 1485 - Batch 582/903 - Train loss: 3318939.904373928\n",
            "Iteration 1486 - Batch 583/903 - Train loss: 3318192.679366438\n",
            "Iteration 1487 - Batch 584/903 - Train loss: 3317904.929059829\n",
            "Iteration 1488 - Batch 585/903 - Train loss: 3317666.6770477816\n",
            "Iteration 1489 - Batch 586/903 - Train loss: 3317093.2504258943\n",
            "Iteration 1490 - Batch 587/903 - Train loss: 3316373.2100340137\n",
            "Iteration 1491 - Batch 588/903 - Train loss: 3315700.0560271647\n",
            "Iteration 1492 - Batch 589/903 - Train loss: 3314912.6169491527\n",
            "Iteration 1493 - Batch 590/903 - Train loss: 3314083.8773265653\n",
            "Iteration 1494 - Batch 591/903 - Train loss: 3313733.8488175673\n",
            "Iteration 1495 - Batch 592/903 - Train loss: 3314001.9097807757\n",
            "Iteration 1496 - Batch 593/903 - Train loss: 3313848.034090909\n",
            "Iteration 1497 - Batch 594/903 - Train loss: 3313300.0210084035\n",
            "Iteration 1498 - Batch 595/903 - Train loss: 3312702.7424496645\n",
            "Iteration 1499 - Batch 596/903 - Train loss: 3311480.829564489\n",
            "Iteration 1500 - Batch 597/903 - Train loss: 3310940.04138796\n",
            "Iteration 1501 - Batch 598/903 - Train loss: 3310348.648163606\n",
            "Iteration 1502 - Batch 599/903 - Train loss: 3310282.33875\n",
            "Iteration 1503 - Batch 600/903 - Train loss: 3309403.7624792014\n",
            "Iteration 1504 - Batch 601/903 - Train loss: 3308769.0299003324\n",
            "Iteration 1505 - Batch 602/903 - Train loss: 3308155.6036484246\n",
            "Iteration 1506 - Batch 603/903 - Train loss: 3307698.213576159\n",
            "Iteration 1507 - Batch 604/903 - Train loss: 3307056.831404959\n",
            "Iteration 1508 - Batch 605/903 - Train loss: 3306550.6064356435\n",
            "Iteration 1509 - Batch 606/903 - Train loss: 3305700.142504119\n",
            "Iteration 1510 - Batch 607/903 - Train loss: 3304953.367598684\n",
            "Iteration 1511 - Batch 608/903 - Train loss: 3304379.4634646964\n",
            "Iteration 1512 - Batch 609/903 - Train loss: 3304072.1086065574\n",
            "Iteration 1513 - Batch 610/903 - Train loss: 3303908.4357610475\n",
            "Iteration 1514 - Batch 611/903 - Train loss: 3303441.9142156863\n",
            "Iteration 1515 - Batch 612/903 - Train loss: 3303127.5179445352\n",
            "Iteration 1516 - Batch 613/903 - Train loss: 3301921.552117264\n",
            "Iteration 1517 - Batch 614/903 - Train loss: 3301487.711382114\n",
            "Iteration 1518 - Batch 615/903 - Train loss: 3300977.627840909\n",
            "Iteration 1519 - Batch 616/903 - Train loss: 3300158.0611831443\n",
            "Iteration 1520 - Batch 617/903 - Train loss: 3299398.039239482\n",
            "Iteration 1521 - Batch 618/903 - Train loss: 3299137.8897415185\n",
            "Iteration 1522 - Batch 619/903 - Train loss: 3298777.1387096774\n",
            "Iteration 1523 - Batch 620/903 - Train loss: 3298369.2194041866\n",
            "Iteration 1524 - Batch 621/903 - Train loss: 3297839.584807074\n",
            "Iteration 1525 - Batch 622/903 - Train loss: 3297394.1492776885\n",
            "Iteration 1526 - Batch 623/903 - Train loss: 3297162.4887820515\n",
            "Iteration 1527 - Batch 624/903 - Train loss: 3296944.502\n",
            "Iteration 1528 - Batch 625/903 - Train loss: 3296659.9900159743\n",
            "Iteration 1529 - Batch 626/903 - Train loss: 3296799.2093301434\n",
            "Iteration 1530 - Batch 627/903 - Train loss: 3296598.245621019\n",
            "Iteration 1531 - Batch 628/903 - Train loss: 3296620.9996025437\n",
            "Iteration 1532 - Batch 629/903 - Train loss: 3296366.7115079365\n",
            "Iteration 1533 - Batch 630/903 - Train loss: 3295968.5863708397\n",
            "Iteration 1534 - Batch 631/903 - Train loss: 3295646.6036392404\n",
            "Iteration 1535 - Batch 632/903 - Train loss: 3295391.5327804107\n",
            "Iteration 1536 - Batch 633/903 - Train loss: 3294982.475157729\n",
            "Iteration 1537 - Batch 634/903 - Train loss: 3294625.407480315\n",
            "Iteration 1538 - Batch 635/903 - Train loss: 3293937.3258647798\n",
            "Iteration 1539 - Batch 636/903 - Train loss: 3293332.8810832025\n",
            "Iteration 1540 - Batch 637/903 - Train loss: 3292754.1637931033\n",
            "Iteration 1541 - Batch 638/903 - Train loss: 3292437.798513302\n",
            "Iteration 1542 - Batch 639/903 - Train loss: 3291868.00703125\n",
            "Iteration 1543 - Batch 640/903 - Train loss: 3291695.627145086\n",
            "Iteration 1544 - Batch 641/903 - Train loss: 3291294.015576324\n",
            "Iteration 1545 - Batch 642/903 - Train loss: 3290371.080482115\n",
            "Iteration 1546 - Batch 643/903 - Train loss: 3289519.107919255\n",
            "Iteration 1547 - Batch 644/903 - Train loss: 3288944.641860465\n",
            "Iteration 1548 - Batch 645/903 - Train loss: 3287900.669117647\n",
            "Iteration 1549 - Batch 646/903 - Train loss: 3287138.728748068\n",
            "Iteration 1550 - Batch 647/903 - Train loss: 3286149.649691358\n",
            "Iteration 1551 - Batch 648/903 - Train loss: 3285304.342835131\n",
            "Iteration 1552 - Batch 649/903 - Train loss: 3284359.7853846154\n",
            "Iteration 1553 - Batch 650/903 - Train loss: 3283758.470046083\n",
            "Iteration 1554 - Batch 651/903 - Train loss: 3283112.3907208587\n",
            "Iteration 1555 - Batch 652/903 - Train loss: 3282685.295558959\n",
            "Iteration 1556 - Batch 653/903 - Train loss: 3281903.2339449544\n",
            "Iteration 1557 - Batch 654/903 - Train loss: 3281273.6416030535\n",
            "Iteration 1558 - Batch 655/903 - Train loss: 3280614.8849085364\n",
            "Iteration 1559 - Batch 656/903 - Train loss: 3279888.028538813\n",
            "Iteration 1560 - Batch 657/903 - Train loss: 3279650.3537234045\n",
            "Iteration 1561 - Batch 658/903 - Train loss: 3279463.5037936266\n",
            "Iteration 1562 - Batch 659/903 - Train loss: 3279059.024621212\n",
            "Iteration 1563 - Batch 660/903 - Train loss: 3278761.7008320726\n",
            "Iteration 1564 - Batch 661/903 - Train loss: 3278402.537386707\n",
            "Iteration 1565 - Batch 662/903 - Train loss: 3278372.467571644\n",
            "Iteration 1566 - Batch 663/903 - Train loss: 3277825.2466114457\n",
            "Iteration 1567 - Batch 664/903 - Train loss: 3276947.805263158\n",
            "Iteration 1568 - Batch 665/903 - Train loss: 3276467.993993994\n",
            "Iteration 1569 - Batch 666/903 - Train loss: 3275988.4985007495\n",
            "Iteration 1570 - Batch 667/903 - Train loss: 3275396.7638473054\n",
            "Iteration 1571 - Batch 668/903 - Train loss: 3274912.922272048\n",
            "Iteration 1572 - Batch 669/903 - Train loss: 3274416.331716418\n",
            "Iteration 1573 - Batch 670/903 - Train loss: 3274246.6736214603\n",
            "Iteration 1574 - Batch 671/903 - Train loss: 3273772.1398809524\n",
            "Iteration 1575 - Batch 672/903 - Train loss: 3273480.13410104\n",
            "Iteration 1576 - Batch 673/903 - Train loss: 3273121.0459940652\n",
            "Iteration 1577 - Batch 674/903 - Train loss: 3272653.408888889\n",
            "Iteration 1578 - Batch 675/903 - Train loss: 3272255.2259615385\n",
            "Iteration 1579 - Batch 676/903 - Train loss: 3271513.172082718\n",
            "Iteration 1580 - Batch 677/903 - Train loss: 3270926.3222713866\n",
            "Iteration 1581 - Batch 678/903 - Train loss: 3270382.768777614\n",
            "Iteration 1582 - Batch 679/903 - Train loss: 3269841.011764706\n",
            "Iteration 1583 - Batch 680/903 - Train loss: 3269037.4335535974\n",
            "Iteration 1584 - Batch 681/903 - Train loss: 3268203.836876833\n",
            "Iteration 1585 - Batch 682/903 - Train loss: 3267535.048682284\n",
            "Iteration 1586 - Batch 683/903 - Train loss: 3267159.2609649124\n",
            "Iteration 1587 - Batch 684/903 - Train loss: 3266730.6718978104\n",
            "Iteration 1588 - Batch 685/903 - Train loss: 3266407.975947522\n",
            "Iteration 1589 - Batch 686/903 - Train loss: 3265898.906113537\n",
            "Iteration 1590 - Batch 687/903 - Train loss: 3265429.5363372094\n",
            "Iteration 1591 - Batch 688/903 - Train loss: 3265167.1846879534\n",
            "Iteration 1592 - Batch 689/903 - Train loss: 3264349.624275362\n",
            "Iteration 1593 - Batch 690/903 - Train loss: 3263872.6157742403\n",
            "Iteration 1594 - Batch 691/903 - Train loss: 3263361.0578034683\n",
            "Iteration 1595 - Batch 692/903 - Train loss: 3263025.940836941\n",
            "Iteration 1596 - Batch 693/903 - Train loss: 3263299.629322767\n",
            "Iteration 1597 - Batch 694/903 - Train loss: 3263641.7370503596\n",
            "Iteration 1598 - Batch 695/903 - Train loss: 3264420.5754310344\n",
            "Iteration 1599 - Batch 696/903 - Train loss: 3265617.9092539456\n",
            "Iteration 1600 - Batch 697/903 - Train loss: 3267071.1285816617\n",
            "Iteration 1601 - Batch 698/903 - Train loss: 3267925.668454936\n",
            "Iteration 1602 - Batch 699/903 - Train loss: 3267514.907142857\n",
            "Iteration 1603 - Batch 700/903 - Train loss: 3266990.552068474\n",
            "Iteration 1604 - Batch 701/903 - Train loss: 3266743.157763533\n",
            "Iteration 1605 - Batch 702/903 - Train loss: 3266879.7510668566\n",
            "Iteration 1606 - Batch 703/903 - Train loss: 3266085.9925426138\n",
            "Iteration 1607 - Batch 704/903 - Train loss: 3265812.5432624114\n",
            "Iteration 1608 - Batch 705/903 - Train loss: 3266513.2705382435\n",
            "Iteration 1609 - Batch 706/903 - Train loss: 3266221.533592645\n",
            "Iteration 1610 - Batch 707/903 - Train loss: 3266080.467161017\n",
            "Iteration 1611 - Batch 708/903 - Train loss: 3266397.0997884343\n",
            "Iteration 1612 - Batch 709/903 - Train loss: 3266110.9235915495\n",
            "Iteration 1613 - Batch 710/903 - Train loss: 3265930.70464135\n",
            "Iteration 1614 - Batch 711/903 - Train loss: 3265461.150280899\n",
            "Iteration 1615 - Batch 712/903 - Train loss: 3264850.162692847\n",
            "Iteration 1616 - Batch 713/903 - Train loss: 3264481.7955182074\n",
            "Iteration 1617 - Batch 714/903 - Train loss: 3264121.184265734\n",
            "Iteration 1618 - Batch 715/903 - Train loss: 3263804.5143156424\n",
            "Iteration 1619 - Batch 716/903 - Train loss: 3263681.6457461645\n",
            "Iteration 1620 - Batch 717/903 - Train loss: 3263482.8635097495\n",
            "Iteration 1621 - Batch 718/903 - Train loss: 3262982.2315716273\n",
            "Iteration 1622 - Batch 719/903 - Train loss: 3262942.367013889\n",
            "Iteration 1623 - Batch 720/903 - Train loss: 3262635.4993065186\n",
            "Iteration 1624 - Batch 721/903 - Train loss: 3261793.4522160664\n",
            "Iteration 1625 - Batch 722/903 - Train loss: 3261513.92219917\n",
            "Iteration 1626 - Batch 723/903 - Train loss: 3261230.1398480665\n",
            "Iteration 1627 - Batch 724/903 - Train loss: 3260857.9713793104\n",
            "Iteration 1628 - Batch 725/903 - Train loss: 3260332.754476584\n",
            "Iteration 1629 - Batch 726/903 - Train loss: 3259954.9346629987\n",
            "Iteration 1630 - Batch 727/903 - Train loss: 3259531.826923077\n",
            "Iteration 1631 - Batch 728/903 - Train loss: 3259384.670096022\n",
            "Iteration 1632 - Batch 729/903 - Train loss: 3259267.2626712327\n",
            "Iteration 1633 - Batch 730/903 - Train loss: 3258911.7089603283\n",
            "Iteration 1634 - Batch 731/903 - Train loss: 3258672.8227459015\n",
            "Iteration 1635 - Batch 732/903 - Train loss: 3258116.925648022\n",
            "Iteration 1636 - Batch 733/903 - Train loss: 3257594.591280654\n",
            "Iteration 1637 - Batch 734/903 - Train loss: 3257206.9238095237\n",
            "Iteration 1638 - Batch 735/903 - Train loss: 3256642.4487092393\n",
            "Iteration 1639 - Batch 736/903 - Train loss: 3256354.3113975576\n",
            "Iteration 1640 - Batch 737/903 - Train loss: 3256074.5450542006\n",
            "Iteration 1641 - Batch 738/903 - Train loss: 3255686.7916102842\n",
            "Iteration 1642 - Batch 739/903 - Train loss: 3255167.4875\n",
            "Iteration 1643 - Batch 740/903 - Train loss: 3254207.4163292847\n",
            "Iteration 1644 - Batch 741/903 - Train loss: 3253939.793463612\n",
            "Iteration 1645 - Batch 742/903 - Train loss: 3253459.5198519514\n",
            "Iteration 1646 - Batch 743/903 - Train loss: 3253313.955981183\n",
            "Iteration 1647 - Batch 744/903 - Train loss: 3252691.7802013424\n",
            "Iteration 1648 - Batch 745/903 - Train loss: 3252235.6152815013\n",
            "Iteration 1649 - Batch 746/903 - Train loss: 3251924.1472556894\n",
            "Iteration 1650 - Batch 747/903 - Train loss: 3251567.0755347596\n",
            "Iteration 1651 - Batch 748/903 - Train loss: 3251329.400867824\n",
            "Iteration 1652 - Batch 749/903 - Train loss: 3250981.7956666667\n",
            "Iteration 1653 - Batch 750/903 - Train loss: 3250338.7213715045\n",
            "Iteration 1654 - Batch 751/903 - Train loss: 3249757.4501329786\n",
            "Iteration 1655 - Batch 752/903 - Train loss: 3249383.2569721118\n",
            "Iteration 1656 - Batch 753/903 - Train loss: 3249053.9963527853\n",
            "Iteration 1657 - Batch 754/903 - Train loss: 3248859.126821192\n",
            "Iteration 1658 - Batch 755/903 - Train loss: 3248387.1458333335\n",
            "Iteration 1659 - Batch 756/903 - Train loss: 3248431.6908850726\n",
            "Iteration 1660 - Batch 757/903 - Train loss: 3247899.7691292875\n",
            "Iteration 1661 - Batch 758/903 - Train loss: 3247124.4808959155\n",
            "Iteration 1662 - Batch 759/903 - Train loss: 3246390.049013158\n",
            "Iteration 1663 - Batch 760/903 - Train loss: 3246121.714520368\n",
            "Iteration 1664 - Batch 761/903 - Train loss: 3245848.4153543306\n",
            "Iteration 1665 - Batch 762/903 - Train loss: 3245005.5717562255\n",
            "Iteration 1666 - Batch 763/903 - Train loss: 3244457.9345549736\n",
            "Iteration 1667 - Batch 764/903 - Train loss: 3243850.5689542484\n",
            "Iteration 1668 - Batch 765/903 - Train loss: 3243197.062989556\n",
            "Iteration 1669 - Batch 766/903 - Train loss: 3242595.862451108\n",
            "Iteration 1670 - Batch 767/903 - Train loss: 3242376.8037109375\n",
            "Iteration 1671 - Batch 768/903 - Train loss: 3242285.5962288687\n",
            "Iteration 1672 - Batch 769/903 - Train loss: 3242355.672077922\n",
            "Iteration 1673 - Batch 770/903 - Train loss: 3242421.649481193\n",
            "Iteration 1674 - Batch 771/903 - Train loss: 3241430.850064767\n",
            "Iteration 1675 - Batch 772/903 - Train loss: 3241140.314683053\n",
            "Iteration 1676 - Batch 773/903 - Train loss: 3241052.4958010335\n",
            "Iteration 1677 - Batch 774/903 - Train loss: 3240935.3512903224\n",
            "Iteration 1678 - Batch 775/903 - Train loss: 3240235.15818299\n",
            "Iteration 1679 - Batch 776/903 - Train loss: 3239974.8462033463\n",
            "Iteration 1680 - Batch 777/903 - Train loss: 3239604.4710796913\n",
            "Iteration 1681 - Batch 778/903 - Train loss: 3239341.023427471\n",
            "Iteration 1682 - Batch 779/903 - Train loss: 3239159.453525641\n",
            "Iteration 1683 - Batch 780/903 - Train loss: 3238567.7615236877\n",
            "Iteration 1684 - Batch 781/903 - Train loss: 3237915.2193094627\n",
            "Iteration 1685 - Batch 782/903 - Train loss: 3237537.0265006386\n",
            "Iteration 1686 - Batch 783/903 - Train loss: 3237655.913265306\n",
            "Iteration 1687 - Batch 784/903 - Train loss: 3237807.274522293\n",
            "Iteration 1688 - Batch 785/903 - Train loss: 3237124.842557252\n",
            "Iteration 1689 - Batch 786/903 - Train loss: 3236451.7728716647\n",
            "Iteration 1690 - Batch 787/903 - Train loss: 3236142.6941624368\n",
            "Iteration 1691 - Batch 788/903 - Train loss: 3236231.3852978456\n",
            "Iteration 1692 - Batch 789/903 - Train loss: 3236037.385759494\n",
            "Iteration 1693 - Batch 790/903 - Train loss: 3236818.6532869786\n",
            "Iteration 1694 - Batch 791/903 - Train loss: 3237134.569128788\n",
            "Iteration 1695 - Batch 792/903 - Train loss: 3236992.517969735\n",
            "Iteration 1696 - Batch 793/903 - Train loss: 3236782.34918136\n",
            "Iteration 1697 - Batch 794/903 - Train loss: 3236931.728301887\n",
            "Iteration 1698 - Batch 795/903 - Train loss: 3237130.774811558\n",
            "Iteration 1699 - Batch 796/903 - Train loss: 3236916.1737766624\n",
            "Iteration 1700 - Batch 797/903 - Train loss: 3236880.244047619\n",
            "Iteration 1701 - Batch 798/903 - Train loss: 3237340.53379224\n",
            "Iteration 1702 - Batch 799/903 - Train loss: 3237745.295\n",
            "Iteration 1703 - Batch 800/903 - Train loss: 3237566.5942571787\n",
            "Iteration 1704 - Batch 801/903 - Train loss: 3237592.9083541147\n",
            "Iteration 1705 - Batch 802/903 - Train loss: 3238372.622353674\n",
            "Iteration 1706 - Batch 803/903 - Train loss: 3239011.9698383086\n",
            "Iteration 1707 - Batch 804/903 - Train loss: 3238707.566770186\n",
            "Iteration 1708 - Batch 805/903 - Train loss: 3238544.95719603\n",
            "Iteration 1709 - Batch 806/903 - Train loss: 3238846.905204461\n",
            "Iteration 1710 - Batch 807/903 - Train loss: 3238718.926670792\n",
            "Iteration 1711 - Batch 808/903 - Train loss: 3238328.171817058\n",
            "Iteration 1712 - Batch 809/903 - Train loss: 3237991.365123457\n",
            "Iteration 1713 - Batch 810/903 - Train loss: 3237897.221948212\n",
            "Iteration 1714 - Batch 811/903 - Train loss: 3237652.4491995075\n",
            "Iteration 1715 - Batch 812/903 - Train loss: 3237380.4898523986\n",
            "Iteration 1716 - Batch 813/903 - Train loss: 3236950.0334766586\n",
            "Iteration 1717 - Batch 814/903 - Train loss: 3236728.0273006135\n",
            "Iteration 1718 - Batch 815/903 - Train loss: 3236408.186887255\n",
            "Iteration 1719 - Batch 816/903 - Train loss: 3235939.990208078\n",
            "Iteration 1720 - Batch 817/903 - Train loss: 3235325.1995721273\n",
            "Iteration 1721 - Batch 818/903 - Train loss: 3235054.412087912\n",
            "Iteration 1722 - Batch 819/903 - Train loss: 3234493.843292683\n",
            "Iteration 1723 - Batch 820/903 - Train loss: 3234101.677527406\n",
            "Iteration 1724 - Batch 821/903 - Train loss: 3233792.0921532847\n",
            "Iteration 1725 - Batch 822/903 - Train loss: 3233530.07290401\n",
            "Iteration 1726 - Batch 823/903 - Train loss: 3233093.744538835\n",
            "Iteration 1727 - Batch 824/903 - Train loss: 3232671.196969697\n",
            "Iteration 1728 - Batch 825/903 - Train loss: 3232289.0553874094\n",
            "Iteration 1729 - Batch 826/903 - Train loss: 3231744.837968561\n",
            "Iteration 1730 - Batch 827/903 - Train loss: 3231235.1174516906\n",
            "Iteration 1731 - Batch 828/903 - Train loss: 3230777.9137515076\n",
            "Iteration 1732 - Batch 829/903 - Train loss: 3230314.304819277\n",
            "Iteration 1733 - Batch 830/903 - Train loss: 3229577.456377858\n",
            "Iteration 1734 - Batch 831/903 - Train loss: 3228932.836237981\n",
            "Iteration 1735 - Batch 832/903 - Train loss: 3228856.444477791\n",
            "Iteration 1736 - Batch 833/903 - Train loss: 3228540.801558753\n",
            "Iteration 1737 - Batch 834/903 - Train loss: 3228382.761676647\n",
            "Iteration 1738 - Batch 835/903 - Train loss: 3228032.852272727\n",
            "Iteration 1739 - Batch 836/903 - Train loss: 3227615.3691756274\n",
            "Iteration 1740 - Batch 837/903 - Train loss: 3227144.1521479716\n",
            "Iteration 1741 - Batch 838/903 - Train loss: 3226514.4025625745\n",
            "Iteration 1742 - Batch 839/903 - Train loss: 3226253.808035714\n",
            "Iteration 1743 - Batch 840/903 - Train loss: 3225931.0543995243\n",
            "Iteration 1744 - Batch 841/903 - Train loss: 3225614.507125891\n",
            "Iteration 1745 - Batch 842/903 - Train loss: 3225333.8988730726\n",
            "Iteration 1746 - Batch 843/903 - Train loss: 3224952.8400473935\n",
            "Iteration 1747 - Batch 844/903 - Train loss: 3224796.707988166\n",
            "Iteration 1748 - Batch 845/903 - Train loss: 3224239.392434988\n",
            "Iteration 1749 - Batch 846/903 - Train loss: 3223893.077331759\n",
            "Iteration 1750 - Batch 847/903 - Train loss: 3223228.9814268867\n",
            "Iteration 1751 - Batch 848/903 - Train loss: 3222990.233510012\n",
            "Iteration 1752 - Batch 849/903 - Train loss: 3222668.4176470586\n",
            "Iteration 1753 - Batch 850/903 - Train loss: 3222222.0405405406\n",
            "Iteration 1754 - Batch 851/903 - Train loss: 3221766.515258216\n",
            "Iteration 1755 - Batch 852/903 - Train loss: 3221471.331184056\n",
            "Iteration 1756 - Batch 853/903 - Train loss: 3221076.197306792\n",
            "Iteration 1757 - Batch 854/903 - Train loss: 3220865.7941520466\n",
            "Iteration 1758 - Batch 855/903 - Train loss: 3220333.5846962617\n",
            "Iteration 1759 - Batch 856/903 - Train loss: 3219920.240956826\n",
            "Iteration 1760 - Batch 857/903 - Train loss: 3219359.148601399\n",
            "Iteration 1761 - Batch 858/903 - Train loss: 3218711.050058207\n",
            "Iteration 1762 - Batch 859/903 - Train loss: 3218502.677616279\n",
            "Iteration 1763 - Batch 860/903 - Train loss: 3218310.3742740997\n",
            "Iteration 1764 - Batch 861/903 - Train loss: 3217743.2415893273\n",
            "Iteration 1765 - Batch 862/903 - Train loss: 3217216.245654693\n",
            "Iteration 1766 - Batch 863/903 - Train loss: 3217091.345775463\n",
            "Iteration 1767 - Batch 864/903 - Train loss: 3216595.7213872834\n",
            "Iteration 1768 - Batch 865/903 - Train loss: 3216059.2502886835\n",
            "Iteration 1769 - Batch 866/903 - Train loss: 3215700.7061707038\n",
            "Iteration 1770 - Batch 867/903 - Train loss: 3215241.0334101385\n",
            "Iteration 1771 - Batch 868/903 - Train loss: 3214733.7097238204\n",
            "Iteration 1772 - Batch 869/903 - Train loss: 3214242.1614942527\n",
            "Iteration 1773 - Batch 870/903 - Train loss: 3213693.782433984\n",
            "Iteration 1774 - Batch 871/903 - Train loss: 3213273.3755733944\n",
            "Iteration 1775 - Batch 872/903 - Train loss: 3212926.21534937\n",
            "Iteration 1776 - Batch 873/903 - Train loss: 3212547.8741418766\n",
            "Iteration 1777 - Batch 874/903 - Train loss: 3212153.8722857144\n",
            "Iteration 1778 - Batch 875/903 - Train loss: 3211669.414383562\n",
            "Iteration 1779 - Batch 876/903 - Train loss: 3210876.2129418473\n",
            "Iteration 1780 - Batch 877/903 - Train loss: 3210216.694191344\n",
            "Iteration 1781 - Batch 878/903 - Train loss: 3209496.2909556315\n",
            "Iteration 1782 - Batch 879/903 - Train loss: 3209129.9786931816\n",
            "Iteration 1783 - Batch 880/903 - Train loss: 3208790.078887628\n",
            "Iteration 1784 - Batch 881/903 - Train loss: 3208290.9693877553\n",
            "Iteration 1785 - Batch 882/903 - Train loss: 3207685.060872027\n",
            "Iteration 1786 - Batch 883/903 - Train loss: 3207197.8215497737\n",
            "Iteration 1787 - Batch 884/903 - Train loss: 3206755.8350282484\n",
            "Iteration 1788 - Batch 885/903 - Train loss: 3206255.7344808127\n",
            "Iteration 1789 - Batch 886/903 - Train loss: 3206044.2564825253\n",
            "Iteration 1790 - Batch 887/903 - Train loss: 3205787.459740991\n",
            "Iteration 1791 - Batch 888/903 - Train loss: 3205429.53655793\n",
            "Iteration 1792 - Batch 889/903 - Train loss: 3204764.052247191\n",
            "Iteration 1793 - Batch 890/903 - Train loss: 3204114.541245791\n",
            "Iteration 1794 - Batch 891/903 - Train loss: 3203377.0616591927\n",
            "Iteration 1795 - Batch 892/903 - Train loss: 3202644.015957447\n",
            "Iteration 1796 - Batch 893/903 - Train loss: 3202208.831655481\n",
            "Iteration 1797 - Batch 894/903 - Train loss: 3201752.4896648047\n",
            "Iteration 1798 - Batch 895/903 - Train loss: 3201119.2179129464\n",
            "Iteration 1799 - Batch 896/903 - Train loss: 3200682.192865106\n",
            "Iteration 1800 - Batch 897/903 - Train loss: 3200165.4938752786\n",
            "Iteration 1801 - Batch 898/903 - Train loss: 3199552.41323693\n",
            "Iteration 1802 - Batch 899/903 - Train loss: 3199115.776666667\n",
            "Iteration 1803 - Batch 900/903 - Train loss: 3198572.750554939\n",
            "Iteration 1804 - Batch 901/903 - Train loss: 3198182.4204545454\n",
            "Iteration 1805 - Batch 902/903 - Train loss: 3195290.588385936\n",
            "Val loss: 2362221.125\n",
            "Epoch 3/6\n",
            "Iteration 1807 - Batch 1/903 - Train loss: 3372355.75\n",
            "Iteration 1808 - Batch 2/903 - Train loss: 3431313.75\n",
            "Iteration 1809 - Batch 3/903 - Train loss: 3338047.0\n",
            "Iteration 1810 - Batch 4/903 - Train loss: 3294530.5\n",
            "Iteration 1811 - Batch 5/903 - Train loss: 3379601.7916666665\n",
            "Iteration 1812 - Batch 6/903 - Train loss: 3460133.3928571427\n",
            "Iteration 1813 - Batch 7/903 - Train loss: 3457029.9375\n",
            "Iteration 1814 - Batch 8/903 - Train loss: 3425280.1666666665\n",
            "Iteration 1815 - Batch 9/903 - Train loss: 3441070.725\n",
            "Iteration 1816 - Batch 10/903 - Train loss: 3580608.7045454546\n",
            "Iteration 1817 - Batch 11/903 - Train loss: 3623188.3125\n",
            "Iteration 1818 - Batch 12/903 - Train loss: 3571573.653846154\n",
            "Iteration 1819 - Batch 13/903 - Train loss: 3593324.214285714\n",
            "Iteration 1820 - Batch 14/903 - Train loss: 3622878.2\n",
            "Iteration 1821 - Batch 15/903 - Train loss: 3563105.984375\n",
            "Iteration 1822 - Batch 16/903 - Train loss: 3551673.838235294\n",
            "Iteration 1823 - Batch 17/903 - Train loss: 3549010.513888889\n",
            "Iteration 1824 - Batch 18/903 - Train loss: 3523449.986842105\n",
            "Iteration 1825 - Batch 19/903 - Train loss: 3514648.425\n",
            "Iteration 1826 - Batch 20/903 - Train loss: 3496349.8571428573\n",
            "Iteration 1827 - Batch 21/903 - Train loss: 3471243.659090909\n",
            "Iteration 1828 - Batch 22/903 - Train loss: 3449698.3586956523\n",
            "Iteration 1829 - Batch 23/903 - Train loss: 3436203.71875\n",
            "Iteration 1830 - Batch 24/903 - Train loss: 3418789.38\n",
            "Iteration 1831 - Batch 25/903 - Train loss: 3403019.269230769\n",
            "Iteration 1832 - Batch 26/903 - Train loss: 3387131.722222222\n",
            "Iteration 1833 - Batch 27/903 - Train loss: 3371361.0803571427\n",
            "Iteration 1834 - Batch 28/903 - Train loss: 3362178.3362068967\n",
            "Iteration 1835 - Batch 29/903 - Train loss: 3346481.841666667\n",
            "Iteration 1836 - Batch 30/903 - Train loss: 3330976.39516129\n",
            "Iteration 1837 - Batch 31/903 - Train loss: 3317520.5625\n",
            "Iteration 1838 - Batch 32/903 - Train loss: 3299380.878787879\n",
            "Iteration 1839 - Batch 33/903 - Train loss: 3282395.779411765\n",
            "Iteration 1840 - Batch 34/903 - Train loss: 3284603.992857143\n",
            "Iteration 1841 - Batch 35/903 - Train loss: 3277289.7569444445\n",
            "Iteration 1842 - Batch 36/903 - Train loss: 3283972.0\n",
            "Iteration 1843 - Batch 37/903 - Train loss: 3277920.9210526315\n",
            "Iteration 1844 - Batch 38/903 - Train loss: 3266509.2756410255\n",
            "Iteration 1845 - Batch 39/903 - Train loss: 3266825.16875\n",
            "Iteration 1846 - Batch 40/903 - Train loss: 3255442.823170732\n",
            "Iteration 1847 - Batch 41/903 - Train loss: 3244678.970238095\n",
            "Iteration 1848 - Batch 42/903 - Train loss: 3238250.8895348837\n",
            "Iteration 1849 - Batch 43/903 - Train loss: 3232988.5454545454\n",
            "Iteration 1850 - Batch 44/903 - Train loss: 3231610.6611111113\n",
            "Iteration 1851 - Batch 45/903 - Train loss: 3221876.782608696\n",
            "Iteration 1852 - Batch 46/903 - Train loss: 3210051.313829787\n",
            "Iteration 1853 - Batch 47/903 - Train loss: 3209281.40625\n",
            "Iteration 1854 - Batch 48/903 - Train loss: 3209736.3928571427\n",
            "Iteration 1855 - Batch 49/903 - Train loss: 3202640.585\n",
            "Iteration 1856 - Batch 50/903 - Train loss: 3197565.5196078434\n",
            "Iteration 1857 - Batch 51/903 - Train loss: 3192566.9903846155\n",
            "Iteration 1858 - Batch 52/903 - Train loss: 3186827.797169811\n",
            "Iteration 1859 - Batch 53/903 - Train loss: 3183735.6296296297\n",
            "Iteration 1860 - Batch 54/903 - Train loss: 3175033.609090909\n",
            "Iteration 1861 - Batch 55/903 - Train loss: 3171468.526785714\n",
            "Iteration 1862 - Batch 56/903 - Train loss: 3168109.5701754387\n",
            "Iteration 1863 - Batch 57/903 - Train loss: 3166475.8146551726\n",
            "Iteration 1864 - Batch 58/903 - Train loss: 3161683.504237288\n",
            "Iteration 1865 - Batch 59/903 - Train loss: 3157425.025\n",
            "Iteration 1866 - Batch 60/903 - Train loss: 3150733.704918033\n",
            "Iteration 1867 - Batch 61/903 - Train loss: 3143855.370967742\n",
            "Iteration 1868 - Batch 62/903 - Train loss: 3140455.162698413\n",
            "Iteration 1869 - Batch 63/903 - Train loss: 3138473.69140625\n",
            "Iteration 1870 - Batch 64/903 - Train loss: 3134915.480769231\n",
            "Iteration 1871 - Batch 65/903 - Train loss: 3134564.212121212\n",
            "Iteration 1872 - Batch 66/903 - Train loss: 3126892.962686567\n",
            "Iteration 1873 - Batch 67/903 - Train loss: 3118305.944852941\n",
            "Iteration 1874 - Batch 68/903 - Train loss: 3115694.6195652173\n",
            "Iteration 1875 - Batch 69/903 - Train loss: 3115064.410714286\n",
            "Iteration 1876 - Batch 70/903 - Train loss: 3115105.8380281692\n",
            "Iteration 1877 - Batch 71/903 - Train loss: 3112115.375\n",
            "Iteration 1878 - Batch 72/903 - Train loss: 3104760.506849315\n",
            "Iteration 1879 - Batch 73/903 - Train loss: 3104517.043918919\n",
            "Iteration 1880 - Batch 74/903 - Train loss: 3102481.1233333335\n",
            "Iteration 1881 - Batch 75/903 - Train loss: 3098825.164473684\n",
            "Iteration 1882 - Batch 76/903 - Train loss: 3091755.7824675324\n",
            "Iteration 1883 - Batch 77/903 - Train loss: 3087471.6506410255\n",
            "Iteration 1884 - Batch 78/903 - Train loss: 3086394.661392405\n",
            "Iteration 1885 - Batch 79/903 - Train loss: 3082265.578125\n",
            "Iteration 1886 - Batch 80/903 - Train loss: 3077529.287037037\n",
            "Iteration 1887 - Batch 81/903 - Train loss: 3074195.350609756\n",
            "Iteration 1888 - Batch 82/903 - Train loss: 3069387.876506024\n",
            "Iteration 1889 - Batch 83/903 - Train loss: 3064574.8125\n",
            "Iteration 1890 - Batch 84/903 - Train loss: 3062066.879411765\n",
            "Iteration 1891 - Batch 85/903 - Train loss: 3057286.226744186\n",
            "Iteration 1892 - Batch 86/903 - Train loss: 3050784.074712644\n",
            "Iteration 1893 - Batch 87/903 - Train loss: 3048133.9204545454\n",
            "Iteration 1894 - Batch 88/903 - Train loss: 3047219.938202247\n",
            "Iteration 1895 - Batch 89/903 - Train loss: 3041064.0944444444\n",
            "Iteration 1896 - Batch 90/903 - Train loss: 3034384.9423076925\n",
            "Iteration 1897 - Batch 91/903 - Train loss: 3030436.2635869565\n",
            "Iteration 1898 - Batch 92/903 - Train loss: 3029320.795698925\n",
            "Iteration 1899 - Batch 93/903 - Train loss: 3024691.8563829786\n",
            "Iteration 1900 - Batch 94/903 - Train loss: 3020156.6236842107\n",
            "Iteration 1901 - Batch 95/903 - Train loss: 3015822.8046875\n",
            "Iteration 1902 - Batch 96/903 - Train loss: 3013183.1675257734\n",
            "Iteration 1903 - Batch 97/903 - Train loss: 3012068.9464285714\n",
            "Iteration 1904 - Batch 98/903 - Train loss: 3006835.9444444445\n",
            "Iteration 1905 - Batch 99/903 - Train loss: 3005542.595\n",
            "Iteration 1906 - Batch 100/903 - Train loss: 3003504.6039603963\n",
            "Iteration 1907 - Batch 101/903 - Train loss: 3001082.850490196\n",
            "Iteration 1908 - Batch 102/903 - Train loss: 2996301.368932039\n",
            "Iteration 1909 - Batch 103/903 - Train loss: 2993746.4375\n",
            "Iteration 1910 - Batch 104/903 - Train loss: 2991491.4904761906\n",
            "Iteration 1911 - Batch 105/903 - Train loss: 2986727.8254716983\n",
            "Iteration 1912 - Batch 106/903 - Train loss: 2983352.8738317755\n",
            "Iteration 1913 - Batch 107/903 - Train loss: 2980401.907407407\n",
            "Iteration 1914 - Batch 108/903 - Train loss: 2978825.0963302753\n",
            "Iteration 1915 - Batch 109/903 - Train loss: 2979669.75\n",
            "Iteration 1916 - Batch 110/903 - Train loss: 2976658.403153153\n",
            "Iteration 1917 - Batch 111/903 - Train loss: 2973877.7589285714\n",
            "Iteration 1918 - Batch 112/903 - Train loss: 2970380.5044247787\n",
            "Iteration 1919 - Batch 113/903 - Train loss: 2968434.822368421\n",
            "Iteration 1920 - Batch 114/903 - Train loss: 2965340.4239130435\n",
            "Iteration 1921 - Batch 115/903 - Train loss: 2963763.7262931033\n",
            "Iteration 1922 - Batch 116/903 - Train loss: 2961674.876068376\n",
            "Iteration 1923 - Batch 117/903 - Train loss: 2960486.5084745763\n",
            "Iteration 1924 - Batch 118/903 - Train loss: 2958251.9831932774\n",
            "Iteration 1925 - Batch 119/903 - Train loss: 2955193.08125\n",
            "Iteration 1926 - Batch 120/903 - Train loss: 2952071.138429752\n",
            "Iteration 1927 - Batch 121/903 - Train loss: 2948999.0368852457\n",
            "Iteration 1928 - Batch 122/903 - Train loss: 2946914.7703252034\n",
            "Iteration 1929 - Batch 123/903 - Train loss: 2944944.9233870967\n",
            "Iteration 1930 - Batch 124/903 - Train loss: 2943125.01\n",
            "Iteration 1931 - Batch 125/903 - Train loss: 2941468.0853174604\n",
            "Iteration 1932 - Batch 126/903 - Train loss: 2941056.2598425196\n",
            "Iteration 1933 - Batch 127/903 - Train loss: 2938440.6953125\n",
            "Iteration 1934 - Batch 128/903 - Train loss: 2936964.2596899224\n",
            "Iteration 1935 - Batch 129/903 - Train loss: 2934058.1346153845\n",
            "Iteration 1936 - Batch 130/903 - Train loss: 2933328.763358779\n",
            "Iteration 1937 - Batch 131/903 - Train loss: 2934258.1818181816\n",
            "Iteration 1938 - Batch 132/903 - Train loss: 2933525.7387218047\n",
            "Iteration 1939 - Batch 133/903 - Train loss: 2931095.7052238807\n",
            "Iteration 1940 - Batch 134/903 - Train loss: 2929447.9296296295\n",
            "Iteration 1941 - Batch 135/903 - Train loss: 2927336.049632353\n",
            "Iteration 1942 - Batch 136/903 - Train loss: 2926592.950729927\n",
            "Iteration 1943 - Batch 137/903 - Train loss: 2925178.2391304346\n",
            "Iteration 1944 - Batch 138/903 - Train loss: 2924004.1816546763\n",
            "Iteration 1945 - Batch 139/903 - Train loss: 2922559.6482142857\n",
            "Iteration 1946 - Batch 140/903 - Train loss: 2919999.7287234045\n",
            "Iteration 1947 - Batch 141/903 - Train loss: 2919339.221830986\n",
            "Iteration 1948 - Batch 142/903 - Train loss: 2918491.937062937\n",
            "Iteration 1949 - Batch 143/903 - Train loss: 2917575.2552083335\n",
            "Iteration 1950 - Batch 144/903 - Train loss: 2915758.9051724137\n",
            "Iteration 1951 - Batch 145/903 - Train loss: 2914213.960616438\n",
            "Iteration 1952 - Batch 146/903 - Train loss: 2912370.7482993198\n",
            "Iteration 1953 - Batch 147/903 - Train loss: 2912054.6097972975\n",
            "Iteration 1954 - Batch 148/903 - Train loss: 2909943.902684564\n",
            "Iteration 1955 - Batch 149/903 - Train loss: 2907501.696666667\n",
            "Iteration 1956 - Batch 150/903 - Train loss: 2907035.30794702\n",
            "Iteration 1957 - Batch 151/903 - Train loss: 2904709.018092105\n",
            "Iteration 1958 - Batch 152/903 - Train loss: 2903734.8741830066\n",
            "Iteration 1959 - Batch 153/903 - Train loss: 2902539.9837662335\n",
            "Iteration 1960 - Batch 154/903 - Train loss: 2901976.7451612903\n",
            "Iteration 1961 - Batch 155/903 - Train loss: 2903466.278846154\n",
            "Iteration 1962 - Batch 156/903 - Train loss: 2903467.51910828\n",
            "Iteration 1963 - Batch 157/903 - Train loss: 2903118.685126582\n",
            "Iteration 1964 - Batch 158/903 - Train loss: 2902342.0613207547\n",
            "Iteration 1965 - Batch 159/903 - Train loss: 2902847.9484375\n",
            "Iteration 1966 - Batch 160/903 - Train loss: 2902837.0403726706\n",
            "Iteration 1967 - Batch 161/903 - Train loss: 2901944.9814814813\n",
            "Iteration 1968 - Batch 162/903 - Train loss: 2900399.5582822086\n",
            "Iteration 1969 - Batch 163/903 - Train loss: 2899374.3810975607\n",
            "Iteration 1970 - Batch 164/903 - Train loss: 2899129.8863636362\n",
            "Iteration 1971 - Batch 165/903 - Train loss: 2899625.548192771\n",
            "Iteration 1972 - Batch 166/903 - Train loss: 2900518.2844311376\n",
            "Iteration 1973 - Batch 167/903 - Train loss: 2900652.026785714\n",
            "Iteration 1974 - Batch 168/903 - Train loss: 2900052.0384615385\n",
            "Iteration 1975 - Batch 169/903 - Train loss: 2898007.963235294\n",
            "Iteration 1976 - Batch 170/903 - Train loss: 2898600.308479532\n",
            "Iteration 1977 - Batch 171/903 - Train loss: 2897286.4593023257\n",
            "Iteration 1978 - Batch 172/903 - Train loss: 2899011.447976879\n",
            "Iteration 1979 - Batch 173/903 - Train loss: 2899871.0718390807\n",
            "Iteration 1980 - Batch 174/903 - Train loss: 2899035.4485714287\n",
            "Iteration 1981 - Batch 175/903 - Train loss: 2897923.5610795454\n",
            "Iteration 1982 - Batch 176/903 - Train loss: 2897913.5677966103\n",
            "Iteration 1983 - Batch 177/903 - Train loss: 2896987.9803370787\n",
            "Iteration 1984 - Batch 178/903 - Train loss: 2896255.553072626\n",
            "Iteration 1985 - Batch 179/903 - Train loss: 2895365.798611111\n",
            "Iteration 1986 - Batch 180/903 - Train loss: 2893452.877071823\n",
            "Iteration 1987 - Batch 181/903 - Train loss: 2893032.1236263737\n",
            "Iteration 1988 - Batch 182/903 - Train loss: 2893058.856557377\n",
            "Iteration 1989 - Batch 183/903 - Train loss: 2891544.1154891304\n",
            "Iteration 1990 - Batch 184/903 - Train loss: 2890090.9918918917\n",
            "Iteration 1991 - Batch 185/903 - Train loss: 2891110.0134408604\n",
            "Iteration 1992 - Batch 186/903 - Train loss: 2890973.4946524063\n",
            "Iteration 1993 - Batch 187/903 - Train loss: 2891847.7180851065\n",
            "Iteration 1994 - Batch 188/903 - Train loss: 2891748.5171957673\n",
            "Iteration 1995 - Batch 189/903 - Train loss: 2893191.1618421054\n",
            "Iteration 1996 - Batch 190/903 - Train loss: 2891060.769633508\n",
            "Iteration 1997 - Batch 191/903 - Train loss: 2892738.85546875\n",
            "Iteration 1998 - Batch 192/903 - Train loss: 2893815.274611399\n",
            "Iteration 1999 - Batch 193/903 - Train loss: 2895235.599226804\n",
            "Iteration 2000 - Batch 194/903 - Train loss: 2896083.530769231\n",
            "Iteration 2001 - Batch 195/903 - Train loss: 2894355.649234694\n",
            "Iteration 2002 - Batch 196/903 - Train loss: 2894525.2487309645\n",
            "Iteration 2003 - Batch 197/903 - Train loss: 2892923.642676768\n",
            "Iteration 2004 - Batch 198/903 - Train loss: 2892959.2927135676\n",
            "Iteration 2005 - Batch 199/903 - Train loss: 2892207.89375\n",
            "Iteration 2006 - Batch 200/903 - Train loss: 2890276.1393034826\n",
            "Iteration 2007 - Batch 201/903 - Train loss: 2889365.375\n",
            "Iteration 2008 - Batch 202/903 - Train loss: 2889485.5036945813\n",
            "Iteration 2009 - Batch 203/903 - Train loss: 2888312.0796568627\n",
            "Iteration 2010 - Batch 204/903 - Train loss: 2888335.5414634147\n",
            "Iteration 2011 - Batch 205/903 - Train loss: 2886568.36407767\n",
            "Iteration 2012 - Batch 206/903 - Train loss: 2885734.1014492754\n",
            "Iteration 2013 - Batch 207/903 - Train loss: 2884740.566105769\n",
            "Iteration 2014 - Batch 208/903 - Train loss: 2883912.2954545454\n",
            "Iteration 2015 - Batch 209/903 - Train loss: 2883452.7226190474\n",
            "Iteration 2016 - Batch 210/903 - Train loss: 2882614.4170616115\n",
            "Iteration 2017 - Batch 211/903 - Train loss: 2881426.738207547\n",
            "Iteration 2018 - Batch 212/903 - Train loss: 2880830.3051643195\n",
            "Iteration 2019 - Batch 213/903 - Train loss: 2880423.4591121497\n",
            "Iteration 2020 - Batch 214/903 - Train loss: 2880131.9941860465\n",
            "Iteration 2021 - Batch 215/903 - Train loss: 2880333.465277778\n",
            "Iteration 2022 - Batch 216/903 - Train loss: 2879603.594470046\n",
            "Iteration 2023 - Batch 217/903 - Train loss: 2878318.847477064\n",
            "Iteration 2024 - Batch 218/903 - Train loss: 2877730.327625571\n",
            "Iteration 2025 - Batch 219/903 - Train loss: 2877245.680681818\n",
            "Iteration 2026 - Batch 220/903 - Train loss: 2876063.108597285\n",
            "Iteration 2027 - Batch 221/903 - Train loss: 2874242.7635135134\n",
            "Iteration 2028 - Batch 222/903 - Train loss: 2872664.6502242154\n",
            "Iteration 2029 - Batch 223/903 - Train loss: 2872369.5636160714\n",
            "Iteration 2030 - Batch 224/903 - Train loss: 2872375.953333333\n",
            "Iteration 2031 - Batch 225/903 - Train loss: 2872953.700221239\n",
            "Iteration 2032 - Batch 226/903 - Train loss: 2873044.691629956\n",
            "Iteration 2033 - Batch 227/903 - Train loss: 2872483.663377193\n",
            "Iteration 2034 - Batch 228/903 - Train loss: 2872559.693231441\n",
            "Iteration 2035 - Batch 229/903 - Train loss: 2871896.0945652174\n",
            "Iteration 2036 - Batch 230/903 - Train loss: 2870698.0043290043\n",
            "Iteration 2037 - Batch 231/903 - Train loss: 2869739.0829741377\n",
            "Iteration 2038 - Batch 232/903 - Train loss: 2869397.7542918455\n",
            "Iteration 2039 - Batch 233/903 - Train loss: 2869125.9957264955\n",
            "Iteration 2040 - Batch 234/903 - Train loss: 2868817.6521276594\n",
            "Iteration 2041 - Batch 235/903 - Train loss: 2869062.092161017\n",
            "Iteration 2042 - Batch 236/903 - Train loss: 2869288.793248945\n",
            "Iteration 2043 - Batch 237/903 - Train loss: 2869525.904411765\n",
            "Iteration 2044 - Batch 238/903 - Train loss: 2869259.8640167364\n",
            "Iteration 2045 - Batch 239/903 - Train loss: 2869795.707291667\n",
            "Iteration 2046 - Batch 240/903 - Train loss: 2870278.92219917\n",
            "Iteration 2047 - Batch 241/903 - Train loss: 2871810.2799586775\n",
            "Iteration 2048 - Batch 242/903 - Train loss: 2872954.5020576133\n",
            "Iteration 2049 - Batch 243/903 - Train loss: 2873863.475409836\n",
            "Iteration 2050 - Batch 244/903 - Train loss: 2872727.3765306124\n",
            "Iteration 2051 - Batch 245/903 - Train loss: 2872385.463414634\n",
            "Iteration 2052 - Batch 246/903 - Train loss: 2872923.995951417\n",
            "Iteration 2053 - Batch 247/903 - Train loss: 2873075.8629032257\n",
            "Iteration 2054 - Batch 248/903 - Train loss: 2872600.498995984\n",
            "Iteration 2055 - Batch 249/903 - Train loss: 2873324.436\n",
            "Iteration 2056 - Batch 250/903 - Train loss: 2874118.418326693\n",
            "Iteration 2057 - Batch 251/903 - Train loss: 2872769.850198413\n",
            "Iteration 2058 - Batch 252/903 - Train loss: 2872286.783596838\n",
            "Iteration 2059 - Batch 253/903 - Train loss: 2871356.554133858\n",
            "Iteration 2060 - Batch 254/903 - Train loss: 2871618.412745098\n",
            "Iteration 2061 - Batch 255/903 - Train loss: 2872270.2646484375\n",
            "Iteration 2062 - Batch 256/903 - Train loss: 2872189.03307393\n",
            "Iteration 2063 - Batch 257/903 - Train loss: 2872511.2558139535\n",
            "Iteration 2064 - Batch 258/903 - Train loss: 2872836.752895753\n",
            "Iteration 2065 - Batch 259/903 - Train loss: 2872252.7423076923\n",
            "Iteration 2066 - Batch 260/903 - Train loss: 2872224.049808429\n",
            "Iteration 2067 - Batch 261/903 - Train loss: 2872384.7929389314\n",
            "Iteration 2068 - Batch 262/903 - Train loss: 2871306.576996198\n",
            "Iteration 2069 - Batch 263/903 - Train loss: 2871496.0757575757\n",
            "Iteration 2070 - Batch 264/903 - Train loss: 2872057.8216981133\n",
            "Iteration 2071 - Batch 265/903 - Train loss: 2872198.7189849624\n",
            "Iteration 2072 - Batch 266/903 - Train loss: 2871172.202247191\n",
            "Iteration 2073 - Batch 267/903 - Train loss: 2871351.718283582\n",
            "Iteration 2074 - Batch 268/903 - Train loss: 2870773.839219331\n",
            "Iteration 2075 - Batch 269/903 - Train loss: 2870428.175\n",
            "Iteration 2076 - Batch 270/903 - Train loss: 2870121.4197416976\n",
            "Iteration 2077 - Batch 271/903 - Train loss: 2870173.471507353\n",
            "Iteration 2078 - Batch 272/903 - Train loss: 2868991.004578755\n",
            "Iteration 2079 - Batch 273/903 - Train loss: 2868336.0885036495\n",
            "Iteration 2080 - Batch 274/903 - Train loss: 2867754.909090909\n",
            "Iteration 2081 - Batch 275/903 - Train loss: 2867004.7617753623\n",
            "Iteration 2082 - Batch 276/903 - Train loss: 2866264.739169675\n",
            "Iteration 2083 - Batch 277/903 - Train loss: 2865459.5944244605\n",
            "Iteration 2084 - Batch 278/903 - Train loss: 2865632.6362007167\n",
            "Iteration 2085 - Batch 279/903 - Train loss: 2863982.2401785715\n",
            "Iteration 2086 - Batch 280/903 - Train loss: 2862306.669039146\n",
            "Iteration 2087 - Batch 281/903 - Train loss: 2861969.2916666665\n",
            "Iteration 2088 - Batch 282/903 - Train loss: 2860544.796819788\n",
            "Iteration 2089 - Batch 283/903 - Train loss: 2860480.2147887326\n",
            "Iteration 2090 - Batch 284/903 - Train loss: 2860006.059649123\n",
            "Iteration 2091 - Batch 285/903 - Train loss: 2858942.5270979023\n",
            "Iteration 2092 - Batch 286/903 - Train loss: 2858021.608885017\n",
            "Iteration 2093 - Batch 287/903 - Train loss: 2857178.615451389\n",
            "Iteration 2094 - Batch 288/903 - Train loss: 2855984.881487889\n",
            "Iteration 2095 - Batch 289/903 - Train loss: 2855516.6905172416\n",
            "Iteration 2096 - Batch 290/903 - Train loss: 2855197.8049828177\n",
            "Iteration 2097 - Batch 291/903 - Train loss: 2854501.491438356\n",
            "Iteration 2098 - Batch 292/903 - Train loss: 2853952.870307167\n",
            "Iteration 2099 - Batch 293/903 - Train loss: 2853193.8273809524\n",
            "Iteration 2100 - Batch 294/903 - Train loss: 2852368.5711864405\n",
            "Iteration 2101 - Batch 295/903 - Train loss: 2851345.6511824327\n",
            "Iteration 2102 - Batch 296/903 - Train loss: 2851224.827441077\n",
            "Iteration 2103 - Batch 297/903 - Train loss: 2851032.0645973156\n",
            "Iteration 2104 - Batch 298/903 - Train loss: 2850832.6906354516\n",
            "Iteration 2105 - Batch 299/903 - Train loss: 2849546.884166667\n",
            "Iteration 2106 - Batch 300/903 - Train loss: 2848231.338039867\n",
            "Iteration 2107 - Batch 301/903 - Train loss: 2847491.0256622517\n",
            "Iteration 2108 - Batch 302/903 - Train loss: 2846741.0346534653\n",
            "Iteration 2109 - Batch 303/903 - Train loss: 2845084.8955592103\n",
            "Iteration 2110 - Batch 304/903 - Train loss: 2844473.0327868853\n",
            "Iteration 2111 - Batch 305/903 - Train loss: 2843284.165849673\n",
            "Iteration 2112 - Batch 306/903 - Train loss: 2843166.07980456\n",
            "Iteration 2113 - Batch 307/903 - Train loss: 2842090.9724025973\n",
            "Iteration 2114 - Batch 308/903 - Train loss: 2841715.8276699027\n",
            "Iteration 2115 - Batch 309/903 - Train loss: 2841230.4491935484\n",
            "Iteration 2116 - Batch 310/903 - Train loss: 2840635.17926045\n",
            "Iteration 2117 - Batch 311/903 - Train loss: 2840641.6875\n",
            "Iteration 2118 - Batch 312/903 - Train loss: 2839446.1333865817\n",
            "Iteration 2119 - Batch 313/903 - Train loss: 2838466.7189490446\n",
            "Iteration 2120 - Batch 314/903 - Train loss: 2838344.7849206347\n",
            "Iteration 2121 - Batch 315/903 - Train loss: 2838021.231012658\n",
            "Iteration 2122 - Batch 316/903 - Train loss: 2837038.5567823346\n",
            "Iteration 2123 - Batch 317/903 - Train loss: 2836687.4913522014\n",
            "Iteration 2124 - Batch 318/903 - Train loss: 2836165.9051724137\n",
            "Iteration 2125 - Batch 319/903 - Train loss: 2835385.040625\n",
            "Iteration 2126 - Batch 320/903 - Train loss: 2834820.479750779\n",
            "Iteration 2127 - Batch 321/903 - Train loss: 2834000.2010869565\n",
            "Iteration 2128 - Batch 322/903 - Train loss: 2833444.499226006\n",
            "Iteration 2129 - Batch 323/903 - Train loss: 2833269.073302469\n",
            "Iteration 2130 - Batch 324/903 - Train loss: 2832526.4361538463\n",
            "Iteration 2131 - Batch 325/903 - Train loss: 2831915.0904907975\n",
            "Iteration 2132 - Batch 326/903 - Train loss: 2831369.1444954127\n",
            "Iteration 2133 - Batch 327/903 - Train loss: 2830983.081554878\n",
            "Iteration 2134 - Batch 328/903 - Train loss: 2831308.651975684\n",
            "Iteration 2135 - Batch 329/903 - Train loss: 2830806.0825757575\n",
            "Iteration 2136 - Batch 330/903 - Train loss: 2830266.802114804\n",
            "Iteration 2137 - Batch 331/903 - Train loss: 2830499.5421686745\n",
            "Iteration 2138 - Batch 332/903 - Train loss: 2830686.4016516516\n",
            "Iteration 2139 - Batch 333/903 - Train loss: 2830599.741766467\n",
            "Iteration 2140 - Batch 334/903 - Train loss: 2829488.530597015\n",
            "Iteration 2141 - Batch 335/903 - Train loss: 2829190.615327381\n",
            "Iteration 2142 - Batch 336/903 - Train loss: 2828772.761869436\n",
            "Iteration 2143 - Batch 337/903 - Train loss: 2828003.024408284\n",
            "Iteration 2144 - Batch 338/903 - Train loss: 2828264.361356932\n",
            "Iteration 2145 - Batch 339/903 - Train loss: 2828248.488235294\n",
            "Iteration 2146 - Batch 340/903 - Train loss: 2828230.478005865\n",
            "Iteration 2147 - Batch 341/903 - Train loss: 2827924.9166666665\n",
            "Iteration 2148 - Batch 342/903 - Train loss: 2826843.864431487\n",
            "Iteration 2149 - Batch 343/903 - Train loss: 2826110.96875\n",
            "Iteration 2150 - Batch 344/903 - Train loss: 2825730.068115942\n",
            "Iteration 2151 - Batch 345/903 - Train loss: 2825126.9682080923\n",
            "Iteration 2152 - Batch 346/903 - Train loss: 2825104.6873198845\n",
            "Iteration 2153 - Batch 347/903 - Train loss: 2825573.762212644\n",
            "Iteration 2154 - Batch 348/903 - Train loss: 2825673.8058739253\n",
            "Iteration 2155 - Batch 349/903 - Train loss: 2825640.318571429\n",
            "Iteration 2156 - Batch 350/903 - Train loss: 2825614.915954416\n",
            "Iteration 2157 - Batch 351/903 - Train loss: 2826076.9730113638\n",
            "Iteration 2158 - Batch 352/903 - Train loss: 2826299.8626062325\n",
            "Iteration 2159 - Batch 353/903 - Train loss: 2826532.256355932\n",
            "Iteration 2160 - Batch 354/903 - Train loss: 2826519.0676056338\n",
            "Iteration 2161 - Batch 355/903 - Train loss: 2826026.9936797754\n",
            "Iteration 2162 - Batch 356/903 - Train loss: 2826053.3298319327\n",
            "Iteration 2163 - Batch 357/903 - Train loss: 2825872.2122905026\n",
            "Iteration 2164 - Batch 358/903 - Train loss: 2825778.2054317547\n",
            "Iteration 2165 - Batch 359/903 - Train loss: 2826133.198611111\n",
            "Iteration 2166 - Batch 360/903 - Train loss: 2825658.858725762\n",
            "Iteration 2167 - Batch 361/903 - Train loss: 2825390.05801105\n",
            "Iteration 2168 - Batch 362/903 - Train loss: 2824164.9249311294\n",
            "Iteration 2169 - Batch 363/903 - Train loss: 2823642.563186813\n",
            "Iteration 2170 - Batch 364/903 - Train loss: 2823535.02739726\n",
            "Iteration 2171 - Batch 365/903 - Train loss: 2824122.144808743\n",
            "Iteration 2172 - Batch 366/903 - Train loss: 2824578.8215258857\n",
            "Iteration 2173 - Batch 367/903 - Train loss: 2824985.071331522\n",
            "Iteration 2174 - Batch 368/903 - Train loss: 2824731.247289973\n",
            "Iteration 2175 - Batch 369/903 - Train loss: 2824827.6648648647\n",
            "Iteration 2176 - Batch 370/903 - Train loss: 2823446.090296496\n",
            "Iteration 2177 - Batch 371/903 - Train loss: 2822755.2883064514\n",
            "Iteration 2178 - Batch 372/903 - Train loss: 2821767.7714477214\n",
            "Iteration 2179 - Batch 373/903 - Train loss: 2821135.9578877008\n",
            "Iteration 2180 - Batch 374/903 - Train loss: 2820182.4486666666\n",
            "Iteration 2181 - Batch 375/903 - Train loss: 2819179.7719414895\n",
            "Iteration 2182 - Batch 376/903 - Train loss: 2819249.970822281\n",
            "Iteration 2183 - Batch 377/903 - Train loss: 2819624.9272486772\n",
            "Iteration 2184 - Batch 378/903 - Train loss: 2820410.8713720315\n",
            "Iteration 2185 - Batch 379/903 - Train loss: 2821405.130263158\n",
            "Iteration 2186 - Batch 380/903 - Train loss: 2822219.5879265093\n",
            "Iteration 2187 - Batch 381/903 - Train loss: 2822802.1184554975\n",
            "Iteration 2188 - Batch 382/903 - Train loss: 2822658.450391645\n",
            "Iteration 2189 - Batch 383/903 - Train loss: 2822595.9498697915\n",
            "Iteration 2190 - Batch 384/903 - Train loss: 2823110.0915584415\n",
            "Iteration 2191 - Batch 385/903 - Train loss: 2823394.0511658033\n",
            "Iteration 2192 - Batch 386/903 - Train loss: 2823156.3998708013\n",
            "Iteration 2193 - Batch 387/903 - Train loss: 2822804.132087629\n",
            "Iteration 2194 - Batch 388/903 - Train loss: 2822142.13688946\n",
            "Iteration 2195 - Batch 389/903 - Train loss: 2821771.594871795\n",
            "Iteration 2196 - Batch 390/903 - Train loss: 2821134.557544757\n",
            "Iteration 2197 - Batch 391/903 - Train loss: 2821195.9209183673\n",
            "Iteration 2198 - Batch 392/903 - Train loss: 2820882.340330789\n",
            "Iteration 2199 - Batch 393/903 - Train loss: 2820297.4663705584\n",
            "Iteration 2200 - Batch 394/903 - Train loss: 2819247.7227848102\n",
            "Iteration 2201 - Batch 395/903 - Train loss: 2819170.392676768\n",
            "Iteration 2202 - Batch 396/903 - Train loss: 2818467.651763224\n",
            "Iteration 2203 - Batch 397/903 - Train loss: 2817570.59798995\n",
            "Iteration 2204 - Batch 398/903 - Train loss: 2817086.26754386\n",
            "Iteration 2205 - Batch 399/903 - Train loss: 2816535.421875\n",
            "Iteration 2206 - Batch 400/903 - Train loss: 2815963.2855361598\n",
            "Iteration 2207 - Batch 401/903 - Train loss: 2815562.887437811\n",
            "Iteration 2208 - Batch 402/903 - Train loss: 2815345.740074442\n",
            "Iteration 2209 - Batch 403/903 - Train loss: 2815146.7988861385\n",
            "Iteration 2210 - Batch 404/903 - Train loss: 2815058.4049382717\n",
            "Iteration 2211 - Batch 405/903 - Train loss: 2814421.251231527\n",
            "Iteration 2212 - Batch 406/903 - Train loss: 2813894.4213759215\n",
            "Iteration 2213 - Batch 407/903 - Train loss: 2814013.474877451\n",
            "Iteration 2214 - Batch 408/903 - Train loss: 2814156.7420537896\n",
            "Iteration 2215 - Batch 409/903 - Train loss: 2813752.5536585366\n",
            "Iteration 2216 - Batch 410/903 - Train loss: 2814196.081508516\n",
            "Iteration 2217 - Batch 411/903 - Train loss: 2814542.472694175\n",
            "Iteration 2218 - Batch 412/903 - Train loss: 2813721.992130751\n",
            "Iteration 2219 - Batch 413/903 - Train loss: 2813233.3599033817\n",
            "Iteration 2220 - Batch 414/903 - Train loss: 2813097.1873493977\n",
            "Iteration 2221 - Batch 415/903 - Train loss: 2813146.7169471155\n",
            "Iteration 2222 - Batch 416/903 - Train loss: 2812977.2086330936\n",
            "Iteration 2223 - Batch 417/903 - Train loss: 2813726.5789473685\n",
            "Iteration 2224 - Batch 418/903 - Train loss: 2814008.1479713605\n",
            "Iteration 2225 - Batch 419/903 - Train loss: 2813939.225595238\n",
            "Iteration 2226 - Batch 420/903 - Train loss: 2814283.9994061757\n",
            "Iteration 2227 - Batch 421/903 - Train loss: 2814627.7268957347\n",
            "Iteration 2228 - Batch 422/903 - Train loss: 2814815.9391252957\n",
            "Iteration 2229 - Batch 423/903 - Train loss: 2813793.289504717\n",
            "Iteration 2230 - Batch 424/903 - Train loss: 2813774.3417647057\n",
            "Iteration 2231 - Batch 425/903 - Train loss: 2813988.0246478873\n",
            "Iteration 2232 - Batch 426/903 - Train loss: 2813448.300351288\n",
            "Iteration 2233 - Batch 427/903 - Train loss: 2812732.0064252336\n",
            "Iteration 2234 - Batch 428/903 - Train loss: 2812826.0466200467\n",
            "Iteration 2235 - Batch 429/903 - Train loss: 2812800.550581395\n",
            "Iteration 2236 - Batch 430/903 - Train loss: 2812827.040023202\n",
            "Iteration 2237 - Batch 431/903 - Train loss: 2812082.141203704\n",
            "Iteration 2238 - Batch 432/903 - Train loss: 2811619.8464203235\n",
            "Iteration 2239 - Batch 433/903 - Train loss: 2811620.0034562214\n",
            "Iteration 2240 - Batch 434/903 - Train loss: 2811437.3425287358\n",
            "Iteration 2241 - Batch 435/903 - Train loss: 2810962.961009174\n",
            "Iteration 2242 - Batch 436/903 - Train loss: 2811449.5429061786\n",
            "Iteration 2243 - Batch 437/903 - Train loss: 2811392.710616438\n",
            "Iteration 2244 - Batch 438/903 - Train loss: 2811183.717539863\n",
            "Iteration 2245 - Batch 439/903 - Train loss: 2811160.098863636\n",
            "Iteration 2246 - Batch 440/903 - Train loss: 2810845.675170068\n",
            "Iteration 2247 - Batch 441/903 - Train loss: 2810235.911764706\n",
            "Iteration 2248 - Batch 442/903 - Train loss: 2810184.7477426636\n",
            "Iteration 2249 - Batch 443/903 - Train loss: 2809958.1007882883\n",
            "Iteration 2250 - Batch 444/903 - Train loss: 2809395.054494382\n",
            "Iteration 2251 - Batch 445/903 - Train loss: 2808743.317264574\n",
            "Iteration 2252 - Batch 446/903 - Train loss: 2808010.2108501117\n",
            "Iteration 2253 - Batch 447/903 - Train loss: 2807397.5055803573\n",
            "Iteration 2254 - Batch 448/903 - Train loss: 2807590.3630289533\n",
            "Iteration 2255 - Batch 449/903 - Train loss: 2807003.743888889\n",
            "Iteration 2256 - Batch 450/903 - Train loss: 2807041.4307095343\n",
            "Iteration 2257 - Batch 451/903 - Train loss: 2806862.1305309734\n",
            "Iteration 2258 - Batch 452/903 - Train loss: 2806662.3526490065\n",
            "Iteration 2259 - Batch 453/903 - Train loss: 2806310.5781938327\n",
            "Iteration 2260 - Batch 454/903 - Train loss: 2806292.1313186814\n",
            "Iteration 2261 - Batch 455/903 - Train loss: 2805984.724232456\n",
            "Iteration 2262 - Batch 456/903 - Train loss: 2805780.2417943105\n",
            "Iteration 2263 - Batch 457/903 - Train loss: 2805530.615720524\n",
            "Iteration 2264 - Batch 458/903 - Train loss: 2805139.5250544664\n",
            "Iteration 2265 - Batch 459/903 - Train loss: 2804888.5619565216\n",
            "Iteration 2266 - Batch 460/903 - Train loss: 2806236.5856832974\n",
            "Iteration 2267 - Batch 461/903 - Train loss: 2807415.9604978357\n",
            "Iteration 2268 - Batch 462/903 - Train loss: 2808315.260799136\n",
            "Iteration 2269 - Batch 463/903 - Train loss: 2808525.390625\n",
            "Iteration 2270 - Batch 464/903 - Train loss: 2808200.192473118\n",
            "Iteration 2271 - Batch 465/903 - Train loss: 2808679.2489270386\n",
            "Iteration 2272 - Batch 466/903 - Train loss: 2809210.248394004\n",
            "Iteration 2273 - Batch 467/903 - Train loss: 2809358.143162393\n",
            "Iteration 2274 - Batch 468/903 - Train loss: 2809038.87793177\n",
            "Iteration 2275 - Batch 469/903 - Train loss: 2808713.2984042554\n",
            "Iteration 2276 - Batch 470/903 - Train loss: 2809637.2675159234\n",
            "Iteration 2277 - Batch 471/903 - Train loss: 2811168.7102754237\n",
            "Iteration 2278 - Batch 472/903 - Train loss: 2811322.065010571\n",
            "Iteration 2279 - Batch 473/903 - Train loss: 2810915.5311181433\n",
            "Iteration 2280 - Batch 474/903 - Train loss: 2811516.475263158\n",
            "Iteration 2281 - Batch 475/903 - Train loss: 2812011.737394958\n",
            "Iteration 2282 - Batch 476/903 - Train loss: 2812473.395702306\n",
            "Iteration 2283 - Batch 477/903 - Train loss: 2813287.8504184103\n",
            "Iteration 2284 - Batch 478/903 - Train loss: 2813878.979645094\n",
            "Iteration 2285 - Batch 479/903 - Train loss: 2813651.6484375\n",
            "Iteration 2286 - Batch 480/903 - Train loss: 2813132.769230769\n",
            "Iteration 2287 - Batch 481/903 - Train loss: 2813373.890560166\n",
            "Iteration 2288 - Batch 482/903 - Train loss: 2814805.9704968943\n",
            "Iteration 2289 - Batch 483/903 - Train loss: 2814666.4359504133\n",
            "Iteration 2290 - Batch 484/903 - Train loss: 2814774.150515464\n",
            "Iteration 2291 - Batch 485/903 - Train loss: 2816741.4727366255\n",
            "Iteration 2292 - Batch 486/903 - Train loss: 2817308.414784394\n",
            "Iteration 2293 - Batch 487/903 - Train loss: 2817252.9538934426\n",
            "Iteration 2294 - Batch 488/903 - Train loss: 2817363.143149284\n",
            "Iteration 2295 - Batch 489/903 - Train loss: 2818886.5841836734\n",
            "Iteration 2296 - Batch 490/903 - Train loss: 2819396.404786151\n",
            "Iteration 2297 - Batch 491/903 - Train loss: 2819613.5665650405\n",
            "Iteration 2298 - Batch 492/903 - Train loss: 2819501.210953347\n",
            "Iteration 2299 - Batch 493/903 - Train loss: 2819967.539473684\n",
            "Iteration 2300 - Batch 494/903 - Train loss: 2820127.43989899\n",
            "Iteration 2301 - Batch 495/903 - Train loss: 2820491.0776209678\n",
            "Iteration 2302 - Batch 496/903 - Train loss: 2820544.860160966\n",
            "Iteration 2303 - Batch 497/903 - Train loss: 2820872.7243975904\n",
            "Iteration 2304 - Batch 498/903 - Train loss: 2820474.2935871743\n",
            "Iteration 2305 - Batch 499/903 - Train loss: 2820699.666\n",
            "Iteration 2306 - Batch 500/903 - Train loss: 2821443.0978043913\n",
            "Iteration 2307 - Batch 501/903 - Train loss: 2821830.421812749\n",
            "Iteration 2308 - Batch 502/903 - Train loss: 2821560.3305168985\n",
            "Iteration 2309 - Batch 503/903 - Train loss: 2821603.678075397\n",
            "Iteration 2310 - Batch 504/903 - Train loss: 2821452.5193069307\n",
            "Iteration 2311 - Batch 505/903 - Train loss: 2821473.1694664033\n",
            "Iteration 2312 - Batch 506/903 - Train loss: 2820898.0690335305\n",
            "Iteration 2313 - Batch 507/903 - Train loss: 2820929.7303149607\n",
            "Iteration 2314 - Batch 508/903 - Train loss: 2820697.845284872\n",
            "Iteration 2315 - Batch 509/903 - Train loss: 2820363.125980392\n",
            "Iteration 2316 - Batch 510/903 - Train loss: 2820444.6130136987\n",
            "Iteration 2317 - Batch 511/903 - Train loss: 2820120.73046875\n",
            "Iteration 2318 - Batch 512/903 - Train loss: 2819775.452729045\n",
            "Iteration 2319 - Batch 513/903 - Train loss: 2819115.358463035\n",
            "Iteration 2320 - Batch 514/903 - Train loss: 2818839.472330097\n",
            "Iteration 2321 - Batch 515/903 - Train loss: 2818989.8614341086\n",
            "Iteration 2322 - Batch 516/903 - Train loss: 2818360.167311412\n",
            "Iteration 2323 - Batch 517/903 - Train loss: 2818375.791988417\n",
            "Iteration 2324 - Batch 518/903 - Train loss: 2818154.8737957613\n",
            "Iteration 2325 - Batch 519/903 - Train loss: 2817983.7865384617\n",
            "Iteration 2326 - Batch 520/903 - Train loss: 2817470.989923225\n",
            "Iteration 2327 - Batch 521/903 - Train loss: 2817265.655651341\n",
            "Iteration 2328 - Batch 522/903 - Train loss: 2816720.615200765\n",
            "Iteration 2329 - Batch 523/903 - Train loss: 2816651.8854961833\n",
            "Iteration 2330 - Batch 524/903 - Train loss: 2816577.356190476\n",
            "Iteration 2331 - Batch 525/903 - Train loss: 2815751.9828897337\n",
            "Iteration 2332 - Batch 526/903 - Train loss: 2815233.5479127136\n",
            "Iteration 2333 - Batch 527/903 - Train loss: 2814750.315340909\n",
            "Iteration 2334 - Batch 528/903 - Train loss: 2814484.210775047\n",
            "Iteration 2335 - Batch 529/903 - Train loss: 2813923.550943396\n",
            "Iteration 2336 - Batch 530/903 - Train loss: 2813141.524482109\n",
            "Iteration 2337 - Batch 531/903 - Train loss: 2812836.7406015038\n",
            "Iteration 2338 - Batch 532/903 - Train loss: 2812659.012664165\n",
            "Iteration 2339 - Batch 533/903 - Train loss: 2812215.5262172283\n",
            "Iteration 2340 - Batch 534/903 - Train loss: 2812245.9523364487\n",
            "Iteration 2341 - Batch 535/903 - Train loss: 2811783.312966418\n",
            "Iteration 2342 - Batch 536/903 - Train loss: 2811515.3724394785\n",
            "Iteration 2343 - Batch 537/903 - Train loss: 2811230.2207249072\n",
            "Iteration 2344 - Batch 538/903 - Train loss: 2811233.962894249\n",
            "Iteration 2345 - Batch 539/903 - Train loss: 2811250.7106481483\n",
            "Iteration 2346 - Batch 540/903 - Train loss: 2810984.4209796675\n",
            "Iteration 2347 - Batch 541/903 - Train loss: 2810812.13699262\n",
            "Iteration 2348 - Batch 542/903 - Train loss: 2810060.364640884\n",
            "Iteration 2349 - Batch 543/903 - Train loss: 2809530.099724265\n",
            "Iteration 2350 - Batch 544/903 - Train loss: 2808786.5165137616\n",
            "Iteration 2351 - Batch 545/903 - Train loss: 2808041.83470696\n",
            "Iteration 2352 - Batch 546/903 - Train loss: 2807712.0845521023\n",
            "Iteration 2353 - Batch 547/903 - Train loss: 2806929.052919708\n",
            "Iteration 2354 - Batch 548/903 - Train loss: 2806487.318761384\n",
            "Iteration 2355 - Batch 549/903 - Train loss: 2806036.505909091\n",
            "Iteration 2356 - Batch 550/903 - Train loss: 2805599.597096189\n",
            "Iteration 2357 - Batch 551/903 - Train loss: 2805072.7282608696\n",
            "Iteration 2358 - Batch 552/903 - Train loss: 2804734.9692585897\n",
            "Iteration 2359 - Batch 553/903 - Train loss: 2804153.391696751\n",
            "Iteration 2360 - Batch 554/903 - Train loss: 2803711.5594594595\n",
            "Iteration 2361 - Batch 555/903 - Train loss: 2803386.223471223\n",
            "Iteration 2362 - Batch 556/903 - Train loss: 2803074.0637342907\n",
            "Iteration 2363 - Batch 557/903 - Train loss: 2802474.5161290322\n",
            "Iteration 2364 - Batch 558/903 - Train loss: 2801876.8233452593\n",
            "Iteration 2365 - Batch 559/903 - Train loss: 2801298.832589286\n",
            "Iteration 2366 - Batch 560/903 - Train loss: 2800915.854723708\n",
            "Iteration 2367 - Batch 561/903 - Train loss: 2800583.5235765125\n",
            "Iteration 2368 - Batch 562/903 - Train loss: 2800450.0026642983\n",
            "Iteration 2369 - Batch 563/903 - Train loss: 2800016.9791666665\n",
            "Iteration 2370 - Batch 564/903 - Train loss: 2800037.3075221237\n",
            "Iteration 2371 - Batch 565/903 - Train loss: 2799956.116607774\n",
            "Iteration 2372 - Batch 566/903 - Train loss: 2799842.8386243386\n",
            "Iteration 2373 - Batch 567/903 - Train loss: 2799208.99471831\n",
            "Iteration 2374 - Batch 568/903 - Train loss: 2798388.2122144112\n",
            "Iteration 2375 - Batch 569/903 - Train loss: 2798426.6894736844\n",
            "Iteration 2376 - Batch 570/903 - Train loss: 2798478.1808231175\n",
            "Iteration 2377 - Batch 571/903 - Train loss: 2798271.430944056\n",
            "Iteration 2378 - Batch 572/903 - Train loss: 2797667.64921466\n",
            "Iteration 2379 - Batch 573/903 - Train loss: 2797523.156794425\n",
            "Iteration 2380 - Batch 574/903 - Train loss: 2796979.782173913\n",
            "Iteration 2381 - Batch 575/903 - Train loss: 2796552.418402778\n",
            "Iteration 2382 - Batch 576/903 - Train loss: 2796187.5719237435\n",
            "Iteration 2383 - Batch 577/903 - Train loss: 2795841.5679065743\n",
            "Iteration 2384 - Batch 578/903 - Train loss: 2795860.392055268\n",
            "Iteration 2385 - Batch 579/903 - Train loss: 2795866.457758621\n",
            "Iteration 2386 - Batch 580/903 - Train loss: 2795634.7740963856\n",
            "Iteration 2387 - Batch 581/903 - Train loss: 2795307.22895189\n",
            "Iteration 2388 - Batch 582/903 - Train loss: 2795061.8893653518\n",
            "Iteration 2389 - Batch 583/903 - Train loss: 2795402.5547945206\n",
            "Iteration 2390 - Batch 584/903 - Train loss: 2795622.429059829\n",
            "Iteration 2391 - Batch 585/903 - Train loss: 2795079.5571672353\n",
            "Iteration 2392 - Batch 586/903 - Train loss: 2794825.3645655876\n",
            "Iteration 2393 - Batch 587/903 - Train loss: 2794525.005952381\n",
            "Iteration 2394 - Batch 588/903 - Train loss: 2794510.285229202\n",
            "Iteration 2395 - Batch 589/903 - Train loss: 2794662.8436440676\n",
            "Iteration 2396 - Batch 590/903 - Train loss: 2794603.64678511\n",
            "Iteration 2397 - Batch 591/903 - Train loss: 2794179.612331081\n",
            "Iteration 2398 - Batch 592/903 - Train loss: 2793929.3397976393\n",
            "Iteration 2399 - Batch 593/903 - Train loss: 2793614.397727273\n",
            "Iteration 2400 - Batch 594/903 - Train loss: 2793711.612605042\n",
            "Iteration 2401 - Batch 595/903 - Train loss: 2793401.019295302\n",
            "Iteration 2402 - Batch 596/903 - Train loss: 2793688.296482412\n",
            "Iteration 2403 - Batch 597/903 - Train loss: 2793636.1032608696\n",
            "Iteration 2404 - Batch 598/903 - Train loss: 2793323.7863105177\n",
            "Iteration 2405 - Batch 599/903 - Train loss: 2792712.7204166665\n",
            "Iteration 2406 - Batch 600/903 - Train loss: 2792433.398502496\n",
            "Iteration 2407 - Batch 601/903 - Train loss: 2792074.075996678\n",
            "Iteration 2408 - Batch 602/903 - Train loss: 2792107.228855721\n",
            "Iteration 2409 - Batch 603/903 - Train loss: 2792007.684602649\n",
            "Iteration 2410 - Batch 604/903 - Train loss: 2791564.256198347\n",
            "Iteration 2411 - Batch 605/903 - Train loss: 2791340.6534653464\n",
            "Iteration 2412 - Batch 606/903 - Train loss: 2791053.688220758\n",
            "Iteration 2413 - Batch 607/903 - Train loss: 2790615.1287006577\n",
            "Iteration 2414 - Batch 608/903 - Train loss: 2790262.5320197046\n",
            "Iteration 2415 - Batch 609/903 - Train loss: 2789882.9647540986\n",
            "Iteration 2416 - Batch 610/903 - Train loss: 2789489.3711129297\n",
            "Iteration 2417 - Batch 611/903 - Train loss: 2789447.7647058824\n",
            "Iteration 2418 - Batch 612/903 - Train loss: 2788991.070146819\n",
            "Iteration 2419 - Batch 613/903 - Train loss: 2788745.483306189\n",
            "Iteration 2420 - Batch 614/903 - Train loss: 2788243.9219512194\n",
            "Iteration 2421 - Batch 615/903 - Train loss: 2787923.409090909\n",
            "Iteration 2422 - Batch 616/903 - Train loss: 2787941.9773095623\n",
            "Iteration 2423 - Batch 617/903 - Train loss: 2787821.216828479\n",
            "Iteration 2424 - Batch 618/903 - Train loss: 2787370.266155089\n",
            "Iteration 2425 - Batch 619/903 - Train loss: 2787160.2883064514\n",
            "Iteration 2426 - Batch 620/903 - Train loss: 2786693.303140097\n",
            "Iteration 2427 - Batch 621/903 - Train loss: 2786346.627813505\n",
            "Iteration 2428 - Batch 622/903 - Train loss: 2785935.6998394863\n",
            "Iteration 2429 - Batch 623/903 - Train loss: 2785753.296474359\n",
            "Iteration 2430 - Batch 624/903 - Train loss: 2785372.79\n",
            "Iteration 2431 - Batch 625/903 - Train loss: 2785030.1178115015\n",
            "Iteration 2432 - Batch 626/903 - Train loss: 2784503.1184210526\n",
            "Iteration 2433 - Batch 627/903 - Train loss: 2784287.5935509554\n",
            "Iteration 2434 - Batch 628/903 - Train loss: 2784178.4236883945\n",
            "Iteration 2435 - Batch 629/903 - Train loss: 2783920.1865079366\n",
            "Iteration 2436 - Batch 630/903 - Train loss: 2784142.8831220283\n",
            "Iteration 2437 - Batch 631/903 - Train loss: 2783496.108386076\n",
            "Iteration 2438 - Batch 632/903 - Train loss: 2783098.501184834\n",
            "Iteration 2439 - Batch 633/903 - Train loss: 2783217.6025236594\n",
            "Iteration 2440 - Batch 634/903 - Train loss: 2783020.420472441\n",
            "Iteration 2441 - Batch 635/903 - Train loss: 2782843.5719339624\n",
            "Iteration 2442 - Batch 636/903 - Train loss: 2782615.5977237048\n",
            "Iteration 2443 - Batch 637/903 - Train loss: 2782558.5893416926\n",
            "Iteration 2444 - Batch 638/903 - Train loss: 2782446.998826291\n",
            "Iteration 2445 - Batch 639/903 - Train loss: 2782311.114453125\n",
            "Iteration 2446 - Batch 640/903 - Train loss: 2781823.244149766\n",
            "Iteration 2447 - Batch 641/903 - Train loss: 2781629.6409657323\n",
            "Iteration 2448 - Batch 642/903 - Train loss: 2781302.66874028\n",
            "Iteration 2449 - Batch 643/903 - Train loss: 2780993.399068323\n",
            "Iteration 2450 - Batch 644/903 - Train loss: 2781193.0089147286\n",
            "Iteration 2451 - Batch 645/903 - Train loss: 2780915.0220588236\n",
            "Iteration 2452 - Batch 646/903 - Train loss: 2780733.1901081917\n",
            "Iteration 2453 - Batch 647/903 - Train loss: 2780396.3838734566\n",
            "Iteration 2454 - Batch 648/903 - Train loss: 2779881.4872881356\n",
            "Iteration 2455 - Batch 649/903 - Train loss: 2779590.5073076924\n",
            "Iteration 2456 - Batch 650/903 - Train loss: 2779531.2830261136\n",
            "Iteration 2457 - Batch 651/903 - Train loss: 2779679.0640337425\n",
            "Iteration 2458 - Batch 652/903 - Train loss: 2779974.9452526798\n",
            "Iteration 2459 - Batch 653/903 - Train loss: 2779936.153287462\n",
            "Iteration 2460 - Batch 654/903 - Train loss: 2779784.502671756\n",
            "Iteration 2461 - Batch 655/903 - Train loss: 2779724.2389481706\n",
            "Iteration 2462 - Batch 656/903 - Train loss: 2779546.25456621\n",
            "Iteration 2463 - Batch 657/903 - Train loss: 2779460.0987841943\n",
            "Iteration 2464 - Batch 658/903 - Train loss: 2779073.5193474963\n",
            "Iteration 2465 - Batch 659/903 - Train loss: 2778540.521590909\n",
            "Iteration 2466 - Batch 660/903 - Train loss: 2778347.441376702\n",
            "Iteration 2467 - Batch 661/903 - Train loss: 2778029.145770393\n",
            "Iteration 2468 - Batch 662/903 - Train loss: 2778029.069381599\n",
            "Iteration 2469 - Batch 663/903 - Train loss: 2777866.360316265\n",
            "Iteration 2470 - Batch 664/903 - Train loss: 2777513.986842105\n",
            "Iteration 2471 - Batch 665/903 - Train loss: 2777181.033033033\n",
            "Iteration 2472 - Batch 666/903 - Train loss: 2776702.5007496253\n",
            "Iteration 2473 - Batch 667/903 - Train loss: 2776850.691991018\n",
            "Iteration 2474 - Batch 668/903 - Train loss: 2776437.322122571\n",
            "Iteration 2475 - Batch 669/903 - Train loss: 2775988.0414179103\n",
            "Iteration 2476 - Batch 670/903 - Train loss: 2775774.5890462\n",
            "Iteration 2477 - Batch 671/903 - Train loss: 2775357.046875\n",
            "Iteration 2478 - Batch 672/903 - Train loss: 2775313.22102526\n",
            "Iteration 2479 - Batch 673/903 - Train loss: 2775227.0838278932\n",
            "Iteration 2480 - Batch 674/903 - Train loss: 2775296.9196296297\n",
            "Iteration 2481 - Batch 675/903 - Train loss: 2775263.610576923\n",
            "Iteration 2482 - Batch 676/903 - Train loss: 2775344.8257016246\n",
            "Iteration 2483 - Batch 677/903 - Train loss: 2775510.2562684366\n",
            "Iteration 2484 - Batch 678/903 - Train loss: 2775592.3932253313\n",
            "Iteration 2485 - Batch 679/903 - Train loss: 2776286.880882353\n",
            "Iteration 2486 - Batch 680/903 - Train loss: 2776645.9974302496\n",
            "Iteration 2487 - Batch 681/903 - Train loss: 2776432.8643695014\n",
            "Iteration 2488 - Batch 682/903 - Train loss: 2776791.425695461\n",
            "Iteration 2489 - Batch 683/903 - Train loss: 2776670.5544590643\n",
            "Iteration 2490 - Batch 684/903 - Train loss: 2776712.8974452554\n",
            "Iteration 2491 - Batch 685/903 - Train loss: 2776494.220116618\n",
            "Iteration 2492 - Batch 686/903 - Train loss: 2776192.0065502184\n",
            "Iteration 2493 - Batch 687/903 - Train loss: 2775780.206758721\n",
            "Iteration 2494 - Batch 688/903 - Train loss: 2775747.140058055\n",
            "Iteration 2495 - Batch 689/903 - Train loss: 2776035.795652174\n",
            "Iteration 2496 - Batch 690/903 - Train loss: 2776468.756512301\n",
            "Iteration 2497 - Batch 691/903 - Train loss: 2776551.949783237\n",
            "Iteration 2498 - Batch 692/903 - Train loss: 2776513.4404761903\n",
            "Iteration 2499 - Batch 693/903 - Train loss: 2776337.0425072047\n",
            "Iteration 2500 - Batch 694/903 - Train loss: 2776047.1528776977\n",
            "Iteration 2501 - Batch 695/903 - Train loss: 2776430.512571839\n",
            "Iteration 2502 - Batch 696/903 - Train loss: 2775888.055954089\n",
            "Iteration 2503 - Batch 697/903 - Train loss: 2775652.011103152\n",
            "Iteration 2504 - Batch 698/903 - Train loss: 2775408.486051502\n",
            "Iteration 2505 - Batch 699/903 - Train loss: 2775421.882142857\n",
            "Iteration 2506 - Batch 700/903 - Train loss: 2775377.25\n",
            "Iteration 2507 - Batch 701/903 - Train loss: 2774830.839031339\n",
            "Iteration 2508 - Batch 702/903 - Train loss: 2775061.585704125\n",
            "Iteration 2509 - Batch 703/903 - Train loss: 2775389.733309659\n",
            "Iteration 2510 - Batch 704/903 - Train loss: 2775226.3418439715\n",
            "Iteration 2511 - Batch 705/903 - Train loss: 2775335.3491501417\n",
            "Iteration 2512 - Batch 706/903 - Train loss: 2775033.246463932\n",
            "Iteration 2513 - Batch 707/903 - Train loss: 2774674.576977401\n",
            "Iteration 2514 - Batch 708/903 - Train loss: 2774708.473554302\n",
            "Iteration 2515 - Batch 709/903 - Train loss: 2774685.9883802817\n",
            "Iteration 2516 - Batch 710/903 - Train loss: 2774664.7812939524\n",
            "Iteration 2517 - Batch 711/903 - Train loss: 2774187.0491573033\n",
            "Iteration 2518 - Batch 712/903 - Train loss: 2773844.6616409537\n",
            "Iteration 2519 - Batch 713/903 - Train loss: 2773567.8963585435\n",
            "Iteration 2520 - Batch 714/903 - Train loss: 2773212.76993007\n",
            "Iteration 2521 - Batch 715/903 - Train loss: 2772900.7842178773\n",
            "Iteration 2522 - Batch 716/903 - Train loss: 2772676.1956066946\n",
            "Iteration 2523 - Batch 717/903 - Train loss: 2772405.827994429\n",
            "Iteration 2524 - Batch 718/903 - Train loss: 2772309.719054242\n",
            "Iteration 2525 - Batch 719/903 - Train loss: 2771896.4850694444\n",
            "Iteration 2526 - Batch 720/903 - Train loss: 2771912.7149791955\n",
            "Iteration 2527 - Batch 721/903 - Train loss: 2771577.7915512463\n",
            "Iteration 2528 - Batch 722/903 - Train loss: 2771497.2036652835\n",
            "Iteration 2529 - Batch 723/903 - Train loss: 2770947.1391574587\n",
            "Iteration 2530 - Batch 724/903 - Train loss: 2770781.0117241377\n",
            "Iteration 2531 - Batch 725/903 - Train loss: 2770584.727272727\n",
            "Iteration 2532 - Batch 726/903 - Train loss: 2770750.5082530947\n",
            "Iteration 2533 - Batch 727/903 - Train loss: 2770595.4965659343\n",
            "Iteration 2534 - Batch 728/903 - Train loss: 2770297.3360768175\n",
            "Iteration 2535 - Batch 729/903 - Train loss: 2769965.823630137\n",
            "Iteration 2536 - Batch 730/903 - Train loss: 2769588.8553351574\n",
            "Iteration 2537 - Batch 731/903 - Train loss: 2769083.7459016396\n",
            "Iteration 2538 - Batch 732/903 - Train loss: 2768781.269099591\n",
            "Iteration 2539 - Batch 733/903 - Train loss: 2768562.88726158\n",
            "Iteration 2540 - Batch 734/903 - Train loss: 2768097.8612244898\n",
            "Iteration 2541 - Batch 735/903 - Train loss: 2767783.1569293477\n",
            "Iteration 2542 - Batch 736/903 - Train loss: 2767673.467774763\n",
            "Iteration 2543 - Batch 737/903 - Train loss: 2767158.288617886\n",
            "Iteration 2544 - Batch 738/903 - Train loss: 2766524.6204330176\n",
            "Iteration 2545 - Batch 739/903 - Train loss: 2765973.7195945946\n",
            "Iteration 2546 - Batch 740/903 - Train loss: 2765566.654183536\n",
            "Iteration 2547 - Batch 741/903 - Train loss: 2765421.620283019\n",
            "Iteration 2548 - Batch 742/903 - Train loss: 2765124.038021534\n",
            "Iteration 2549 - Batch 743/903 - Train loss: 2764919.1370967743\n",
            "Iteration 2550 - Batch 744/903 - Train loss: 2765043.9610738256\n",
            "Iteration 2551 - Batch 745/903 - Train loss: 2764587.5968498657\n",
            "Iteration 2552 - Batch 746/903 - Train loss: 2764220.93373494\n",
            "Iteration 2553 - Batch 747/903 - Train loss: 2763976.118649733\n",
            "Iteration 2554 - Batch 748/903 - Train loss: 2763979.8674899866\n",
            "Iteration 2555 - Batch 749/903 - Train loss: 2764398.469666667\n",
            "Iteration 2556 - Batch 750/903 - Train loss: 2764712.4503994673\n",
            "Iteration 2557 - Batch 751/903 - Train loss: 2765055.158577128\n",
            "Iteration 2558 - Batch 752/903 - Train loss: 2764875.7695883135\n",
            "Iteration 2559 - Batch 753/903 - Train loss: 2764602.520557029\n",
            "Iteration 2560 - Batch 754/903 - Train loss: 2764227.668874172\n",
            "Iteration 2561 - Batch 755/903 - Train loss: 2763929.185515873\n",
            "Iteration 2562 - Batch 756/903 - Train loss: 2763318.241413474\n",
            "Iteration 2563 - Batch 757/903 - Train loss: 2762950.6698548812\n",
            "Iteration 2564 - Batch 758/903 - Train loss: 2762782.69400527\n",
            "Iteration 2565 - Batch 759/903 - Train loss: 2763017.5914473683\n",
            "Iteration 2566 - Batch 760/903 - Train loss: 2762867.2565703024\n",
            "Iteration 2567 - Batch 761/903 - Train loss: 2762634.031496063\n",
            "Iteration 2568 - Batch 762/903 - Train loss: 2762493.6877457406\n",
            "Iteration 2569 - Batch 763/903 - Train loss: 2762199.61289267\n",
            "Iteration 2570 - Batch 764/903 - Train loss: 2761903.421895425\n",
            "Iteration 2571 - Batch 765/903 - Train loss: 2761804.5127284597\n",
            "Iteration 2572 - Batch 766/903 - Train loss: 2761708.043024772\n",
            "Iteration 2573 - Batch 767/903 - Train loss: 2761143.3645833335\n",
            "Iteration 2574 - Batch 768/903 - Train loss: 2760761.8010403123\n",
            "Iteration 2575 - Batch 769/903 - Train loss: 2760632.027922078\n",
            "Iteration 2576 - Batch 770/903 - Train loss: 2760329.153372244\n",
            "Iteration 2577 - Batch 771/903 - Train loss: 2759886.697215026\n",
            "Iteration 2578 - Batch 772/903 - Train loss: 2759448.8088615783\n",
            "Iteration 2579 - Batch 773/903 - Train loss: 2759206.944121447\n",
            "Iteration 2580 - Batch 774/903 - Train loss: 2758945.3432258065\n",
            "Iteration 2581 - Batch 775/903 - Train loss: 2758740.004188144\n",
            "Iteration 2582 - Batch 776/903 - Train loss: 2758419.831081081\n",
            "Iteration 2583 - Batch 777/903 - Train loss: 2758106.1455655526\n",
            "Iteration 2584 - Batch 778/903 - Train loss: 2758295.974326059\n",
            "Iteration 2585 - Batch 779/903 - Train loss: 2758155.1782051283\n",
            "Iteration 2586 - Batch 780/903 - Train loss: 2757787.1056338027\n",
            "Iteration 2587 - Batch 781/903 - Train loss: 2757640.1707161125\n",
            "Iteration 2588 - Batch 782/903 - Train loss: 2757629.6526181353\n",
            "Iteration 2589 - Batch 783/903 - Train loss: 2757636.8683035714\n",
            "Iteration 2590 - Batch 784/903 - Train loss: 2757208.7589171976\n",
            "Iteration 2591 - Batch 785/903 - Train loss: 2756861.1526717558\n",
            "Iteration 2592 - Batch 786/903 - Train loss: 2756703.869758577\n",
            "Iteration 2593 - Batch 787/903 - Train loss: 2756331.7944162437\n",
            "Iteration 2594 - Batch 788/903 - Train loss: 2755990.7100760457\n",
            "Iteration 2595 - Batch 789/903 - Train loss: 2755739.2506329115\n",
            "Iteration 2596 - Batch 790/903 - Train loss: 2755566.7512642224\n",
            "Iteration 2597 - Batch 791/903 - Train loss: 2755464.053030303\n",
            "Iteration 2598 - Batch 792/903 - Train loss: 2755139.9079445144\n",
            "Iteration 2599 - Batch 793/903 - Train loss: 2754724.6961586904\n",
            "Iteration 2600 - Batch 794/903 - Train loss: 2754402.9179245285\n",
            "Iteration 2601 - Batch 795/903 - Train loss: 2754018.139447236\n",
            "Iteration 2602 - Batch 796/903 - Train loss: 2753822.4222082812\n",
            "Iteration 2603 - Batch 797/903 - Train loss: 2753340.0003132834\n",
            "Iteration 2604 - Batch 798/903 - Train loss: 2753331.5735294116\n",
            "Iteration 2605 - Batch 799/903 - Train loss: 2752760.07125\n",
            "Iteration 2606 - Batch 800/903 - Train loss: 2752629.3217852684\n",
            "Iteration 2607 - Batch 801/903 - Train loss: 2752306.922693267\n",
            "Iteration 2608 - Batch 802/903 - Train loss: 2751780.5395392277\n",
            "Iteration 2609 - Batch 803/903 - Train loss: 2751398.643034826\n",
            "Iteration 2610 - Batch 804/903 - Train loss: 2751105.834782609\n",
            "Iteration 2611 - Batch 805/903 - Train loss: 2750810.040012407\n",
            "Iteration 2612 - Batch 806/903 - Train loss: 2750401.822490706\n",
            "Iteration 2613 - Batch 807/903 - Train loss: 2750047.7469059406\n",
            "Iteration 2614 - Batch 808/903 - Train loss: 2749653.8272558716\n",
            "Iteration 2615 - Batch 809/903 - Train loss: 2749091.114197531\n",
            "Iteration 2616 - Batch 810/903 - Train loss: 2748553.8782367446\n",
            "Iteration 2617 - Batch 811/903 - Train loss: 2748303.798029557\n",
            "Iteration 2618 - Batch 812/903 - Train loss: 2747972.9160516607\n",
            "Iteration 2619 - Batch 813/903 - Train loss: 2747599.813267813\n",
            "Iteration 2620 - Batch 814/903 - Train loss: 2747242.9957055217\n",
            "Iteration 2621 - Batch 815/903 - Train loss: 2746757.2984068627\n",
            "Iteration 2622 - Batch 816/903 - Train loss: 2746331.6808445533\n",
            "Iteration 2623 - Batch 817/903 - Train loss: 2746095.2845354523\n",
            "Iteration 2624 - Batch 818/903 - Train loss: 2745823.8617216116\n",
            "Iteration 2625 - Batch 819/903 - Train loss: 2745589.076219512\n",
            "Iteration 2626 - Batch 820/903 - Train loss: 2745199.961632156\n",
            "Iteration 2627 - Batch 821/903 - Train loss: 2744995.4066301705\n",
            "Iteration 2628 - Batch 822/903 - Train loss: 2744614.294653706\n",
            "Iteration 2629 - Batch 823/903 - Train loss: 2744132.610436893\n",
            "Iteration 2630 - Batch 824/903 - Train loss: 2743805.188181818\n",
            "Iteration 2631 - Batch 825/903 - Train loss: 2743231.4415859566\n",
            "Iteration 2632 - Batch 826/903 - Train loss: 2742590.181076179\n",
            "Iteration 2633 - Batch 827/903 - Train loss: 2742261.2617753623\n",
            "Iteration 2634 - Batch 828/903 - Train loss: 2741885.842279855\n",
            "Iteration 2635 - Batch 829/903 - Train loss: 2741501.109638554\n",
            "Iteration 2636 - Batch 830/903 - Train loss: 2741213.2629362214\n",
            "Iteration 2637 - Batch 831/903 - Train loss: 2741064.2259615385\n",
            "Iteration 2638 - Batch 832/903 - Train loss: 2740570.6365546216\n",
            "Iteration 2639 - Batch 833/903 - Train loss: 2740313.9289568346\n",
            "Iteration 2640 - Batch 834/903 - Train loss: 2739925.8976047905\n",
            "Iteration 2641 - Batch 835/903 - Train loss: 2739765.624401914\n",
            "Iteration 2642 - Batch 836/903 - Train loss: 2739320.029271207\n",
            "Iteration 2643 - Batch 837/903 - Train loss: 2739028.5501193316\n",
            "Iteration 2644 - Batch 838/903 - Train loss: 2738630.052741359\n",
            "Iteration 2645 - Batch 839/903 - Train loss: 2738502.595833333\n",
            "Iteration 2646 - Batch 840/903 - Train loss: 2738184.464625446\n",
            "Iteration 2647 - Batch 841/903 - Train loss: 2737912.312648456\n",
            "Iteration 2648 - Batch 842/903 - Train loss: 2737696.6548042702\n",
            "Iteration 2649 - Batch 843/903 - Train loss: 2737231.4351303317\n",
            "Iteration 2650 - Batch 844/903 - Train loss: 2737045.345857988\n",
            "Iteration 2651 - Batch 845/903 - Train loss: 2736956.209219858\n",
            "Iteration 2652 - Batch 846/903 - Train loss: 2736859.9566115704\n",
            "Iteration 2653 - Batch 847/903 - Train loss: 2736633.2597287735\n",
            "Iteration 2654 - Batch 848/903 - Train loss: 2736505.9702591286\n",
            "Iteration 2655 - Batch 849/903 - Train loss: 2736047.586470588\n",
            "Iteration 2656 - Batch 850/903 - Train loss: 2736002.502056404\n",
            "Iteration 2657 - Batch 851/903 - Train loss: 2736385.4512910796\n",
            "Iteration 2658 - Batch 852/903 - Train loss: 2736661.092907386\n",
            "Iteration 2659 - Batch 853/903 - Train loss: 2736600.3387002344\n",
            "Iteration 2660 - Batch 854/903 - Train loss: 2736415.142105263\n",
            "Iteration 2661 - Batch 855/903 - Train loss: 2736325.570385514\n",
            "Iteration 2662 - Batch 856/903 - Train loss: 2736203.1998249707\n",
            "Iteration 2663 - Batch 857/903 - Train loss: 2736091.9857226107\n",
            "Iteration 2664 - Batch 858/903 - Train loss: 2736062.207217695\n",
            "Iteration 2665 - Batch 859/903 - Train loss: 2735986.4375\n",
            "Iteration 2666 - Batch 860/903 - Train loss: 2735765.0087108016\n",
            "Iteration 2667 - Batch 861/903 - Train loss: 2735394.5507540605\n",
            "Iteration 2668 - Batch 862/903 - Train loss: 2735186.478563152\n",
            "Iteration 2669 - Batch 863/903 - Train loss: 2735107.769675926\n",
            "Iteration 2670 - Batch 864/903 - Train loss: 2735170.4080924857\n",
            "Iteration 2671 - Batch 865/903 - Train loss: 2734784.2618360277\n",
            "Iteration 2672 - Batch 866/903 - Train loss: 2734298.6286043827\n",
            "Iteration 2673 - Batch 867/903 - Train loss: 2734406.839285714\n",
            "Iteration 2674 - Batch 868/903 - Train loss: 2734804.5397008057\n",
            "Iteration 2675 - Batch 869/903 - Train loss: 2735299.8833333333\n",
            "Iteration 2676 - Batch 870/903 - Train loss: 2735701.824052813\n",
            "Iteration 2677 - Batch 871/903 - Train loss: 2735684.9036697247\n",
            "Iteration 2678 - Batch 872/903 - Train loss: 2735239.758591065\n",
            "Iteration 2679 - Batch 873/903 - Train loss: 2735025.0168764303\n",
            "Iteration 2680 - Batch 874/903 - Train loss: 2735249.608\n",
            "Iteration 2681 - Batch 875/903 - Train loss: 2735622.702910959\n",
            "Iteration 2682 - Batch 876/903 - Train loss: 2735391.02622577\n",
            "Iteration 2683 - Batch 877/903 - Train loss: 2735206.3986332575\n",
            "Iteration 2684 - Batch 878/903 - Train loss: 2735271.8688850966\n",
            "Iteration 2685 - Batch 879/903 - Train loss: 2734939.261931818\n",
            "Iteration 2686 - Batch 880/903 - Train loss: 2734725.717366629\n",
            "Iteration 2687 - Batch 881/903 - Train loss: 2734592.2454648525\n",
            "Iteration 2688 - Batch 882/903 - Train loss: 2734266.6356172143\n",
            "Iteration 2689 - Batch 883/903 - Train loss: 2733941.080882353\n",
            "Iteration 2690 - Batch 884/903 - Train loss: 2733650.3324858756\n",
            "Iteration 2691 - Batch 885/903 - Train loss: 2733522.421839729\n",
            "Iteration 2692 - Batch 886/903 - Train loss: 2733402.1161217587\n",
            "Iteration 2693 - Batch 887/903 - Train loss: 2733123.7395833335\n",
            "Iteration 2694 - Batch 888/903 - Train loss: 2732899.7249718783\n",
            "Iteration 2695 - Batch 889/903 - Train loss: 2732676.620786517\n",
            "Iteration 2696 - Batch 890/903 - Train loss: 2732611.906285073\n",
            "Iteration 2697 - Batch 891/903 - Train loss: 2732115.360426009\n",
            "Iteration 2698 - Batch 892/903 - Train loss: 2731800.149776036\n",
            "Iteration 2699 - Batch 893/903 - Train loss: 2731506.5318791945\n",
            "Iteration 2700 - Batch 894/903 - Train loss: 2731288.7776536313\n",
            "Iteration 2701 - Batch 895/903 - Train loss: 2731494.2653459823\n",
            "Iteration 2702 - Batch 896/903 - Train loss: 2731694.350055741\n",
            "Iteration 2703 - Batch 897/903 - Train loss: 2731733.756403118\n",
            "Iteration 2704 - Batch 898/903 - Train loss: 2731468.9007230257\n",
            "Iteration 2705 - Batch 899/903 - Train loss: 2731595.9172222223\n",
            "Iteration 2706 - Batch 900/903 - Train loss: 2732003.445615982\n",
            "Iteration 2707 - Batch 901/903 - Train loss: 2732646.1136363638\n",
            "Iteration 2708 - Batch 902/903 - Train loss: 2730337.5174418604\n",
            "Val loss: 2862852.125\n",
            "Epoch 4/6\n",
            "Iteration 2710 - Batch 1/903 - Train loss: 5633912.375\n",
            "Iteration 2711 - Batch 2/903 - Train loss: 5381611.75\n",
            "Iteration 2712 - Batch 3/903 - Train loss: 4686366.9375\n",
            "Iteration 2713 - Batch 4/903 - Train loss: 4670707.95\n",
            "Iteration 2714 - Batch 5/903 - Train loss: 4557133.666666667\n",
            "Iteration 2715 - Batch 6/903 - Train loss: 4324002.321428572\n",
            "Iteration 2716 - Batch 7/903 - Train loss: 4203366.125\n",
            "Iteration 2717 - Batch 8/903 - Train loss: 4067118.638888889\n",
            "Iteration 2718 - Batch 9/903 - Train loss: 3936776.025\n",
            "Iteration 2719 - Batch 10/903 - Train loss: 3844211.5681818184\n",
            "Iteration 2720 - Batch 11/903 - Train loss: 3755825.7291666665\n",
            "Iteration 2721 - Batch 12/903 - Train loss: 3692156.3846153845\n",
            "Iteration 2722 - Batch 13/903 - Train loss: 3631157.1785714286\n",
            "Iteration 2723 - Batch 14/903 - Train loss: 3579591.2333333334\n",
            "Iteration 2724 - Batch 15/903 - Train loss: 3550324.828125\n",
            "Iteration 2725 - Batch 16/903 - Train loss: 3510677.3088235296\n",
            "Iteration 2726 - Batch 17/903 - Train loss: 3466394.2916666665\n",
            "Iteration 2727 - Batch 18/903 - Train loss: 3441094.8289473685\n",
            "Iteration 2728 - Batch 19/903 - Train loss: 3402714.9375\n",
            "Iteration 2729 - Batch 20/903 - Train loss: 3366051.035714286\n",
            "Iteration 2730 - Batch 21/903 - Train loss: 3323330.272727273\n",
            "Iteration 2731 - Batch 22/903 - Train loss: 3293293.9782608696\n",
            "Iteration 2732 - Batch 23/903 - Train loss: 3262150.2083333335\n",
            "Iteration 2733 - Batch 24/903 - Train loss: 3233351.03\n",
            "Iteration 2734 - Batch 25/903 - Train loss: 3215123.576923077\n",
            "Iteration 2735 - Batch 26/903 - Train loss: 3189713.8518518517\n",
            "Iteration 2736 - Batch 27/903 - Train loss: 3152620.473214286\n",
            "Iteration 2737 - Batch 28/903 - Train loss: 3136089.9051724137\n",
            "Iteration 2738 - Batch 29/903 - Train loss: 3122958.4166666665\n",
            "Iteration 2739 - Batch 30/903 - Train loss: 3112685.5241935486\n",
            "Iteration 2740 - Batch 31/903 - Train loss: 3103421.8515625\n",
            "Iteration 2741 - Batch 32/903 - Train loss: 3097754.9015151514\n",
            "Iteration 2742 - Batch 33/903 - Train loss: 3081092.169117647\n",
            "Iteration 2743 - Batch 34/903 - Train loss: 3070285.064285714\n",
            "Iteration 2744 - Batch 35/903 - Train loss: 3055468.763888889\n",
            "Iteration 2745 - Batch 36/903 - Train loss: 3043227.8445945946\n",
            "Iteration 2746 - Batch 37/903 - Train loss: 3029992.710526316\n",
            "Iteration 2747 - Batch 38/903 - Train loss: 3015656.6987179485\n",
            "Iteration 2748 - Batch 39/903 - Train loss: 3005312.79375\n",
            "Iteration 2749 - Batch 40/903 - Train loss: 2990384.676829268\n",
            "Iteration 2750 - Batch 41/903 - Train loss: 2979498.1845238097\n",
            "Iteration 2751 - Batch 42/903 - Train loss: 2965297.854651163\n",
            "Iteration 2752 - Batch 43/903 - Train loss: 2956600.659090909\n",
            "Iteration 2753 - Batch 44/903 - Train loss: 2946265.422222222\n",
            "Iteration 2754 - Batch 45/903 - Train loss: 2933969.804347826\n",
            "Iteration 2755 - Batch 46/903 - Train loss: 2925853.723404255\n",
            "Iteration 2756 - Batch 47/903 - Train loss: 2913017.9114583335\n",
            "Iteration 2757 - Batch 48/903 - Train loss: 2906015.6428571427\n",
            "Iteration 2758 - Batch 49/903 - Train loss: 2900806.005\n",
            "Iteration 2759 - Batch 50/903 - Train loss: 2893224.343137255\n",
            "Iteration 2760 - Batch 51/903 - Train loss: 2880816.7115384615\n",
            "Iteration 2761 - Batch 52/903 - Train loss: 2875241.966981132\n",
            "Iteration 2762 - Batch 53/903 - Train loss: 2869774.2268518517\n",
            "Iteration 2763 - Batch 54/903 - Train loss: 2868299.7590909093\n",
            "Iteration 2764 - Batch 55/903 - Train loss: 2862466.316964286\n",
            "Iteration 2765 - Batch 56/903 - Train loss: 2855846.850877193\n",
            "Iteration 2766 - Batch 57/903 - Train loss: 2846258.672413793\n",
            "Iteration 2767 - Batch 58/903 - Train loss: 2836457.059322034\n",
            "Iteration 2768 - Batch 59/903 - Train loss: 2830812.691666667\n",
            "Iteration 2769 - Batch 60/903 - Train loss: 2827546.4672131147\n",
            "Iteration 2770 - Batch 61/903 - Train loss: 2820875.310483871\n",
            "Iteration 2771 - Batch 62/903 - Train loss: 2814375.9087301586\n",
            "Iteration 2772 - Batch 63/903 - Train loss: 2810616.69921875\n",
            "Iteration 2773 - Batch 64/903 - Train loss: 2811808.2884615385\n",
            "Iteration 2774 - Batch 65/903 - Train loss: 2805693.768939394\n",
            "Iteration 2775 - Batch 66/903 - Train loss: 2801276.1791044776\n",
            "Iteration 2776 - Batch 67/903 - Train loss: 2799010.779411765\n",
            "Iteration 2777 - Batch 68/903 - Train loss: 2792999.0217391304\n",
            "Iteration 2778 - Batch 69/903 - Train loss: 2790934.3642857145\n",
            "Iteration 2779 - Batch 70/903 - Train loss: 2782857.795774648\n",
            "Iteration 2780 - Batch 71/903 - Train loss: 2778442.0625\n",
            "Iteration 2781 - Batch 72/903 - Train loss: 2773981.712328767\n",
            "Iteration 2782 - Batch 73/903 - Train loss: 2769719.4054054054\n",
            "Iteration 2783 - Batch 74/903 - Train loss: 2768408.0633333335\n",
            "Iteration 2784 - Batch 75/903 - Train loss: 2765236.8388157897\n",
            "Iteration 2785 - Batch 76/903 - Train loss: 2760264.4188311687\n",
            "Iteration 2786 - Batch 77/903 - Train loss: 2756101.012820513\n",
            "Iteration 2787 - Batch 78/903 - Train loss: 2752046.0664556962\n",
            "Iteration 2788 - Batch 79/903 - Train loss: 2748308.309375\n",
            "Iteration 2789 - Batch 80/903 - Train loss: 2745537.774691358\n",
            "Iteration 2790 - Batch 81/903 - Train loss: 2743173.8323170734\n",
            "Iteration 2791 - Batch 82/903 - Train loss: 2738279.825301205\n",
            "Iteration 2792 - Batch 83/903 - Train loss: 2735971.8035714286\n",
            "Iteration 2793 - Batch 84/903 - Train loss: 2734016.867647059\n",
            "Iteration 2794 - Batch 85/903 - Train loss: 2732837.069767442\n",
            "Iteration 2795 - Batch 86/903 - Train loss: 2733187.824712644\n",
            "Iteration 2796 - Batch 87/903 - Train loss: 2732745.4857954546\n",
            "Iteration 2797 - Batch 88/903 - Train loss: 2729628.452247191\n",
            "Iteration 2798 - Batch 89/903 - Train loss: 2728234.7472222224\n",
            "Iteration 2799 - Batch 90/903 - Train loss: 2727014.241758242\n",
            "Iteration 2800 - Batch 91/903 - Train loss: 2725208.6440217393\n",
            "Iteration 2801 - Batch 92/903 - Train loss: 2722830.255376344\n",
            "Iteration 2802 - Batch 93/903 - Train loss: 2719958.188829787\n",
            "Iteration 2803 - Batch 94/903 - Train loss: 2715748.613157895\n",
            "Iteration 2804 - Batch 95/903 - Train loss: 2710839.8541666665\n",
            "Iteration 2805 - Batch 96/903 - Train loss: 2705798.345360825\n",
            "Iteration 2806 - Batch 97/903 - Train loss: 2702646.448979592\n",
            "Iteration 2807 - Batch 98/903 - Train loss: 2702866.1792929294\n",
            "Iteration 2808 - Batch 99/903 - Train loss: 2701042.315\n",
            "Iteration 2809 - Batch 100/903 - Train loss: 2698501.287128713\n",
            "Iteration 2810 - Batch 101/903 - Train loss: 2697710.068627451\n",
            "Iteration 2811 - Batch 102/903 - Train loss: 2692965.2524271845\n",
            "Iteration 2812 - Batch 103/903 - Train loss: 2688897.4927884615\n",
            "Iteration 2813 - Batch 104/903 - Train loss: 2686998.8976190477\n",
            "Iteration 2814 - Batch 105/903 - Train loss: 2687224.2641509436\n",
            "Iteration 2815 - Batch 106/903 - Train loss: 2683994.448598131\n",
            "Iteration 2816 - Batch 107/903 - Train loss: 2682677.1805555555\n",
            "Iteration 2817 - Batch 108/903 - Train loss: 2679976.961009174\n",
            "Iteration 2818 - Batch 109/903 - Train loss: 2680443.0090909093\n",
            "Iteration 2819 - Batch 110/903 - Train loss: 2678019.4662162163\n",
            "Iteration 2820 - Batch 111/903 - Train loss: 2676101.5491071427\n",
            "Iteration 2821 - Batch 112/903 - Train loss: 2674497.7699115044\n",
            "Iteration 2822 - Batch 113/903 - Train loss: 2672331.7127192984\n",
            "Iteration 2823 - Batch 114/903 - Train loss: 2673824.8673913046\n",
            "Iteration 2824 - Batch 115/903 - Train loss: 2673315.6788793104\n",
            "Iteration 2825 - Batch 116/903 - Train loss: 2670987.4743589745\n",
            "Iteration 2826 - Batch 117/903 - Train loss: 2671739.319915254\n",
            "Iteration 2827 - Batch 118/903 - Train loss: 2669098.972689076\n",
            "Iteration 2828 - Batch 119/903 - Train loss: 2668529.3583333334\n",
            "Iteration 2829 - Batch 120/903 - Train loss: 2665686.8491735538\n",
            "Iteration 2830 - Batch 121/903 - Train loss: 2662652.9282786883\n",
            "Iteration 2831 - Batch 122/903 - Train loss: 2661351.036585366\n",
            "Iteration 2832 - Batch 123/903 - Train loss: 2660515.2177419355\n",
            "Iteration 2833 - Batch 124/903 - Train loss: 2661073.518\n",
            "Iteration 2834 - Batch 125/903 - Train loss: 2658303.238095238\n",
            "Iteration 2835 - Batch 126/903 - Train loss: 2655257.940944882\n",
            "Iteration 2836 - Batch 127/903 - Train loss: 2655138.837890625\n",
            "Iteration 2837 - Batch 128/903 - Train loss: 2653840.7868217053\n",
            "Iteration 2838 - Batch 129/903 - Train loss: 2651943.417307692\n",
            "Iteration 2839 - Batch 130/903 - Train loss: 2652120.013358779\n",
            "Iteration 2840 - Batch 131/903 - Train loss: 2650750.1893939395\n",
            "Iteration 2841 - Batch 132/903 - Train loss: 2650309.323308271\n",
            "Iteration 2842 - Batch 133/903 - Train loss: 2651015.031716418\n",
            "Iteration 2843 - Batch 134/903 - Train loss: 2650041.3574074074\n",
            "Iteration 2844 - Batch 135/903 - Train loss: 2650926.976102941\n",
            "Iteration 2845 - Batch 136/903 - Train loss: 2648498.855839416\n",
            "Iteration 2846 - Batch 137/903 - Train loss: 2647125.740942029\n",
            "Iteration 2847 - Batch 138/903 - Train loss: 2646153.7661870504\n",
            "Iteration 2848 - Batch 139/903 - Train loss: 2644638.719642857\n",
            "Iteration 2849 - Batch 140/903 - Train loss: 2644081.5514184395\n",
            "Iteration 2850 - Batch 141/903 - Train loss: 2641985.9225352113\n",
            "Iteration 2851 - Batch 142/903 - Train loss: 2642677.8723776224\n",
            "Iteration 2852 - Batch 143/903 - Train loss: 2641163.5364583335\n",
            "Iteration 2853 - Batch 144/903 - Train loss: 2639378.124137931\n",
            "Iteration 2854 - Batch 145/903 - Train loss: 2636454.051369863\n",
            "Iteration 2855 - Batch 146/903 - Train loss: 2634164.988095238\n",
            "Iteration 2856 - Batch 147/903 - Train loss: 2631588.731418919\n",
            "Iteration 2857 - Batch 148/903 - Train loss: 2629055.139261745\n",
            "Iteration 2858 - Batch 149/903 - Train loss: 2626824.2333333334\n",
            "Iteration 2859 - Batch 150/903 - Train loss: 2625673.2798013245\n",
            "Iteration 2860 - Batch 151/903 - Train loss: 2624148.6200657897\n",
            "Iteration 2861 - Batch 152/903 - Train loss: 2623845.017973856\n",
            "Iteration 2862 - Batch 153/903 - Train loss: 2622708.1103896103\n",
            "Iteration 2863 - Batch 154/903 - Train loss: 2620554.814516129\n",
            "Iteration 2864 - Batch 155/903 - Train loss: 2620696.576923077\n",
            "Iteration 2865 - Batch 156/903 - Train loss: 2619849.378980892\n",
            "Iteration 2866 - Batch 157/903 - Train loss: 2618768.2278481014\n",
            "Iteration 2867 - Batch 158/903 - Train loss: 2618390.245283019\n",
            "Iteration 2868 - Batch 159/903 - Train loss: 2616156.9109375\n",
            "Iteration 2869 - Batch 160/903 - Train loss: 2614860.116459627\n",
            "Iteration 2870 - Batch 161/903 - Train loss: 2614557.25308642\n",
            "Iteration 2871 - Batch 162/903 - Train loss: 2615182.6411042945\n",
            "Iteration 2872 - Batch 163/903 - Train loss: 2615175.1585365855\n",
            "Iteration 2873 - Batch 164/903 - Train loss: 2615540.2363636363\n",
            "Iteration 2874 - Batch 165/903 - Train loss: 2615570.2575301207\n",
            "Iteration 2875 - Batch 166/903 - Train loss: 2613228.6721556885\n",
            "Iteration 2876 - Batch 167/903 - Train loss: 2612110.6026785714\n",
            "Iteration 2877 - Batch 168/903 - Train loss: 2613070.226331361\n",
            "Iteration 2878 - Batch 169/903 - Train loss: 2612581.8588235294\n",
            "Iteration 2879 - Batch 170/903 - Train loss: 2611709.2923976607\n",
            "Iteration 2880 - Batch 171/903 - Train loss: 2610722.6002906975\n",
            "Iteration 2881 - Batch 172/903 - Train loss: 2609103.6632947978\n",
            "Iteration 2882 - Batch 173/903 - Train loss: 2608229.077586207\n",
            "Iteration 2883 - Batch 174/903 - Train loss: 2607685.36\n",
            "Iteration 2884 - Batch 175/903 - Train loss: 2605869.705965909\n",
            "Iteration 2885 - Batch 176/903 - Train loss: 2605607.8898305083\n",
            "Iteration 2886 - Batch 177/903 - Train loss: 2604273.106741573\n",
            "Iteration 2887 - Batch 178/903 - Train loss: 2603261.399441341\n",
            "Iteration 2888 - Batch 179/903 - Train loss: 2602753.9777777777\n",
            "Iteration 2889 - Batch 180/903 - Train loss: 2603666.606353591\n",
            "Iteration 2890 - Batch 181/903 - Train loss: 2601788.561813187\n",
            "Iteration 2891 - Batch 182/903 - Train loss: 2600651.517759563\n",
            "Iteration 2892 - Batch 183/903 - Train loss: 2601712.5366847827\n",
            "Iteration 2893 - Batch 184/903 - Train loss: 2601504.4216216216\n",
            "Iteration 2894 - Batch 185/903 - Train loss: 2600223.676075269\n",
            "Iteration 2895 - Batch 186/903 - Train loss: 2598430.645721925\n",
            "Iteration 2896 - Batch 187/903 - Train loss: 2599054.904255319\n",
            "Iteration 2897 - Batch 188/903 - Train loss: 2598371.0992063493\n",
            "Iteration 2898 - Batch 189/903 - Train loss: 2598030.2131578946\n",
            "Iteration 2899 - Batch 190/903 - Train loss: 2598748.657068063\n",
            "Iteration 2900 - Batch 191/903 - Train loss: 2598374.9166666665\n",
            "Iteration 2901 - Batch 192/903 - Train loss: 2597161.1256476683\n",
            "Iteration 2902 - Batch 193/903 - Train loss: 2596044.975515464\n",
            "Iteration 2903 - Batch 194/903 - Train loss: 2594929.0243589743\n",
            "Iteration 2904 - Batch 195/903 - Train loss: 2594888.2168367347\n",
            "Iteration 2905 - Batch 196/903 - Train loss: 2594885.8223350253\n",
            "Iteration 2906 - Batch 197/903 - Train loss: 2593897.523989899\n",
            "Iteration 2907 - Batch 198/903 - Train loss: 2593948.4309045225\n",
            "Iteration 2908 - Batch 199/903 - Train loss: 2592911.55125\n",
            "Iteration 2909 - Batch 200/903 - Train loss: 2591958.3893034826\n",
            "Iteration 2910 - Batch 201/903 - Train loss: 2591741.051980198\n",
            "Iteration 2911 - Batch 202/903 - Train loss: 2591327.910098522\n",
            "Iteration 2912 - Batch 203/903 - Train loss: 2590284.658088235\n",
            "Iteration 2913 - Batch 204/903 - Train loss: 2590711.368292683\n",
            "Iteration 2914 - Batch 205/903 - Train loss: 2589604.6589805824\n",
            "Iteration 2915 - Batch 206/903 - Train loss: 2589243.9311594204\n",
            "Iteration 2916 - Batch 207/903 - Train loss: 2588391.6658653845\n",
            "Iteration 2917 - Batch 208/903 - Train loss: 2588362.715311005\n",
            "Iteration 2918 - Batch 209/903 - Train loss: 2588436.544047619\n",
            "Iteration 2919 - Batch 210/903 - Train loss: 2587162.1007109005\n",
            "Iteration 2920 - Batch 211/903 - Train loss: 2586477.991745283\n",
            "Iteration 2921 - Batch 212/903 - Train loss: 2585746.1948356805\n",
            "Iteration 2922 - Batch 213/903 - Train loss: 2584867.86682243\n",
            "Iteration 2923 - Batch 214/903 - Train loss: 2584457.173255814\n",
            "Iteration 2924 - Batch 215/903 - Train loss: 2584442.2916666665\n",
            "Iteration 2925 - Batch 216/903 - Train loss: 2585469.3732718895\n",
            "Iteration 2926 - Batch 217/903 - Train loss: 2587685.5584862386\n",
            "Iteration 2927 - Batch 218/903 - Train loss: 2590591.7385844747\n",
            "Iteration 2928 - Batch 219/903 - Train loss: 2593572.7295454545\n",
            "Iteration 2929 - Batch 220/903 - Train loss: 2595856.374434389\n",
            "Iteration 2930 - Batch 221/903 - Train loss: 2595335.2094594594\n",
            "Iteration 2931 - Batch 222/903 - Train loss: 2595108.697309417\n",
            "Iteration 2932 - Batch 223/903 - Train loss: 2595528.4006696427\n",
            "Iteration 2933 - Batch 224/903 - Train loss: 2595871.2355555557\n",
            "Iteration 2934 - Batch 225/903 - Train loss: 2596156.511061947\n",
            "Iteration 2935 - Batch 226/903 - Train loss: 2596678.428414097\n",
            "Iteration 2936 - Batch 227/903 - Train loss: 2596036.0986842103\n",
            "Iteration 2937 - Batch 228/903 - Train loss: 2596306.5524017466\n",
            "Iteration 2938 - Batch 229/903 - Train loss: 2597782.527173913\n",
            "Iteration 2939 - Batch 230/903 - Train loss: 2597758.7456709957\n",
            "Iteration 2940 - Batch 231/903 - Train loss: 2596003.0592672415\n",
            "Iteration 2941 - Batch 232/903 - Train loss: 2595324.9034334766\n",
            "Iteration 2942 - Batch 233/903 - Train loss: 2597450.7232905985\n",
            "Iteration 2943 - Batch 234/903 - Train loss: 2597181.790425532\n",
            "Iteration 2944 - Batch 235/903 - Train loss: 2596778.599576271\n",
            "Iteration 2945 - Batch 236/903 - Train loss: 2596779.5010548523\n",
            "Iteration 2946 - Batch 237/903 - Train loss: 2597198.444327731\n",
            "Iteration 2947 - Batch 238/903 - Train loss: 2596038.0167364017\n",
            "Iteration 2948 - Batch 239/903 - Train loss: 2596063.175\n",
            "Iteration 2949 - Batch 240/903 - Train loss: 2596079.824688797\n",
            "Iteration 2950 - Batch 241/903 - Train loss: 2596107.257231405\n",
            "Iteration 2951 - Batch 242/903 - Train loss: 2595720.9372427985\n",
            "Iteration 2952 - Batch 243/903 - Train loss: 2595723.450819672\n",
            "Iteration 2953 - Batch 244/903 - Train loss: 2595322.3346938775\n",
            "Iteration 2954 - Batch 245/903 - Train loss: 2595558.2317073173\n",
            "Iteration 2955 - Batch 246/903 - Train loss: 2595388.097165992\n",
            "Iteration 2956 - Batch 247/903 - Train loss: 2595406.904233871\n",
            "Iteration 2957 - Batch 248/903 - Train loss: 2596378.49497992\n",
            "Iteration 2958 - Batch 249/903 - Train loss: 2597380.719\n",
            "Iteration 2959 - Batch 250/903 - Train loss: 2598432.656374502\n",
            "Iteration 2960 - Batch 251/903 - Train loss: 2599667.29265873\n",
            "Iteration 2961 - Batch 252/903 - Train loss: 2600606.652173913\n",
            "Iteration 2962 - Batch 253/903 - Train loss: 2600542.777559055\n",
            "Iteration 2963 - Batch 254/903 - Train loss: 2601396.444117647\n",
            "Iteration 2964 - Batch 255/903 - Train loss: 2603368.447265625\n",
            "Iteration 2965 - Batch 256/903 - Train loss: 2604166.0204280154\n",
            "Iteration 2966 - Batch 257/903 - Train loss: 2604897.66375969\n",
            "Iteration 2967 - Batch 258/903 - Train loss: 2605834.999034749\n",
            "Iteration 2968 - Batch 259/903 - Train loss: 2607762.616346154\n",
            "Iteration 2969 - Batch 260/903 - Train loss: 2608752.252873563\n",
            "Iteration 2970 - Batch 261/903 - Train loss: 2608351.7986641224\n",
            "Iteration 2971 - Batch 262/903 - Train loss: 2608675.0655893534\n",
            "Iteration 2972 - Batch 263/903 - Train loss: 2609764.3920454546\n",
            "Iteration 2973 - Batch 264/903 - Train loss: 2609494.7849056604\n",
            "Iteration 2974 - Batch 265/903 - Train loss: 2608891.2180451127\n",
            "Iteration 2975 - Batch 266/903 - Train loss: 2609061.619850187\n",
            "Iteration 2976 - Batch 267/903 - Train loss: 2610604.593283582\n",
            "Iteration 2977 - Batch 268/903 - Train loss: 2610352.090148699\n",
            "Iteration 2978 - Batch 269/903 - Train loss: 2609663.928703704\n",
            "Iteration 2979 - Batch 270/903 - Train loss: 2610586.1005535056\n",
            "Iteration 2980 - Batch 271/903 - Train loss: 2611023.8161764704\n",
            "Iteration 2981 - Batch 272/903 - Train loss: 2611202.7335164836\n",
            "Iteration 2982 - Batch 273/903 - Train loss: 2612864.427919708\n",
            "Iteration 2983 - Batch 274/903 - Train loss: 2615284.94\n",
            "Iteration 2984 - Batch 275/903 - Train loss: 2614990.6929347827\n",
            "Iteration 2985 - Batch 276/903 - Train loss: 2614630.224729242\n",
            "Iteration 2986 - Batch 277/903 - Train loss: 2616661.6411870504\n",
            "Iteration 2987 - Batch 278/903 - Train loss: 2617255.914874552\n",
            "Iteration 2988 - Batch 279/903 - Train loss: 2616597.8035714286\n",
            "Iteration 2989 - Batch 280/903 - Train loss: 2617514.0444839858\n",
            "Iteration 2990 - Batch 281/903 - Train loss: 2617455.659574468\n",
            "Iteration 2991 - Batch 282/903 - Train loss: 2616487.6113074203\n",
            "Iteration 2992 - Batch 283/903 - Train loss: 2617495.1100352113\n",
            "Iteration 2993 - Batch 284/903 - Train loss: 2617932.5140350875\n",
            "Iteration 2994 - Batch 285/903 - Train loss: 2617942.3863636362\n",
            "Iteration 2995 - Batch 286/903 - Train loss: 2617778.8806620208\n",
            "Iteration 2996 - Batch 287/903 - Train loss: 2618171.181423611\n",
            "Iteration 2997 - Batch 288/903 - Train loss: 2617427.7525951555\n",
            "Iteration 2998 - Batch 289/903 - Train loss: 2616696.8672413793\n",
            "Iteration 2999 - Batch 290/903 - Train loss: 2616544.8462199313\n",
            "Iteration 3000 - Batch 291/903 - Train loss: 2617189.76369863\n",
            "Iteration 3001 - Batch 292/903 - Train loss: 2617238.1194539247\n",
            "Iteration 3002 - Batch 293/903 - Train loss: 2617760.9345238097\n",
            "Iteration 3003 - Batch 294/903 - Train loss: 2617445.456779661\n",
            "Iteration 3004 - Batch 295/903 - Train loss: 2616849.9712837837\n",
            "Iteration 3005 - Batch 296/903 - Train loss: 2616005.611952862\n",
            "Iteration 3006 - Batch 297/903 - Train loss: 2615132.0947986576\n",
            "Iteration 3007 - Batch 298/903 - Train loss: 2614642.260033445\n",
            "Iteration 3008 - Batch 299/903 - Train loss: 2614165.368333333\n",
            "Iteration 3009 - Batch 300/903 - Train loss: 2614617.9360465114\n",
            "Iteration 3010 - Batch 301/903 - Train loss: 2614168.286423841\n",
            "Iteration 3011 - Batch 302/903 - Train loss: 2613560.6163366335\n",
            "Iteration 3012 - Batch 303/903 - Train loss: 2613186.6595394737\n",
            "Iteration 3013 - Batch 304/903 - Train loss: 2612416.9672131147\n",
            "Iteration 3014 - Batch 305/903 - Train loss: 2612376.1789215687\n",
            "Iteration 3015 - Batch 306/903 - Train loss: 2612750.0513029317\n",
            "Iteration 3016 - Batch 307/903 - Train loss: 2612818.824675325\n",
            "Iteration 3017 - Batch 308/903 - Train loss: 2612405.4312297734\n",
            "Iteration 3018 - Batch 309/903 - Train loss: 2612350.7161290324\n",
            "Iteration 3019 - Batch 310/903 - Train loss: 2611846.9083601288\n",
            "Iteration 3020 - Batch 311/903 - Train loss: 2611391.8950320515\n",
            "Iteration 3021 - Batch 312/903 - Train loss: 2610774.57428115\n",
            "Iteration 3022 - Batch 313/903 - Train loss: 2609915.853503185\n",
            "Iteration 3023 - Batch 314/903 - Train loss: 2609709.3642857145\n",
            "Iteration 3024 - Batch 315/903 - Train loss: 2609040.0340189873\n",
            "Iteration 3025 - Batch 316/903 - Train loss: 2607693.711356467\n",
            "Iteration 3026 - Batch 317/903 - Train loss: 2606841.371855346\n",
            "Iteration 3027 - Batch 318/903 - Train loss: 2606996.0305642635\n",
            "Iteration 3028 - Batch 319/903 - Train loss: 2606310.35390625\n",
            "Iteration 3029 - Batch 320/903 - Train loss: 2606537.5755451713\n",
            "Iteration 3030 - Batch 321/903 - Train loss: 2606757.6436335403\n",
            "Iteration 3031 - Batch 322/903 - Train loss: 2606064.677244582\n",
            "Iteration 3032 - Batch 323/903 - Train loss: 2606866.5671296297\n",
            "Iteration 3033 - Batch 324/903 - Train loss: 2606693.914615385\n",
            "Iteration 3034 - Batch 325/903 - Train loss: 2606071.3044478525\n",
            "Iteration 3035 - Batch 326/903 - Train loss: 2605829.357033639\n",
            "Iteration 3036 - Batch 327/903 - Train loss: 2606913.5899390243\n",
            "Iteration 3037 - Batch 328/903 - Train loss: 2606123.358662614\n",
            "Iteration 3038 - Batch 329/903 - Train loss: 2605642.397727273\n",
            "Iteration 3039 - Batch 330/903 - Train loss: 2605761.450906344\n",
            "Iteration 3040 - Batch 331/903 - Train loss: 2605474.949548193\n",
            "Iteration 3041 - Batch 332/903 - Train loss: 2605325.575075075\n",
            "Iteration 3042 - Batch 333/903 - Train loss: 2605409.357784431\n",
            "Iteration 3043 - Batch 334/903 - Train loss: 2605189.108955224\n",
            "Iteration 3044 - Batch 335/903 - Train loss: 2604940.4188988097\n",
            "Iteration 3045 - Batch 336/903 - Train loss: 2604195.287833828\n",
            "Iteration 3046 - Batch 337/903 - Train loss: 2604198.408284024\n",
            "Iteration 3047 - Batch 338/903 - Train loss: 2603782.6733038346\n",
            "Iteration 3048 - Batch 339/903 - Train loss: 2603350.033088235\n",
            "Iteration 3049 - Batch 340/903 - Train loss: 2603168.9288856303\n",
            "Iteration 3050 - Batch 341/903 - Train loss: 2603016.499269006\n",
            "Iteration 3051 - Batch 342/903 - Train loss: 2602459.853498542\n",
            "Iteration 3052 - Batch 343/903 - Train loss: 2601721.6824127906\n",
            "Iteration 3053 - Batch 344/903 - Train loss: 2601863.783333333\n",
            "Iteration 3054 - Batch 345/903 - Train loss: 2602223.898121387\n",
            "Iteration 3055 - Batch 346/903 - Train loss: 2602010.8083573487\n",
            "Iteration 3056 - Batch 347/903 - Train loss: 2601739.438218391\n",
            "Iteration 3057 - Batch 348/903 - Train loss: 2601336.669054441\n",
            "Iteration 3058 - Batch 349/903 - Train loss: 2600942.935\n",
            "Iteration 3059 - Batch 350/903 - Train loss: 2600802.4964387463\n",
            "Iteration 3060 - Batch 351/903 - Train loss: 2601118.830965909\n",
            "Iteration 3061 - Batch 352/903 - Train loss: 2601666.507790368\n",
            "Iteration 3062 - Batch 353/903 - Train loss: 2600432.370762712\n",
            "Iteration 3063 - Batch 354/903 - Train loss: 2599798.511971831\n",
            "Iteration 3064 - Batch 355/903 - Train loss: 2599859.575140449\n",
            "Iteration 3065 - Batch 356/903 - Train loss: 2599870.2836134452\n",
            "Iteration 3066 - Batch 357/903 - Train loss: 2600362.688547486\n",
            "Iteration 3067 - Batch 358/903 - Train loss: 2600309.6142061283\n",
            "Iteration 3068 - Batch 359/903 - Train loss: 2600460.4618055555\n",
            "Iteration 3069 - Batch 360/903 - Train loss: 2599871.370498615\n",
            "Iteration 3070 - Batch 361/903 - Train loss: 2598983.0635359115\n",
            "Iteration 3071 - Batch 362/903 - Train loss: 2598904.0991735538\n",
            "Iteration 3072 - Batch 363/903 - Train loss: 2599603.500686813\n",
            "Iteration 3073 - Batch 364/903 - Train loss: 2599435.916438356\n",
            "Iteration 3074 - Batch 365/903 - Train loss: 2599062.2903005467\n",
            "Iteration 3075 - Batch 366/903 - Train loss: 2598264.8242506813\n",
            "Iteration 3076 - Batch 367/903 - Train loss: 2597213.898097826\n",
            "Iteration 3077 - Batch 368/903 - Train loss: 2597184.4410569104\n",
            "Iteration 3078 - Batch 369/903 - Train loss: 2596363.041216216\n",
            "Iteration 3079 - Batch 370/903 - Train loss: 2596264.1691374662\n",
            "Iteration 3080 - Batch 371/903 - Train loss: 2595802.231854839\n",
            "Iteration 3081 - Batch 372/903 - Train loss: 2595714.8351206435\n",
            "Iteration 3082 - Batch 373/903 - Train loss: 2595077.987299465\n",
            "Iteration 3083 - Batch 374/903 - Train loss: 2594777.016\n",
            "Iteration 3084 - Batch 375/903 - Train loss: 2594695.614361702\n",
            "Iteration 3085 - Batch 376/903 - Train loss: 2594604.0046419096\n",
            "Iteration 3086 - Batch 377/903 - Train loss: 2594983.236772487\n",
            "Iteration 3087 - Batch 378/903 - Train loss: 2594862.605540897\n",
            "Iteration 3088 - Batch 379/903 - Train loss: 2594741.2256578947\n",
            "Iteration 3089 - Batch 380/903 - Train loss: 2594461.6345144357\n",
            "Iteration 3090 - Batch 381/903 - Train loss: 2594497.470549738\n",
            "Iteration 3091 - Batch 382/903 - Train loss: 2595551.652741514\n",
            "Iteration 3092 - Batch 383/903 - Train loss: 2595667.455078125\n",
            "Iteration 3093 - Batch 384/903 - Train loss: 2595079.654545455\n",
            "Iteration 3094 - Batch 385/903 - Train loss: 2594196.601036269\n",
            "Iteration 3095 - Batch 386/903 - Train loss: 2593823.145348837\n",
            "Iteration 3096 - Batch 387/903 - Train loss: 2592996.119201031\n",
            "Iteration 3097 - Batch 388/903 - Train loss: 2592767.0128534706\n",
            "Iteration 3098 - Batch 389/903 - Train loss: 2592256.539102564\n",
            "Iteration 3099 - Batch 390/903 - Train loss: 2591416.9891304346\n",
            "Iteration 3100 - Batch 391/903 - Train loss: 2591140.5829081633\n",
            "Iteration 3101 - Batch 392/903 - Train loss: 2590376.8187022903\n",
            "Iteration 3102 - Batch 393/903 - Train loss: 2590252.2131979694\n",
            "Iteration 3103 - Batch 394/903 - Train loss: 2589694.2132911393\n",
            "Iteration 3104 - Batch 395/903 - Train loss: 2588938.211489899\n",
            "Iteration 3105 - Batch 396/903 - Train loss: 2588012.532745592\n",
            "Iteration 3106 - Batch 397/903 - Train loss: 2587741.665201005\n",
            "Iteration 3107 - Batch 398/903 - Train loss: 2587889.5231829574\n",
            "Iteration 3108 - Batch 399/903 - Train loss: 2586888.821875\n",
            "Iteration 3109 - Batch 400/903 - Train loss: 2585927.2082294263\n",
            "Iteration 3110 - Batch 401/903 - Train loss: 2585293.2456467664\n",
            "Iteration 3111 - Batch 402/903 - Train loss: 2584924.907568238\n",
            "Iteration 3112 - Batch 403/903 - Train loss: 2584091.292079208\n",
            "Iteration 3113 - Batch 404/903 - Train loss: 2583645.1666666665\n",
            "Iteration 3114 - Batch 405/903 - Train loss: 2583235.4371921183\n",
            "Iteration 3115 - Batch 406/903 - Train loss: 2583225.4637592137\n",
            "Iteration 3116 - Batch 407/903 - Train loss: 2582637.2610294116\n",
            "Iteration 3117 - Batch 408/903 - Train loss: 2582413.466992665\n",
            "Iteration 3118 - Batch 409/903 - Train loss: 2581964.762804878\n",
            "Iteration 3119 - Batch 410/903 - Train loss: 2581881.48540146\n",
            "Iteration 3120 - Batch 411/903 - Train loss: 2581205.2518203883\n",
            "Iteration 3121 - Batch 412/903 - Train loss: 2581131.354116223\n",
            "Iteration 3122 - Batch 413/903 - Train loss: 2580239.5446859905\n",
            "Iteration 3123 - Batch 414/903 - Train loss: 2579867.911445783\n",
            "Iteration 3124 - Batch 415/903 - Train loss: 2579788.178485577\n",
            "Iteration 3125 - Batch 416/903 - Train loss: 2579496.098920863\n",
            "Iteration 3126 - Batch 417/903 - Train loss: 2579135.8325358853\n",
            "Iteration 3127 - Batch 418/903 - Train loss: 2578389.020883055\n",
            "Iteration 3128 - Batch 419/903 - Train loss: 2578390.930357143\n",
            "Iteration 3129 - Batch 420/903 - Train loss: 2578262.8366983375\n",
            "Iteration 3130 - Batch 421/903 - Train loss: 2578477.086492891\n",
            "Iteration 3131 - Batch 422/903 - Train loss: 2579046.9078014186\n",
            "Iteration 3132 - Batch 423/903 - Train loss: 2579683.274764151\n",
            "Iteration 3133 - Batch 424/903 - Train loss: 2579850.9811764704\n",
            "Iteration 3134 - Batch 425/903 - Train loss: 2579743.8820422534\n",
            "Iteration 3135 - Batch 426/903 - Train loss: 2579616.052693208\n",
            "Iteration 3136 - Batch 427/903 - Train loss: 2579989.2476635515\n",
            "Iteration 3137 - Batch 428/903 - Train loss: 2579939.4434731933\n",
            "Iteration 3138 - Batch 429/903 - Train loss: 2579811.868604651\n",
            "Iteration 3139 - Batch 430/903 - Train loss: 2579985.865429234\n",
            "Iteration 3140 - Batch 431/903 - Train loss: 2579866.563657407\n",
            "Iteration 3141 - Batch 432/903 - Train loss: 2579078.575635104\n",
            "Iteration 3142 - Batch 433/903 - Train loss: 2578765.4539170507\n",
            "Iteration 3143 - Batch 434/903 - Train loss: 2578532.8206896554\n",
            "Iteration 3144 - Batch 435/903 - Train loss: 2578381.8836009176\n",
            "Iteration 3145 - Batch 436/903 - Train loss: 2578387.031464531\n",
            "Iteration 3146 - Batch 437/903 - Train loss: 2578425.2899543378\n",
            "Iteration 3147 - Batch 438/903 - Train loss: 2578475.3223234625\n",
            "Iteration 3148 - Batch 439/903 - Train loss: 2578293.4710227274\n",
            "Iteration 3149 - Batch 440/903 - Train loss: 2578062.4926303853\n",
            "Iteration 3150 - Batch 441/903 - Train loss: 2578125.2811085973\n",
            "Iteration 3151 - Batch 442/903 - Train loss: 2578110.8267494356\n",
            "Iteration 3152 - Batch 443/903 - Train loss: 2578100.267454955\n",
            "Iteration 3153 - Batch 444/903 - Train loss: 2578083.3898876403\n",
            "Iteration 3154 - Batch 445/903 - Train loss: 2577647.977017937\n",
            "Iteration 3155 - Batch 446/903 - Train loss: 2576965.5687919464\n",
            "Iteration 3156 - Batch 447/903 - Train loss: 2576641.5647321427\n",
            "Iteration 3157 - Batch 448/903 - Train loss: 2576319.438752784\n",
            "Iteration 3158 - Batch 449/903 - Train loss: 2575946.811111111\n",
            "Iteration 3159 - Batch 450/903 - Train loss: 2575519.3154101996\n",
            "Iteration 3160 - Batch 451/903 - Train loss: 2574864.1244469024\n",
            "Iteration 3161 - Batch 452/903 - Train loss: 2574488.0921633556\n",
            "Iteration 3162 - Batch 453/903 - Train loss: 2574624.4212555066\n",
            "Iteration 3163 - Batch 454/903 - Train loss: 2574684.7543956046\n",
            "Iteration 3164 - Batch 455/903 - Train loss: 2574484.111842105\n",
            "Iteration 3165 - Batch 456/903 - Train loss: 2574305.013129103\n",
            "Iteration 3166 - Batch 457/903 - Train loss: 2573953.7423580787\n",
            "Iteration 3167 - Batch 458/903 - Train loss: 2573875.1307189544\n",
            "Iteration 3168 - Batch 459/903 - Train loss: 2573991.8222826086\n",
            "Iteration 3169 - Batch 460/903 - Train loss: 2573361.7434924077\n",
            "Iteration 3170 - Batch 461/903 - Train loss: 2572880.483225108\n",
            "Iteration 3171 - Batch 462/903 - Train loss: 2572931.9179265657\n",
            "Iteration 3172 - Batch 463/903 - Train loss: 2573124.026939655\n",
            "Iteration 3173 - Batch 464/903 - Train loss: 2573090.5188172045\n",
            "Iteration 3174 - Batch 465/903 - Train loss: 2573181.692060086\n",
            "Iteration 3175 - Batch 466/903 - Train loss: 2572517.94379015\n",
            "Iteration 3176 - Batch 467/903 - Train loss: 2572433.3199786325\n",
            "Iteration 3177 - Batch 468/903 - Train loss: 2572219.1146055437\n",
            "Iteration 3178 - Batch 469/903 - Train loss: 2572102.440425532\n",
            "Iteration 3179 - Batch 470/903 - Train loss: 2572261.799363057\n",
            "Iteration 3180 - Batch 471/903 - Train loss: 2571943.3712923727\n",
            "Iteration 3181 - Batch 472/903 - Train loss: 2571907.5058139535\n",
            "Iteration 3182 - Batch 473/903 - Train loss: 2571205.197257384\n",
            "Iteration 3183 - Batch 474/903 - Train loss: 2571154.0757894735\n",
            "Iteration 3184 - Batch 475/903 - Train loss: 2571039.8324579834\n",
            "Iteration 3185 - Batch 476/903 - Train loss: 2570882.038784067\n",
            "Iteration 3186 - Batch 477/903 - Train loss: 2570836.2400627616\n",
            "Iteration 3187 - Batch 478/903 - Train loss: 2570519.0307933195\n",
            "Iteration 3188 - Batch 479/903 - Train loss: 2569996.978125\n",
            "Iteration 3189 - Batch 480/903 - Train loss: 2569265.106029106\n",
            "Iteration 3190 - Batch 481/903 - Train loss: 2569321.8034232366\n",
            "Iteration 3191 - Batch 482/903 - Train loss: 2568981.2686335403\n",
            "Iteration 3192 - Batch 483/903 - Train loss: 2568652.971590909\n",
            "Iteration 3193 - Batch 484/903 - Train loss: 2568254.3335051546\n",
            "Iteration 3194 - Batch 485/903 - Train loss: 2567363.7258230452\n",
            "Iteration 3195 - Batch 486/903 - Train loss: 2567026.7900410676\n",
            "Iteration 3196 - Batch 487/903 - Train loss: 2566233.5010245903\n",
            "Iteration 3197 - Batch 488/903 - Train loss: 2566413.2862985684\n",
            "Iteration 3198 - Batch 489/903 - Train loss: 2566755.0913265306\n",
            "Iteration 3199 - Batch 490/903 - Train loss: 2567913.5560081466\n",
            "Iteration 3200 - Batch 491/903 - Train loss: 2568382.12550813\n",
            "Iteration 3201 - Batch 492/903 - Train loss: 2568739.8311359026\n",
            "Iteration 3202 - Batch 493/903 - Train loss: 2568738.5941295545\n",
            "Iteration 3203 - Batch 494/903 - Train loss: 2569601.1636363636\n",
            "Iteration 3204 - Batch 495/903 - Train loss: 2571698.634072581\n",
            "Iteration 3205 - Batch 496/903 - Train loss: 2573470.2932595573\n",
            "Iteration 3206 - Batch 497/903 - Train loss: 2573905.3192771086\n",
            "Iteration 3207 - Batch 498/903 - Train loss: 2573983.1077154307\n",
            "Iteration 3208 - Batch 499/903 - Train loss: 2574406.934\n",
            "Iteration 3209 - Batch 500/903 - Train loss: 2576183.263972056\n",
            "Iteration 3210 - Batch 501/903 - Train loss: 2576168.734561753\n",
            "Iteration 3211 - Batch 502/903 - Train loss: 2576560.7972167\n",
            "Iteration 3212 - Batch 503/903 - Train loss: 2577742.4151785714\n",
            "Iteration 3213 - Batch 504/903 - Train loss: 2577489.3960396037\n",
            "Iteration 3214 - Batch 505/903 - Train loss: 2577641.1304347827\n",
            "Iteration 3215 - Batch 506/903 - Train loss: 2577987.8052268242\n",
            "Iteration 3216 - Batch 507/903 - Train loss: 2578763.2076771655\n",
            "Iteration 3217 - Batch 508/903 - Train loss: 2578687.0879174853\n",
            "Iteration 3218 - Batch 509/903 - Train loss: 2578558.53872549\n",
            "Iteration 3219 - Batch 510/903 - Train loss: 2578686.200587084\n",
            "Iteration 3220 - Batch 511/903 - Train loss: 2578911.853515625\n",
            "Iteration 3221 - Batch 512/903 - Train loss: 2579041.285087719\n",
            "Iteration 3222 - Batch 513/903 - Train loss: 2578673.07344358\n",
            "Iteration 3223 - Batch 514/903 - Train loss: 2578417.1019417476\n",
            "Iteration 3224 - Batch 515/903 - Train loss: 2578299.1865310078\n",
            "Iteration 3225 - Batch 516/903 - Train loss: 2578549.661992263\n",
            "Iteration 3226 - Batch 517/903 - Train loss: 2578342.298745174\n",
            "Iteration 3227 - Batch 518/903 - Train loss: 2578007.815510597\n",
            "Iteration 3228 - Batch 519/903 - Train loss: 2578076.420673077\n",
            "Iteration 3229 - Batch 520/903 - Train loss: 2578303.8114203457\n",
            "Iteration 3230 - Batch 521/903 - Train loss: 2577908.995689655\n",
            "Iteration 3231 - Batch 522/903 - Train loss: 2577490.1998087955\n",
            "Iteration 3232 - Batch 523/903 - Train loss: 2576862.9885496185\n",
            "Iteration 3233 - Batch 524/903 - Train loss: 2576673.5561904763\n",
            "Iteration 3234 - Batch 525/903 - Train loss: 2576653.576996198\n",
            "Iteration 3235 - Batch 526/903 - Train loss: 2576347.2476280835\n",
            "Iteration 3236 - Batch 527/903 - Train loss: 2576208.5525568184\n",
            "Iteration 3237 - Batch 528/903 - Train loss: 2576216.8913043477\n",
            "Iteration 3238 - Batch 529/903 - Train loss: 2576259.4103773586\n",
            "Iteration 3239 - Batch 530/903 - Train loss: 2576141.9759887005\n",
            "Iteration 3240 - Batch 531/903 - Train loss: 2575940.026785714\n",
            "Iteration 3241 - Batch 532/903 - Train loss: 2575515.0736397747\n",
            "Iteration 3242 - Batch 533/903 - Train loss: 2574766.91994382\n",
            "Iteration 3243 - Batch 534/903 - Train loss: 2574268.536448598\n",
            "Iteration 3244 - Batch 535/903 - Train loss: 2573833.561100746\n",
            "Iteration 3245 - Batch 536/903 - Train loss: 2573335.765828678\n",
            "Iteration 3246 - Batch 537/903 - Train loss: 2573007.810408922\n",
            "Iteration 3247 - Batch 538/903 - Train loss: 2572723.163729128\n",
            "Iteration 3248 - Batch 539/903 - Train loss: 2572357.249074074\n",
            "Iteration 3249 - Batch 540/903 - Train loss: 2572253.320702403\n",
            "Iteration 3250 - Batch 541/903 - Train loss: 2572205.322416974\n",
            "Iteration 3251 - Batch 542/903 - Train loss: 2572262.2633517496\n",
            "Iteration 3252 - Batch 543/903 - Train loss: 2571759.885110294\n",
            "Iteration 3253 - Batch 544/903 - Train loss: 2571532.109174312\n",
            "Iteration 3254 - Batch 545/903 - Train loss: 2571462.7825091574\n",
            "Iteration 3255 - Batch 546/903 - Train loss: 2571332.418190128\n",
            "Iteration 3256 - Batch 547/903 - Train loss: 2570855.644160584\n",
            "Iteration 3257 - Batch 548/903 - Train loss: 2570740.663479053\n",
            "Iteration 3258 - Batch 549/903 - Train loss: 2570596.6086363634\n",
            "Iteration 3259 - Batch 550/903 - Train loss: 2570485.703266788\n",
            "Iteration 3260 - Batch 551/903 - Train loss: 2570379.9981884058\n",
            "Iteration 3261 - Batch 552/903 - Train loss: 2570179.6216094033\n",
            "Iteration 3262 - Batch 553/903 - Train loss: 2570103.651173285\n",
            "Iteration 3263 - Batch 554/903 - Train loss: 2569546.8756756755\n",
            "Iteration 3264 - Batch 555/903 - Train loss: 2569571.6070143883\n",
            "Iteration 3265 - Batch 556/903 - Train loss: 2569526.232495512\n",
            "Iteration 3266 - Batch 557/903 - Train loss: 2569670.694892473\n",
            "Iteration 3267 - Batch 558/903 - Train loss: 2569224.693649374\n",
            "Iteration 3268 - Batch 559/903 - Train loss: 2569296.195982143\n",
            "Iteration 3269 - Batch 560/903 - Train loss: 2569297.2673796793\n",
            "Iteration 3270 - Batch 561/903 - Train loss: 2569100.231316726\n",
            "Iteration 3271 - Batch 562/903 - Train loss: 2569495.305506217\n",
            "Iteration 3272 - Batch 563/903 - Train loss: 2570107.073138298\n",
            "Iteration 3273 - Batch 564/903 - Train loss: 2569938.7088495577\n",
            "Iteration 3274 - Batch 565/903 - Train loss: 2569838.1660777386\n",
            "Iteration 3275 - Batch 566/903 - Train loss: 2569693.1243386245\n",
            "Iteration 3276 - Batch 567/903 - Train loss: 2569686.61971831\n",
            "Iteration 3277 - Batch 568/903 - Train loss: 2569328.6880492093\n",
            "Iteration 3278 - Batch 569/903 - Train loss: 2569194.8390350877\n",
            "Iteration 3279 - Batch 570/903 - Train loss: 2569271.6431698776\n",
            "Iteration 3280 - Batch 571/903 - Train loss: 2569267.8806818184\n",
            "Iteration 3281 - Batch 572/903 - Train loss: 2568963.445026178\n",
            "Iteration 3282 - Batch 573/903 - Train loss: 2568552.8044425086\n",
            "Iteration 3283 - Batch 574/903 - Train loss: 2568210.0091304346\n",
            "Iteration 3284 - Batch 575/903 - Train loss: 2568361.86328125\n",
            "Iteration 3285 - Batch 576/903 - Train loss: 2567843.8232235704\n",
            "Iteration 3286 - Batch 577/903 - Train loss: 2567853.832179931\n",
            "Iteration 3287 - Batch 578/903 - Train loss: 2568142.1800518134\n",
            "Iteration 3288 - Batch 579/903 - Train loss: 2567998.6370689655\n",
            "Iteration 3289 - Batch 580/903 - Train loss: 2567941.560240964\n",
            "Iteration 3290 - Batch 581/903 - Train loss: 2567558.0541237113\n",
            "Iteration 3291 - Batch 582/903 - Train loss: 2566908.062607204\n",
            "Iteration 3292 - Batch 583/903 - Train loss: 2566777.4434931506\n",
            "Iteration 3293 - Batch 584/903 - Train loss: 2566320.982905983\n",
            "Iteration 3294 - Batch 585/903 - Train loss: 2566432.565699659\n",
            "Iteration 3295 - Batch 586/903 - Train loss: 2566692.198040886\n",
            "Iteration 3296 - Batch 587/903 - Train loss: 2566458.475340136\n",
            "Iteration 3297 - Batch 588/903 - Train loss: 2566159.8531409167\n",
            "Iteration 3298 - Batch 589/903 - Train loss: 2565816.463135593\n",
            "Iteration 3299 - Batch 590/903 - Train loss: 2565236.969966159\n",
            "Iteration 3300 - Batch 591/903 - Train loss: 2564866.985641892\n",
            "Iteration 3301 - Batch 592/903 - Train loss: 2564757.1981450254\n",
            "Iteration 3302 - Batch 593/903 - Train loss: 2564108.6405723905\n",
            "Iteration 3303 - Batch 594/903 - Train loss: 2563482.1529411767\n",
            "Iteration 3304 - Batch 595/903 - Train loss: 2562781.5549496645\n",
            "Iteration 3305 - Batch 596/903 - Train loss: 2562800.637772194\n",
            "Iteration 3306 - Batch 597/903 - Train loss: 2562363.0058528427\n",
            "Iteration 3307 - Batch 598/903 - Train loss: 2562307.7971619368\n",
            "Iteration 3308 - Batch 599/903 - Train loss: 2562132.2158333333\n",
            "Iteration 3309 - Batch 600/903 - Train loss: 2562145.5099833612\n",
            "Iteration 3310 - Batch 601/903 - Train loss: 2561864.049003322\n",
            "Iteration 3311 - Batch 602/903 - Train loss: 2561621.288971808\n",
            "Iteration 3312 - Batch 603/903 - Train loss: 2561558.348509934\n",
            "Iteration 3313 - Batch 604/903 - Train loss: 2560989.8752066116\n",
            "Iteration 3314 - Batch 605/903 - Train loss: 2560883.714108911\n",
            "Iteration 3315 - Batch 606/903 - Train loss: 2560848.238467875\n",
            "Iteration 3316 - Batch 607/903 - Train loss: 2560770.7898848685\n",
            "Iteration 3317 - Batch 608/903 - Train loss: 2560366.4930213466\n",
            "Iteration 3318 - Batch 609/903 - Train loss: 2560095.490983607\n",
            "Iteration 3319 - Batch 610/903 - Train loss: 2560044.06710311\n",
            "Iteration 3320 - Batch 611/903 - Train loss: 2560513.3386437907\n",
            "Iteration 3321 - Batch 612/903 - Train loss: 2560189.1097063622\n",
            "Iteration 3322 - Batch 613/903 - Train loss: 2559735.1820032573\n",
            "Iteration 3323 - Batch 614/903 - Train loss: 2559326.4939024393\n",
            "Iteration 3324 - Batch 615/903 - Train loss: 2558849.829951299\n",
            "Iteration 3325 - Batch 616/903 - Train loss: 2558632.5729335495\n",
            "Iteration 3326 - Batch 617/903 - Train loss: 2558477.5149676376\n",
            "Iteration 3327 - Batch 618/903 - Train loss: 2558144.510500808\n",
            "Iteration 3328 - Batch 619/903 - Train loss: 2558024.585483871\n",
            "Iteration 3329 - Batch 620/903 - Train loss: 2557423.138888889\n",
            "Iteration 3330 - Batch 621/903 - Train loss: 2557385.7339228294\n",
            "Iteration 3331 - Batch 622/903 - Train loss: 2557083.032905297\n",
            "Iteration 3332 - Batch 623/903 - Train loss: 2556733.797676282\n",
            "Iteration 3333 - Batch 624/903 - Train loss: 2556267.8184\n",
            "Iteration 3334 - Batch 625/903 - Train loss: 2555840.001198083\n",
            "Iteration 3335 - Batch 626/903 - Train loss: 2555286.072169059\n",
            "Iteration 3336 - Batch 627/903 - Train loss: 2554902.962977707\n",
            "Iteration 3337 - Batch 628/903 - Train loss: 2554834.778616852\n",
            "Iteration 3338 - Batch 629/903 - Train loss: 2554366.532936508\n",
            "Iteration 3339 - Batch 630/903 - Train loss: 2554229.7016640254\n",
            "Iteration 3340 - Batch 631/903 - Train loss: 2553948.6483386075\n",
            "Iteration 3341 - Batch 632/903 - Train loss: 2553671.1441548183\n",
            "Iteration 3342 - Batch 633/903 - Train loss: 2553469.677444795\n",
            "Iteration 3343 - Batch 634/903 - Train loss: 2553012.6818897636\n",
            "Iteration 3344 - Batch 635/903 - Train loss: 2553063.735455975\n",
            "Iteration 3345 - Batch 636/903 - Train loss: 2553147.742543171\n",
            "Iteration 3346 - Batch 637/903 - Train loss: 2552728.34992163\n",
            "Iteration 3347 - Batch 638/903 - Train loss: 2552425.167057903\n",
            "Iteration 3348 - Batch 639/903 - Train loss: 2552131.846484375\n",
            "Iteration 3349 - Batch 640/903 - Train loss: 2551868.2971918876\n",
            "Iteration 3350 - Batch 641/903 - Train loss: 2551387.6374610593\n",
            "Iteration 3351 - Batch 642/903 - Train loss: 2550942.5276049767\n",
            "Iteration 3352 - Batch 643/903 - Train loss: 2550772.2686335403\n",
            "Iteration 3353 - Batch 644/903 - Train loss: 2550297.276356589\n",
            "Iteration 3354 - Batch 645/903 - Train loss: 2549948.257352941\n",
            "Iteration 3355 - Batch 646/903 - Train loss: 2549509.9091962907\n",
            "Iteration 3356 - Batch 647/903 - Train loss: 2549118.3877314813\n",
            "Iteration 3357 - Batch 648/903 - Train loss: 2549038.845916795\n",
            "Iteration 3358 - Batch 649/903 - Train loss: 2548780.543076923\n",
            "Iteration 3359 - Batch 650/903 - Train loss: 2549070.5483870967\n",
            "Iteration 3360 - Batch 651/903 - Train loss: 2548804.468174847\n",
            "Iteration 3361 - Batch 652/903 - Train loss: 2548488.9575038287\n",
            "Iteration 3362 - Batch 653/903 - Train loss: 2548093.115061162\n",
            "Iteration 3363 - Batch 654/903 - Train loss: 2548372.008778626\n",
            "Iteration 3364 - Batch 655/903 - Train loss: 2548926.7442835364\n",
            "Iteration 3365 - Batch 656/903 - Train loss: 2549254.918569254\n",
            "Iteration 3366 - Batch 657/903 - Train loss: 2550220.247720365\n",
            "Iteration 3367 - Batch 658/903 - Train loss: 2551470.6388467373\n",
            "Iteration 3368 - Batch 659/903 - Train loss: 2551834.8643939393\n",
            "Iteration 3369 - Batch 660/903 - Train loss: 2551723.895612708\n",
            "Iteration 3370 - Batch 661/903 - Train loss: 2551544.4157854985\n",
            "Iteration 3371 - Batch 662/903 - Train loss: 2551892.739064857\n",
            "Iteration 3372 - Batch 663/903 - Train loss: 2552281.745481928\n",
            "Iteration 3373 - Batch 664/903 - Train loss: 2552431.9327067668\n",
            "Iteration 3374 - Batch 665/903 - Train loss: 2551880.396021021\n",
            "Iteration 3375 - Batch 666/903 - Train loss: 2552070.1139430283\n",
            "Iteration 3376 - Batch 667/903 - Train loss: 2552161.84244012\n",
            "Iteration 3377 - Batch 668/903 - Train loss: 2551717.484304933\n",
            "Iteration 3378 - Batch 669/903 - Train loss: 2551413.4843283584\n",
            "Iteration 3379 - Batch 670/903 - Train loss: 2551122.12295082\n",
            "Iteration 3380 - Batch 671/903 - Train loss: 2550820.5729166665\n",
            "Iteration 3381 - Batch 672/903 - Train loss: 2550688.0832095095\n",
            "Iteration 3382 - Batch 673/903 - Train loss: 2550265.7577893175\n",
            "Iteration 3383 - Batch 674/903 - Train loss: 2549822.592222222\n",
            "Iteration 3384 - Batch 675/903 - Train loss: 2549602.8783284025\n",
            "Iteration 3385 - Batch 676/903 - Train loss: 2549284.748153619\n",
            "Iteration 3386 - Batch 677/903 - Train loss: 2548985.5088495575\n",
            "Iteration 3387 - Batch 678/903 - Train loss: 2548792.6568483063\n",
            "Iteration 3388 - Batch 679/903 - Train loss: 2548496.8889705883\n",
            "Iteration 3389 - Batch 680/903 - Train loss: 2547892.8961086636\n",
            "Iteration 3390 - Batch 681/903 - Train loss: 2547593.6436950145\n",
            "Iteration 3391 - Batch 682/903 - Train loss: 2547762.6002928256\n",
            "Iteration 3392 - Batch 683/903 - Train loss: 2547844.986842105\n",
            "Iteration 3393 - Batch 684/903 - Train loss: 2547417.2120437957\n",
            "Iteration 3394 - Batch 685/903 - Train loss: 2547172.4599125367\n",
            "Iteration 3395 - Batch 686/903 - Train loss: 2547106.2026928677\n",
            "Iteration 3396 - Batch 687/903 - Train loss: 2547159.6206395347\n",
            "Iteration 3397 - Batch 688/903 - Train loss: 2547143.2837445573\n",
            "Iteration 3398 - Batch 689/903 - Train loss: 2546766.4043478263\n",
            "Iteration 3399 - Batch 690/903 - Train loss: 2546280.77170767\n",
            "Iteration 3400 - Batch 691/903 - Train loss: 2545662.786849711\n",
            "Iteration 3401 - Batch 692/903 - Train loss: 2545335.347763348\n",
            "Iteration 3402 - Batch 693/903 - Train loss: 2544891.7319884724\n",
            "Iteration 3403 - Batch 694/903 - Train loss: 2545106.4179856116\n",
            "Iteration 3404 - Batch 695/903 - Train loss: 2545115.8577586208\n",
            "Iteration 3405 - Batch 696/903 - Train loss: 2544938.657101865\n",
            "Iteration 3406 - Batch 697/903 - Train loss: 2544504.6923352433\n",
            "Iteration 3407 - Batch 698/903 - Train loss: 2544168.058655222\n",
            "Iteration 3408 - Batch 699/903 - Train loss: 2544122.2946428573\n",
            "Iteration 3409 - Batch 700/903 - Train loss: 2544051.1408701856\n",
            "Iteration 3410 - Batch 701/903 - Train loss: 2543942.016737892\n",
            "Iteration 3411 - Batch 702/903 - Train loss: 2543632.988975818\n",
            "Iteration 3412 - Batch 703/903 - Train loss: 2543287.094460227\n",
            "Iteration 3413 - Batch 704/903 - Train loss: 2543215.70177305\n",
            "Iteration 3414 - Batch 705/903 - Train loss: 2542877.1186260623\n",
            "Iteration 3415 - Batch 706/903 - Train loss: 2542534.5986562944\n",
            "Iteration 3416 - Batch 707/903 - Train loss: 2542472.275423729\n",
            "Iteration 3417 - Batch 708/903 - Train loss: 2542083.446403385\n",
            "Iteration 3418 - Batch 709/903 - Train loss: 2542737.0549295773\n",
            "Iteration 3419 - Batch 710/903 - Train loss: 2543289.1744022504\n",
            "Iteration 3420 - Batch 711/903 - Train loss: 2543643.334269663\n",
            "Iteration 3421 - Batch 712/903 - Train loss: 2543954.73457223\n",
            "Iteration 3422 - Batch 713/903 - Train loss: 2544214.444327731\n",
            "Iteration 3423 - Batch 714/903 - Train loss: 2544916.9562937063\n",
            "Iteration 3424 - Batch 715/903 - Train loss: 2545432.870460894\n",
            "Iteration 3425 - Batch 716/903 - Train loss: 2545776.0142956763\n",
            "Iteration 3426 - Batch 717/903 - Train loss: 2545723.2712395545\n",
            "Iteration 3427 - Batch 718/903 - Train loss: 2545449.1314325454\n",
            "Iteration 3428 - Batch 719/903 - Train loss: 2545352.529513889\n",
            "Iteration 3429 - Batch 720/903 - Train loss: 2544905.0447295425\n",
            "Iteration 3430 - Batch 721/903 - Train loss: 2544708.16066482\n",
            "Iteration 3431 - Batch 722/903 - Train loss: 2544513.840248963\n",
            "Iteration 3432 - Batch 723/903 - Train loss: 2544460.5220994474\n",
            "Iteration 3433 - Batch 724/903 - Train loss: 2544335.491724138\n",
            "Iteration 3434 - Batch 725/903 - Train loss: 2544407.178030303\n",
            "Iteration 3435 - Batch 726/903 - Train loss: 2544505.0412654746\n",
            "Iteration 3436 - Batch 727/903 - Train loss: 2544523.259271978\n",
            "Iteration 3437 - Batch 728/903 - Train loss: 2544615.2969821673\n",
            "Iteration 3438 - Batch 729/903 - Train loss: 2544538.906849315\n",
            "Iteration 3439 - Batch 730/903 - Train loss: 2544487.9965800275\n",
            "Iteration 3440 - Batch 731/903 - Train loss: 2544311.256489071\n",
            "Iteration 3441 - Batch 732/903 - Train loss: 2544368.983628922\n",
            "Iteration 3442 - Batch 733/903 - Train loss: 2543990.4717302453\n",
            "Iteration 3443 - Batch 734/903 - Train loss: 2544284.201020408\n",
            "Iteration 3444 - Batch 735/903 - Train loss: 2544025.222486413\n",
            "Iteration 3445 - Batch 736/903 - Train loss: 2543828.1105834465\n",
            "Iteration 3446 - Batch 737/903 - Train loss: 2543860.977303523\n",
            "Iteration 3447 - Batch 738/903 - Train loss: 2544453.363667118\n",
            "Iteration 3448 - Batch 739/903 - Train loss: 2544557.8591216216\n",
            "Iteration 3449 - Batch 740/903 - Train loss: 2544621.3809041837\n",
            "Iteration 3450 - Batch 741/903 - Train loss: 2544771.5822102427\n",
            "Iteration 3451 - Batch 742/903 - Train loss: 2545008.6968371468\n",
            "Iteration 3452 - Batch 743/903 - Train loss: 2545600.0749327955\n",
            "Iteration 3453 - Batch 744/903 - Train loss: 2545688.8744966444\n",
            "Iteration 3454 - Batch 745/903 - Train loss: 2545692.7111260053\n",
            "Iteration 3455 - Batch 746/903 - Train loss: 2545882.3226238284\n",
            "Iteration 3456 - Batch 747/903 - Train loss: 2546044.17078877\n",
            "Iteration 3457 - Batch 748/903 - Train loss: 2545965.0176902534\n",
            "Iteration 3458 - Batch 749/903 - Train loss: 2546025.43\n",
            "Iteration 3459 - Batch 750/903 - Train loss: 2545999.350199734\n",
            "Iteration 3460 - Batch 751/903 - Train loss: 2545812.55418883\n",
            "Iteration 3461 - Batch 752/903 - Train loss: 2545555.136122178\n",
            "Iteration 3462 - Batch 753/903 - Train loss: 2545261.874668435\n",
            "Iteration 3463 - Batch 754/903 - Train loss: 2545209.366887417\n",
            "Iteration 3464 - Batch 755/903 - Train loss: 2545237.4930555555\n",
            "Iteration 3465 - Batch 756/903 - Train loss: 2545107.2288639364\n",
            "Iteration 3466 - Batch 757/903 - Train loss: 2545220.7048153034\n",
            "Iteration 3467 - Batch 758/903 - Train loss: 2545196.718379447\n",
            "Iteration 3468 - Batch 759/903 - Train loss: 2544936.0328947366\n",
            "Iteration 3469 - Batch 760/903 - Train loss: 2544607.2283180025\n",
            "Iteration 3470 - Batch 761/903 - Train loss: 2544100.87335958\n",
            "Iteration 3471 - Batch 762/903 - Train loss: 2543834.2837483617\n",
            "Iteration 3472 - Batch 763/903 - Train loss: 2543797.9087041887\n",
            "Iteration 3473 - Batch 764/903 - Train loss: 2543468.8369281045\n",
            "Iteration 3474 - Batch 765/903 - Train loss: 2543093.431462141\n",
            "Iteration 3475 - Batch 766/903 - Train loss: 2542998.046936115\n",
            "Iteration 3476 - Batch 767/903 - Train loss: 2542906.80078125\n",
            "Iteration 3477 - Batch 768/903 - Train loss: 2542729.0487646293\n",
            "Iteration 3478 - Batch 769/903 - Train loss: 2542386.595454545\n",
            "Iteration 3479 - Batch 770/903 - Train loss: 2542433.2418936444\n",
            "Iteration 3480 - Batch 771/903 - Train loss: 2542361.9889896372\n",
            "Iteration 3481 - Batch 772/903 - Train loss: 2541922.509379043\n",
            "Iteration 3482 - Batch 773/903 - Train loss: 2541650.1828165376\n",
            "Iteration 3483 - Batch 774/903 - Train loss: 2541305.0719354837\n",
            "Iteration 3484 - Batch 775/903 - Train loss: 2540911.7957474226\n",
            "Iteration 3485 - Batch 776/903 - Train loss: 2540497.4864864866\n",
            "Iteration 3486 - Batch 777/903 - Train loss: 2540369.1552056554\n",
            "Iteration 3487 - Batch 778/903 - Train loss: 2539899.2718228498\n",
            "Iteration 3488 - Batch 779/903 - Train loss: 2539730.633653846\n",
            "Iteration 3489 - Batch 780/903 - Train loss: 2539467.0829065302\n",
            "Iteration 3490 - Batch 781/903 - Train loss: 2539295.117647059\n",
            "Iteration 3491 - Batch 782/903 - Train loss: 2538955.104725415\n",
            "Iteration 3492 - Batch 783/903 - Train loss: 2538555.9467474488\n",
            "Iteration 3493 - Batch 784/903 - Train loss: 2538073.558598726\n",
            "Iteration 3494 - Batch 785/903 - Train loss: 2538053.099236641\n",
            "Iteration 3495 - Batch 786/903 - Train loss: 2537805.8186149937\n",
            "Iteration 3496 - Batch 787/903 - Train loss: 2537799.1729060914\n",
            "Iteration 3497 - Batch 788/903 - Train loss: 2537866.3852978456\n",
            "Iteration 3498 - Batch 789/903 - Train loss: 2537690.6436708863\n",
            "Iteration 3499 - Batch 790/903 - Train loss: 2537680.9348925413\n",
            "Iteration 3500 - Batch 791/903 - Train loss: 2537633.341540404\n",
            "Iteration 3501 - Batch 792/903 - Train loss: 2537570.7922446406\n",
            "Iteration 3502 - Batch 793/903 - Train loss: 2537258.355793451\n",
            "Iteration 3503 - Batch 794/903 - Train loss: 2536756.2553459117\n",
            "Iteration 3504 - Batch 795/903 - Train loss: 2536591.0712939696\n",
            "Iteration 3505 - Batch 796/903 - Train loss: 2536761.6882057716\n",
            "Iteration 3506 - Batch 797/903 - Train loss: 2536458.462406015\n",
            "Iteration 3507 - Batch 798/903 - Train loss: 2536272.1361076348\n",
            "Iteration 3508 - Batch 799/903 - Train loss: 2536188.0503125\n",
            "Iteration 3509 - Batch 800/903 - Train loss: 2536124.6860174784\n",
            "Iteration 3510 - Batch 801/903 - Train loss: 2536017.648690773\n",
            "Iteration 3511 - Batch 802/903 - Train loss: 2535922.75124533\n",
            "Iteration 3512 - Batch 803/903 - Train loss: 2535640.8050373136\n",
            "Iteration 3513 - Batch 804/903 - Train loss: 2535430.7779503106\n",
            "Iteration 3514 - Batch 805/903 - Train loss: 2535313.939516129\n",
            "Iteration 3515 - Batch 806/903 - Train loss: 2535187.5864312267\n",
            "Iteration 3516 - Batch 807/903 - Train loss: 2534840.567759901\n",
            "Iteration 3517 - Batch 808/903 - Train loss: 2534711.7645241036\n",
            "Iteration 3518 - Batch 809/903 - Train loss: 2534557.1182098766\n",
            "Iteration 3519 - Batch 810/903 - Train loss: 2534474.7123921085\n",
            "Iteration 3520 - Batch 811/903 - Train loss: 2534211.233682266\n",
            "Iteration 3521 - Batch 812/903 - Train loss: 2534001.8219557195\n",
            "Iteration 3522 - Batch 813/903 - Train loss: 2533790.894041769\n",
            "Iteration 3523 - Batch 814/903 - Train loss: 2533822.0509202452\n",
            "Iteration 3524 - Batch 815/903 - Train loss: 2533397.192401961\n",
            "Iteration 3525 - Batch 816/903 - Train loss: 2532978.6787025705\n",
            "Iteration 3526 - Batch 817/903 - Train loss: 2532727.7185207824\n",
            "Iteration 3527 - Batch 818/903 - Train loss: 2532635.75\n",
            "Iteration 3528 - Batch 819/903 - Train loss: 2532561.8396341465\n",
            "Iteration 3529 - Batch 820/903 - Train loss: 2532585.50274056\n",
            "Iteration 3530 - Batch 821/903 - Train loss: 2532217.0954987835\n",
            "Iteration 3531 - Batch 822/903 - Train loss: 2531990.92709599\n",
            "Iteration 3532 - Batch 823/903 - Train loss: 2531775.797633495\n",
            "Iteration 3533 - Batch 824/903 - Train loss: 2531568.4024242423\n",
            "Iteration 3534 - Batch 825/903 - Train loss: 2531170.5998789347\n",
            "Iteration 3535 - Batch 826/903 - Train loss: 2531004.736094317\n",
            "Iteration 3536 - Batch 827/903 - Train loss: 2530979.5329106282\n",
            "Iteration 3537 - Batch 828/903 - Train loss: 2530737.2509047044\n",
            "Iteration 3538 - Batch 829/903 - Train loss: 2530624.2855421687\n",
            "Iteration 3539 - Batch 830/903 - Train loss: 2530558.5737063778\n",
            "Iteration 3540 - Batch 831/903 - Train loss: 2530580.291466346\n",
            "Iteration 3541 - Batch 832/903 - Train loss: 2530470.720288115\n",
            "Iteration 3542 - Batch 833/903 - Train loss: 2529983.2290167864\n",
            "Iteration 3543 - Batch 834/903 - Train loss: 2529873.775449102\n",
            "Iteration 3544 - Batch 835/903 - Train loss: 2530025.389952153\n",
            "Iteration 3545 - Batch 836/903 - Train loss: 2529917.3864994026\n",
            "Iteration 3546 - Batch 837/903 - Train loss: 2529971.5399761335\n",
            "Iteration 3547 - Batch 838/903 - Train loss: 2529851.678486293\n",
            "Iteration 3548 - Batch 839/903 - Train loss: 2529356.097916667\n",
            "Iteration 3549 - Batch 840/903 - Train loss: 2529059.757431629\n",
            "Iteration 3550 - Batch 841/903 - Train loss: 2528663.7307007127\n",
            "Iteration 3551 - Batch 842/903 - Train loss: 2528309.069395018\n",
            "Iteration 3552 - Batch 843/903 - Train loss: 2528387.6335900472\n",
            "Iteration 3553 - Batch 844/903 - Train loss: 2527953.7683431953\n",
            "Iteration 3554 - Batch 845/903 - Train loss: 2527973.053782506\n",
            "Iteration 3555 - Batch 846/903 - Train loss: 2527871.0174144036\n",
            "Iteration 3556 - Batch 847/903 - Train loss: 2527757.051591981\n",
            "Iteration 3557 - Batch 848/903 - Train loss: 2527659.8465842167\n",
            "Iteration 3558 - Batch 849/903 - Train loss: 2527677.123235294\n",
            "Iteration 3559 - Batch 850/903 - Train loss: 2527500.5399529967\n",
            "Iteration 3560 - Batch 851/903 - Train loss: 2527418.2312206575\n",
            "Iteration 3561 - Batch 852/903 - Train loss: 2527256.888335287\n",
            "Iteration 3562 - Batch 853/903 - Train loss: 2526938.06323185\n",
            "Iteration 3563 - Batch 854/903 - Train loss: 2526822.5368421054\n",
            "Iteration 3564 - Batch 855/903 - Train loss: 2526627.5581191587\n",
            "Iteration 3565 - Batch 856/903 - Train loss: 2526447.2902567093\n",
            "Iteration 3566 - Batch 857/903 - Train loss: 2526093.560314685\n",
            "Iteration 3567 - Batch 858/903 - Train loss: 2525580.7676076833\n",
            "Iteration 3568 - Batch 859/903 - Train loss: 2525372.2556686047\n",
            "Iteration 3569 - Batch 860/903 - Train loss: 2525003.1583914054\n",
            "Iteration 3570 - Batch 861/903 - Train loss: 2524825.6370359627\n",
            "Iteration 3571 - Batch 862/903 - Train loss: 2524819.498696408\n",
            "Iteration 3572 - Batch 863/903 - Train loss: 2524578.520688657\n",
            "Iteration 3573 - Batch 864/903 - Train loss: 2524472.397254335\n",
            "Iteration 3574 - Batch 865/903 - Train loss: 2524546.204821016\n",
            "Iteration 3575 - Batch 866/903 - Train loss: 2524485.1126009226\n",
            "Iteration 3576 - Batch 867/903 - Train loss: 2524550.155673963\n",
            "Iteration 3577 - Batch 868/903 - Train loss: 2524267.0381185273\n",
            "Iteration 3578 - Batch 869/903 - Train loss: 2524208.1214080458\n",
            "Iteration 3579 - Batch 870/903 - Train loss: 2524164.0523823192\n",
            "Iteration 3580 - Batch 871/903 - Train loss: 2524249.7297878442\n",
            "Iteration 3581 - Batch 872/903 - Train loss: 2524517.1201317296\n",
            "Iteration 3582 - Batch 873/903 - Train loss: 2524532.1445938214\n",
            "Iteration 3583 - Batch 874/903 - Train loss: 2524571.206142857\n",
            "Iteration 3584 - Batch 875/903 - Train loss: 2524652.040667808\n",
            "Iteration 3585 - Batch 876/903 - Train loss: 2524788.6663340935\n",
            "Iteration 3586 - Batch 877/903 - Train loss: 2524889.787158314\n",
            "Iteration 3587 - Batch 878/903 - Train loss: 2524659.5649886234\n",
            "Iteration 3588 - Batch 879/903 - Train loss: 2524465.8365056817\n",
            "Iteration 3589 - Batch 880/903 - Train loss: 2524104.903660613\n",
            "Iteration 3590 - Batch 881/903 - Train loss: 2523911.4094387754\n",
            "Iteration 3591 - Batch 882/903 - Train loss: 2523697.1683182335\n",
            "Iteration 3592 - Batch 883/903 - Train loss: 2523397.9498020364\n",
            "Iteration 3593 - Batch 884/903 - Train loss: 2523277.949576271\n",
            "Iteration 3594 - Batch 885/903 - Train loss: 2523057.3477708804\n",
            "Iteration 3595 - Batch 886/903 - Train loss: 2522932.1351465615\n",
            "Iteration 3596 - Batch 887/903 - Train loss: 2523114.671875\n",
            "Iteration 3597 - Batch 888/903 - Train loss: 2523061.059758155\n",
            "Iteration 3598 - Batch 889/903 - Train loss: 2523014.2125\n",
            "Iteration 3599 - Batch 890/903 - Train loss: 2522719.509399551\n",
            "Iteration 3600 - Batch 891/903 - Train loss: 2522599.920263453\n",
            "Iteration 3601 - Batch 892/903 - Train loss: 2522566.99650056\n",
            "Iteration 3602 - Batch 893/903 - Train loss: 2522799.9646252794\n",
            "Iteration 3603 - Batch 894/903 - Train loss: 2523024.3213687153\n",
            "Iteration 3604 - Batch 895/903 - Train loss: 2522983.1683872766\n",
            "Iteration 3605 - Batch 896/903 - Train loss: 2522818.750696767\n",
            "Iteration 3606 - Batch 897/903 - Train loss: 2522744.6922327396\n",
            "Iteration 3607 - Batch 898/903 - Train loss: 2522585.022664071\n",
            "Iteration 3608 - Batch 899/903 - Train loss: 2522619.960138889\n",
            "Iteration 3609 - Batch 900/903 - Train loss: 2522466.6177857937\n",
            "Iteration 3610 - Batch 901/903 - Train loss: 2522335.99404102\n",
            "Iteration 3611 - Batch 902/903 - Train loss: 2520202.944213732\n",
            "Val loss: 1891311.6875\n",
            "Epoch 5/6\n",
            "Iteration 3613 - Batch 1/903 - Train loss: 2567552.125\n",
            "Iteration 3614 - Batch 2/903 - Train loss: 2522912.3333333335\n",
            "Iteration 3615 - Batch 3/903 - Train loss: 2493901.375\n",
            "Iteration 3616 - Batch 4/903 - Train loss: 2456723.95\n",
            "Iteration 3617 - Batch 5/903 - Train loss: 2485196.9166666665\n",
            "Iteration 3618 - Batch 6/903 - Train loss: 2491015.8571428573\n",
            "Iteration 3619 - Batch 7/903 - Train loss: 2520407.5625\n",
            "Iteration 3620 - Batch 8/903 - Train loss: 2542398.361111111\n",
            "Iteration 3621 - Batch 9/903 - Train loss: 2534086.875\n",
            "Iteration 3622 - Batch 10/903 - Train loss: 2524672.4545454546\n",
            "Iteration 3623 - Batch 11/903 - Train loss: 2523171.3541666665\n",
            "Iteration 3624 - Batch 12/903 - Train loss: 2528423.6923076925\n",
            "Iteration 3625 - Batch 13/903 - Train loss: 2526888.3035714286\n",
            "Iteration 3626 - Batch 14/903 - Train loss: 2522714.283333333\n",
            "Iteration 3627 - Batch 15/903 - Train loss: 2507760.5625\n",
            "Iteration 3628 - Batch 16/903 - Train loss: 2496402.705882353\n",
            "Iteration 3629 - Batch 17/903 - Train loss: 2484228.347222222\n",
            "Iteration 3630 - Batch 18/903 - Train loss: 2483555.947368421\n",
            "Iteration 3631 - Batch 19/903 - Train loss: 2478161.775\n",
            "Iteration 3632 - Batch 20/903 - Train loss: 2472295.761904762\n",
            "Iteration 3633 - Batch 21/903 - Train loss: 2465646.590909091\n",
            "Iteration 3634 - Batch 22/903 - Train loss: 2458528.597826087\n",
            "Iteration 3635 - Batch 23/903 - Train loss: 2463725.0625\n",
            "Iteration 3636 - Batch 24/903 - Train loss: 2460339.85\n",
            "Iteration 3637 - Batch 25/903 - Train loss: 2459605.769230769\n",
            "Iteration 3638 - Batch 26/903 - Train loss: 2456649.407407407\n",
            "Iteration 3639 - Batch 27/903 - Train loss: 2453080.098214286\n",
            "Iteration 3640 - Batch 28/903 - Train loss: 2452556.8448275863\n",
            "Iteration 3641 - Batch 29/903 - Train loss: 2442393.8\n",
            "Iteration 3642 - Batch 30/903 - Train loss: 2434839.879032258\n",
            "Iteration 3643 - Batch 31/903 - Train loss: 2429606.703125\n",
            "Iteration 3644 - Batch 32/903 - Train loss: 2433264.0833333335\n",
            "Iteration 3645 - Batch 33/903 - Train loss: 2424820.5147058824\n",
            "Iteration 3646 - Batch 34/903 - Train loss: 2426696.05\n",
            "Iteration 3647 - Batch 35/903 - Train loss: 2427147.798611111\n",
            "Iteration 3648 - Batch 36/903 - Train loss: 2431574.0945945946\n",
            "Iteration 3649 - Batch 37/903 - Train loss: 2438160.9671052634\n",
            "Iteration 3650 - Batch 38/903 - Train loss: 2447270.3653846155\n",
            "Iteration 3651 - Batch 39/903 - Train loss: 2450193.24375\n",
            "Iteration 3652 - Batch 40/903 - Train loss: 2457282.737804878\n",
            "Iteration 3653 - Batch 41/903 - Train loss: 2455619.0535714286\n",
            "Iteration 3654 - Batch 42/903 - Train loss: 2452829.13372093\n",
            "Iteration 3655 - Batch 43/903 - Train loss: 2457485.1136363638\n",
            "Iteration 3656 - Batch 44/903 - Train loss: 2462621.777777778\n",
            "Iteration 3657 - Batch 45/903 - Train loss: 2464691.7445652173\n",
            "Iteration 3658 - Batch 46/903 - Train loss: 2470094.420212766\n",
            "Iteration 3659 - Batch 47/903 - Train loss: 2473200.046875\n",
            "Iteration 3660 - Batch 48/903 - Train loss: 2474950.2397959186\n",
            "Iteration 3661 - Batch 49/903 - Train loss: 2475702.9\n",
            "Iteration 3662 - Batch 50/903 - Train loss: 2478893.661764706\n",
            "Iteration 3663 - Batch 51/903 - Train loss: 2479725.706730769\n",
            "Iteration 3664 - Batch 52/903 - Train loss: 2477942.933962264\n",
            "Iteration 3665 - Batch 53/903 - Train loss: 2476108.4583333335\n",
            "Iteration 3666 - Batch 54/903 - Train loss: 2477753.5090909093\n",
            "Iteration 3667 - Batch 55/903 - Train loss: 2480000.34375\n",
            "Iteration 3668 - Batch 56/903 - Train loss: 2480937.3596491227\n",
            "Iteration 3669 - Batch 57/903 - Train loss: 2480916.6551724137\n",
            "Iteration 3670 - Batch 58/903 - Train loss: 2482274.694915254\n",
            "Iteration 3671 - Batch 59/903 - Train loss: 2484508.033333333\n",
            "Iteration 3672 - Batch 60/903 - Train loss: 2484913.2131147543\n",
            "Iteration 3673 - Batch 61/903 - Train loss: 2483213.5483870967\n",
            "Iteration 3674 - Batch 62/903 - Train loss: 2481469.3333333335\n",
            "Iteration 3675 - Batch 63/903 - Train loss: 2480002.43359375\n",
            "Iteration 3676 - Batch 64/903 - Train loss: 2483397.8307692306\n",
            "Iteration 3677 - Batch 65/903 - Train loss: 2482406.0757575757\n",
            "Iteration 3678 - Batch 66/903 - Train loss: 2480149.9925373136\n",
            "Iteration 3679 - Batch 67/903 - Train loss: 2478563.8713235296\n",
            "Iteration 3680 - Batch 68/903 - Train loss: 2475485.445652174\n",
            "Iteration 3681 - Batch 69/903 - Train loss: 2474400.1428571427\n",
            "Iteration 3682 - Batch 70/903 - Train loss: 2473070.6161971833\n",
            "Iteration 3683 - Batch 71/903 - Train loss: 2473928.267361111\n",
            "Iteration 3684 - Batch 72/903 - Train loss: 2474207.5034246575\n",
            "Iteration 3685 - Batch 73/903 - Train loss: 2474145.010135135\n",
            "Iteration 3686 - Batch 74/903 - Train loss: 2470889.3633333333\n",
            "Iteration 3687 - Batch 75/903 - Train loss: 2472379.3486842103\n",
            "Iteration 3688 - Batch 76/903 - Train loss: 2470908.298701299\n",
            "Iteration 3689 - Batch 77/903 - Train loss: 2471655.4775641025\n",
            "Iteration 3690 - Batch 78/903 - Train loss: 2470456.2278481014\n",
            "Iteration 3691 - Batch 79/903 - Train loss: 2471161.259375\n",
            "Iteration 3692 - Batch 80/903 - Train loss: 2469676.888888889\n",
            "Iteration 3693 - Batch 81/903 - Train loss: 2467485.963414634\n",
            "Iteration 3694 - Batch 82/903 - Train loss: 2466790.746987952\n",
            "Iteration 3695 - Batch 83/903 - Train loss: 2464877.404761905\n",
            "Iteration 3696 - Batch 84/903 - Train loss: 2465110.0088235294\n",
            "Iteration 3697 - Batch 85/903 - Train loss: 2465617.1075581396\n",
            "Iteration 3698 - Batch 86/903 - Train loss: 2464255.8994252873\n",
            "Iteration 3699 - Batch 87/903 - Train loss: 2465150.585227273\n",
            "Iteration 3700 - Batch 88/903 - Train loss: 2464214.561797753\n",
            "Iteration 3701 - Batch 89/903 - Train loss: 2462167.652777778\n",
            "Iteration 3702 - Batch 90/903 - Train loss: 2460852.9945054944\n",
            "Iteration 3703 - Batch 91/903 - Train loss: 2459776.717391304\n",
            "Iteration 3704 - Batch 92/903 - Train loss: 2458618.134408602\n",
            "Iteration 3705 - Batch 93/903 - Train loss: 2459984.5531914895\n",
            "Iteration 3706 - Batch 94/903 - Train loss: 2459121.6447368423\n",
            "Iteration 3707 - Batch 95/903 - Train loss: 2460997.8515625\n",
            "Iteration 3708 - Batch 96/903 - Train loss: 2459377.1340206186\n",
            "Iteration 3709 - Batch 97/903 - Train loss: 2459296.2704081633\n",
            "Iteration 3710 - Batch 98/903 - Train loss: 2458700.5631313133\n",
            "Iteration 3711 - Batch 99/903 - Train loss: 2461221.4525\n",
            "Iteration 3712 - Batch 100/903 - Train loss: 2462977.4405940594\n",
            "Iteration 3713 - Batch 101/903 - Train loss: 2462589.06372549\n",
            "Iteration 3714 - Batch 102/903 - Train loss: 2459899.4878640776\n",
            "Iteration 3715 - Batch 103/903 - Train loss: 2457880.028846154\n",
            "Iteration 3716 - Batch 104/903 - Train loss: 2456554.95\n",
            "Iteration 3717 - Batch 105/903 - Train loss: 2455592.132075472\n",
            "Iteration 3718 - Batch 106/903 - Train loss: 2453687.4859813084\n",
            "Iteration 3719 - Batch 107/903 - Train loss: 2455084.032407407\n",
            "Iteration 3720 - Batch 108/903 - Train loss: 2453774.0481651374\n",
            "Iteration 3721 - Batch 109/903 - Train loss: 2452483.6795454547\n",
            "Iteration 3722 - Batch 110/903 - Train loss: 2451709.653153153\n",
            "Iteration 3723 - Batch 111/903 - Train loss: 2452313.1004464286\n",
            "Iteration 3724 - Batch 112/903 - Train loss: 2452106.440265487\n",
            "Iteration 3725 - Batch 113/903 - Train loss: 2449532.035087719\n",
            "Iteration 3726 - Batch 114/903 - Train loss: 2449665.263043478\n",
            "Iteration 3727 - Batch 115/903 - Train loss: 2450313.5172413792\n",
            "Iteration 3728 - Batch 116/903 - Train loss: 2450646.1303418805\n",
            "Iteration 3729 - Batch 117/903 - Train loss: 2452596.093220339\n",
            "Iteration 3730 - Batch 118/903 - Train loss: 2454907.9768907563\n",
            "Iteration 3731 - Batch 119/903 - Train loss: 2456647.0479166666\n",
            "Iteration 3732 - Batch 120/903 - Train loss: 2457061.958677686\n",
            "Iteration 3733 - Batch 121/903 - Train loss: 2458176.993852459\n",
            "Iteration 3734 - Batch 122/903 - Train loss: 2458166.2703252034\n",
            "Iteration 3735 - Batch 123/903 - Train loss: 2457791.3629032257\n",
            "Iteration 3736 - Batch 124/903 - Train loss: 2456960.112\n",
            "Iteration 3737 - Batch 125/903 - Train loss: 2456782.1845238097\n",
            "Iteration 3738 - Batch 126/903 - Train loss: 2456270.651574803\n",
            "Iteration 3739 - Batch 127/903 - Train loss: 2453800.095703125\n",
            "Iteration 3740 - Batch 128/903 - Train loss: 2451684.451550388\n",
            "Iteration 3741 - Batch 129/903 - Train loss: 2450921.2769230767\n",
            "Iteration 3742 - Batch 130/903 - Train loss: 2451574.5858778628\n",
            "Iteration 3743 - Batch 131/903 - Train loss: 2450666.378787879\n",
            "Iteration 3744 - Batch 132/903 - Train loss: 2449954.3139097746\n",
            "Iteration 3745 - Batch 133/903 - Train loss: 2449768.4981343285\n",
            "Iteration 3746 - Batch 134/903 - Train loss: 2448987.8203703705\n",
            "Iteration 3747 - Batch 135/903 - Train loss: 2449032.2022058824\n",
            "Iteration 3748 - Batch 136/903 - Train loss: 2447654.166058394\n",
            "Iteration 3749 - Batch 137/903 - Train loss: 2447099.512681159\n",
            "Iteration 3750 - Batch 138/903 - Train loss: 2445888.224820144\n",
            "Iteration 3751 - Batch 139/903 - Train loss: 2445543.4285714286\n",
            "Iteration 3752 - Batch 140/903 - Train loss: 2447106.070921986\n",
            "Iteration 3753 - Batch 141/903 - Train loss: 2445807.9102112674\n",
            "Iteration 3754 - Batch 142/903 - Train loss: 2443873.8916083914\n",
            "Iteration 3755 - Batch 143/903 - Train loss: 2442936.921875\n",
            "Iteration 3756 - Batch 144/903 - Train loss: 2439648.604310345\n",
            "Iteration 3757 - Batch 145/903 - Train loss: 2439078.6635273974\n",
            "Iteration 3758 - Batch 146/903 - Train loss: 2438361.522959184\n",
            "Iteration 3759 - Batch 147/903 - Train loss: 2438052.5886824327\n",
            "Iteration 3760 - Batch 148/903 - Train loss: 2438844.9354026844\n",
            "Iteration 3761 - Batch 149/903 - Train loss: 2438057.4225\n",
            "Iteration 3762 - Batch 150/903 - Train loss: 2437558.492549669\n",
            "Iteration 3763 - Batch 151/903 - Train loss: 2436358.5452302634\n",
            "Iteration 3764 - Batch 152/903 - Train loss: 2435346.473039216\n",
            "Iteration 3765 - Batch 153/903 - Train loss: 2434056.8693181816\n",
            "Iteration 3766 - Batch 154/903 - Train loss: 2432665.3991935486\n",
            "Iteration 3767 - Batch 155/903 - Train loss: 2432629.095352564\n",
            "Iteration 3768 - Batch 156/903 - Train loss: 2432434.562898089\n",
            "Iteration 3769 - Batch 157/903 - Train loss: 2432588.9359177216\n",
            "Iteration 3770 - Batch 158/903 - Train loss: 2432619.9504716983\n",
            "Iteration 3771 - Batch 159/903 - Train loss: 2433792.33828125\n",
            "Iteration 3772 - Batch 160/903 - Train loss: 2434594.1653726706\n",
            "Iteration 3773 - Batch 161/903 - Train loss: 2434433.93441358\n",
            "Iteration 3774 - Batch 162/903 - Train loss: 2434332.402607362\n",
            "Iteration 3775 - Batch 163/903 - Train loss: 2433594.5861280486\n",
            "Iteration 3776 - Batch 164/903 - Train loss: 2434062.2234848486\n",
            "Iteration 3777 - Batch 165/903 - Train loss: 2434216.452560241\n",
            "Iteration 3778 - Batch 166/903 - Train loss: 2433267.9603293412\n",
            "Iteration 3779 - Batch 167/903 - Train loss: 2432362.802827381\n",
            "Iteration 3780 - Batch 168/903 - Train loss: 2433744.302514793\n",
            "Iteration 3781 - Batch 169/903 - Train loss: 2434828.050735294\n",
            "Iteration 3782 - Batch 170/903 - Train loss: 2437711.1703216373\n",
            "Iteration 3783 - Batch 171/903 - Train loss: 2439164.3103197673\n",
            "Iteration 3784 - Batch 172/903 - Train loss: 2438691.610549133\n",
            "Iteration 3785 - Batch 173/903 - Train loss: 2439419.5783045976\n",
            "Iteration 3786 - Batch 174/903 - Train loss: 2441624.3764285715\n",
            "Iteration 3787 - Batch 175/903 - Train loss: 2443834.406960227\n",
            "Iteration 3788 - Batch 176/903 - Train loss: 2444591.390536723\n",
            "Iteration 3789 - Batch 177/903 - Train loss: 2443989.7731741574\n",
            "Iteration 3790 - Batch 178/903 - Train loss: 2442925.537011173\n",
            "Iteration 3791 - Batch 179/903 - Train loss: 2443037.68125\n",
            "Iteration 3792 - Batch 180/903 - Train loss: 2443307.5448895027\n",
            "Iteration 3793 - Batch 181/903 - Train loss: 2442302.9842032967\n",
            "Iteration 3794 - Batch 182/903 - Train loss: 2442627.413251366\n",
            "Iteration 3795 - Batch 183/903 - Train loss: 2442684.231657609\n",
            "Iteration 3796 - Batch 184/903 - Train loss: 2442298.8074324327\n",
            "Iteration 3797 - Batch 185/903 - Train loss: 2441826.2815860217\n",
            "Iteration 3798 - Batch 186/903 - Train loss: 2441966.3402406415\n",
            "Iteration 3799 - Batch 187/903 - Train loss: 2441407.088430851\n",
            "Iteration 3800 - Batch 188/903 - Train loss: 2440927.8366402118\n",
            "Iteration 3801 - Batch 189/903 - Train loss: 2441463.5559210526\n",
            "Iteration 3802 - Batch 190/903 - Train loss: 2441406.597513089\n",
            "Iteration 3803 - Batch 191/903 - Train loss: 2441054.224609375\n",
            "Iteration 3804 - Batch 192/903 - Train loss: 2439857.5433937823\n",
            "Iteration 3805 - Batch 193/903 - Train loss: 2440071.400128866\n",
            "Iteration 3806 - Batch 194/903 - Train loss: 2439446.251923077\n",
            "Iteration 3807 - Batch 195/903 - Train loss: 2437716.2901785714\n",
            "Iteration 3808 - Batch 196/903 - Train loss: 2437425.5666243653\n",
            "Iteration 3809 - Batch 197/903 - Train loss: 2438848.5309343436\n",
            "Iteration 3810 - Batch 198/903 - Train loss: 2439009.593592965\n",
            "Iteration 3811 - Batch 199/903 - Train loss: 2438399.876875\n",
            "Iteration 3812 - Batch 200/903 - Train loss: 2439597.1747512436\n",
            "Iteration 3813 - Batch 201/903 - Train loss: 2439517.5018564356\n",
            "Iteration 3814 - Batch 202/903 - Train loss: 2439155.136083744\n",
            "Iteration 3815 - Batch 203/903 - Train loss: 2439645.9172794116\n",
            "Iteration 3816 - Batch 204/903 - Train loss: 2440152.8786585364\n",
            "Iteration 3817 - Batch 205/903 - Train loss: 2440117.964199029\n",
            "Iteration 3818 - Batch 206/903 - Train loss: 2439835.381038647\n",
            "Iteration 3819 - Batch 207/903 - Train loss: 2439544.521033654\n",
            "Iteration 3820 - Batch 208/903 - Train loss: 2440139.4168660287\n",
            "Iteration 3821 - Batch 209/903 - Train loss: 2440296.2779761907\n",
            "Iteration 3822 - Batch 210/903 - Train loss: 2439909.6392180095\n",
            "Iteration 3823 - Batch 211/903 - Train loss: 2439588.203419811\n",
            "Iteration 3824 - Batch 212/903 - Train loss: 2438533.828051643\n",
            "Iteration 3825 - Batch 213/903 - Train loss: 2438189.4234813084\n",
            "Iteration 3826 - Batch 214/903 - Train loss: 2437420.218023256\n",
            "Iteration 3827 - Batch 215/903 - Train loss: 2436733.958912037\n",
            "Iteration 3828 - Batch 216/903 - Train loss: 2435963.2344470047\n",
            "Iteration 3829 - Batch 217/903 - Train loss: 2434584.811353211\n",
            "Iteration 3830 - Batch 218/903 - Train loss: 2434464.110159817\n",
            "Iteration 3831 - Batch 219/903 - Train loss: 2433674.1676136362\n",
            "Iteration 3832 - Batch 220/903 - Train loss: 2432734.3150452487\n",
            "Iteration 3833 - Batch 221/903 - Train loss: 2432240.2550675673\n",
            "Iteration 3834 - Batch 222/903 - Train loss: 2430501.279147982\n",
            "Iteration 3835 - Batch 223/903 - Train loss: 2429871.081473214\n",
            "Iteration 3836 - Batch 224/903 - Train loss: 2428699.26\n",
            "Iteration 3837 - Batch 225/903 - Train loss: 2428768.829646018\n",
            "Iteration 3838 - Batch 226/903 - Train loss: 2428867.148678414\n",
            "Iteration 3839 - Batch 227/903 - Train loss: 2427869.7653508773\n",
            "Iteration 3840 - Batch 228/903 - Train loss: 2428271.551310044\n",
            "Iteration 3841 - Batch 229/903 - Train loss: 2428188.872826087\n",
            "Iteration 3842 - Batch 230/903 - Train loss: 2428618.7813852816\n",
            "Iteration 3843 - Batch 231/903 - Train loss: 2428357.504310345\n",
            "Iteration 3844 - Batch 232/903 - Train loss: 2427591.4645922747\n",
            "Iteration 3845 - Batch 233/903 - Train loss: 2427456.7094017095\n",
            "Iteration 3846 - Batch 234/903 - Train loss: 2427545.8819148936\n",
            "Iteration 3847 - Batch 235/903 - Train loss: 2428043.1016949154\n",
            "Iteration 3848 - Batch 236/903 - Train loss: 2427756.030590717\n",
            "Iteration 3849 - Batch 237/903 - Train loss: 2426761.987394958\n",
            "Iteration 3850 - Batch 238/903 - Train loss: 2426389.599372385\n",
            "Iteration 3851 - Batch 239/903 - Train loss: 2426851.154166667\n",
            "Iteration 3852 - Batch 240/903 - Train loss: 2426833.3817427387\n",
            "Iteration 3853 - Batch 241/903 - Train loss: 2426537.0733471075\n",
            "Iteration 3854 - Batch 242/903 - Train loss: 2426059.930041152\n",
            "Iteration 3855 - Batch 243/903 - Train loss: 2425672.418032787\n",
            "Iteration 3856 - Batch 244/903 - Train loss: 2427216.8857142855\n",
            "Iteration 3857 - Batch 245/903 - Train loss: 2428625.8790650405\n",
            "Iteration 3858 - Batch 246/903 - Train loss: 2429083.2155870446\n",
            "Iteration 3859 - Batch 247/903 - Train loss: 2429704.759072581\n",
            "Iteration 3860 - Batch 248/903 - Train loss: 2429110.2921686745\n",
            "Iteration 3861 - Batch 249/903 - Train loss: 2429391.579\n",
            "Iteration 3862 - Batch 250/903 - Train loss: 2430426.610557769\n",
            "Iteration 3863 - Batch 251/903 - Train loss: 2430293.011904762\n",
            "Iteration 3864 - Batch 252/903 - Train loss: 2429814.07312253\n",
            "Iteration 3865 - Batch 253/903 - Train loss: 2430299.5098425196\n",
            "Iteration 3866 - Batch 254/903 - Train loss: 2431465.9725490194\n",
            "Iteration 3867 - Batch 255/903 - Train loss: 2433980.82421875\n",
            "Iteration 3868 - Batch 256/903 - Train loss: 2435404.1225680932\n",
            "Iteration 3869 - Batch 257/903 - Train loss: 2435124.6308139535\n",
            "Iteration 3870 - Batch 258/903 - Train loss: 2436016.5048262547\n",
            "Iteration 3871 - Batch 259/903 - Train loss: 2436767.9423076925\n",
            "Iteration 3872 - Batch 260/903 - Train loss: 2437462.2279693484\n",
            "Iteration 3873 - Batch 261/903 - Train loss: 2438439.4828244275\n",
            "Iteration 3874 - Batch 262/903 - Train loss: 2438957.164448669\n",
            "Iteration 3875 - Batch 263/903 - Train loss: 2440916.9668560605\n",
            "Iteration 3876 - Batch 264/903 - Train loss: 2441386.0386792454\n",
            "Iteration 3877 - Batch 265/903 - Train loss: 2442728.703007519\n",
            "Iteration 3878 - Batch 266/903 - Train loss: 2442209.216292135\n",
            "Iteration 3879 - Batch 267/903 - Train loss: 2442124.6222014925\n",
            "Iteration 3880 - Batch 268/903 - Train loss: 2442692.692379182\n",
            "Iteration 3881 - Batch 269/903 - Train loss: 2442286.123148148\n",
            "Iteration 3882 - Batch 270/903 - Train loss: 2442356.9861623617\n",
            "Iteration 3883 - Batch 271/903 - Train loss: 2442901.9696691176\n",
            "Iteration 3884 - Batch 272/903 - Train loss: 2443208.7875457876\n",
            "Iteration 3885 - Batch 273/903 - Train loss: 2443615.9826642335\n",
            "Iteration 3886 - Batch 274/903 - Train loss: 2443302.0518181818\n",
            "Iteration 3887 - Batch 275/903 - Train loss: 2442939.845108696\n",
            "Iteration 3888 - Batch 276/903 - Train loss: 2443025.602888087\n",
            "Iteration 3889 - Batch 277/903 - Train loss: 2442066.529676259\n",
            "Iteration 3890 - Batch 278/903 - Train loss: 2442337.0295698927\n",
            "Iteration 3891 - Batch 279/903 - Train loss: 2442049.0026785713\n",
            "Iteration 3892 - Batch 280/903 - Train loss: 2441563.043594306\n",
            "Iteration 3893 - Batch 281/903 - Train loss: 2441633.094858156\n",
            "Iteration 3894 - Batch 282/903 - Train loss: 2441458.117491166\n",
            "Iteration 3895 - Batch 283/903 - Train loss: 2441804.8873239434\n",
            "Iteration 3896 - Batch 284/903 - Train loss: 2442725.6956140352\n",
            "Iteration 3897 - Batch 285/903 - Train loss: 2443945.6512237764\n",
            "Iteration 3898 - Batch 286/903 - Train loss: 2445053.3780487804\n",
            "Iteration 3899 - Batch 287/903 - Train loss: 2445220.9696180555\n",
            "Iteration 3900 - Batch 288/903 - Train loss: 2445157.3711072663\n",
            "Iteration 3901 - Batch 289/903 - Train loss: 2445287.8025862067\n",
            "Iteration 3902 - Batch 290/903 - Train loss: 2445110.8058419246\n",
            "Iteration 3903 - Batch 291/903 - Train loss: 2444931.9203767125\n",
            "Iteration 3904 - Batch 292/903 - Train loss: 2444598.850682594\n",
            "Iteration 3905 - Batch 293/903 - Train loss: 2445492.5646258504\n",
            "Iteration 3906 - Batch 294/903 - Train loss: 2445789.1991525423\n",
            "Iteration 3907 - Batch 295/903 - Train loss: 2445344.174831081\n",
            "Iteration 3908 - Batch 296/903 - Train loss: 2444304.9335016836\n",
            "Iteration 3909 - Batch 297/903 - Train loss: 2444321.8095637583\n",
            "Iteration 3910 - Batch 298/903 - Train loss: 2443985.0025083614\n",
            "Iteration 3911 - Batch 299/903 - Train loss: 2443825.444166667\n",
            "Iteration 3912 - Batch 300/903 - Train loss: 2443031.3754152823\n",
            "Iteration 3913 - Batch 301/903 - Train loss: 2441794.1283112583\n",
            "Iteration 3914 - Batch 302/903 - Train loss: 2441582.8935643565\n",
            "Iteration 3915 - Batch 303/903 - Train loss: 2441888.888157895\n",
            "Iteration 3916 - Batch 304/903 - Train loss: 2441368.8598360657\n",
            "Iteration 3917 - Batch 305/903 - Train loss: 2440270.4517973857\n",
            "Iteration 3918 - Batch 306/903 - Train loss: 2439696.539087948\n",
            "Iteration 3919 - Batch 307/903 - Train loss: 2438821.4301948054\n",
            "Iteration 3920 - Batch 308/903 - Train loss: 2438277.80987055\n",
            "Iteration 3921 - Batch 309/903 - Train loss: 2438295.066935484\n",
            "Iteration 3922 - Batch 310/903 - Train loss: 2438638.979903537\n",
            "Iteration 3923 - Batch 311/903 - Train loss: 2438685.012019231\n",
            "Iteration 3924 - Batch 312/903 - Train loss: 2438141.2587859426\n",
            "Iteration 3925 - Batch 313/903 - Train loss: 2437831.1265923567\n",
            "Iteration 3926 - Batch 314/903 - Train loss: 2437448.1015873016\n",
            "Iteration 3927 - Batch 315/903 - Train loss: 2436728.2681962023\n",
            "Iteration 3928 - Batch 316/903 - Train loss: 2436504.52681388\n",
            "Iteration 3929 - Batch 317/903 - Train loss: 2435824.8034591195\n",
            "Iteration 3930 - Batch 318/903 - Train loss: 2435287.0313479626\n",
            "Iteration 3931 - Batch 319/903 - Train loss: 2434696.8328125\n",
            "Iteration 3932 - Batch 320/903 - Train loss: 2433502.389797508\n",
            "Iteration 3933 - Batch 321/903 - Train loss: 2432871.178959627\n",
            "Iteration 3934 - Batch 322/903 - Train loss: 2432047.553018576\n",
            "Iteration 3935 - Batch 323/903 - Train loss: 2431420.7002314813\n",
            "Iteration 3936 - Batch 324/903 - Train loss: 2431106.435\n",
            "Iteration 3937 - Batch 325/903 - Train loss: 2430681.942868098\n",
            "Iteration 3938 - Batch 326/903 - Train loss: 2429962.997324159\n",
            "Iteration 3939 - Batch 327/903 - Train loss: 2429898.5636432925\n",
            "Iteration 3940 - Batch 328/903 - Train loss: 2429224.719224924\n",
            "Iteration 3941 - Batch 329/903 - Train loss: 2429165.7859848486\n",
            "Iteration 3942 - Batch 330/903 - Train loss: 2428527.208081571\n",
            "Iteration 3943 - Batch 331/903 - Train loss: 2428361.2518825303\n",
            "Iteration 3944 - Batch 332/903 - Train loss: 2427300.232357357\n",
            "Iteration 3945 - Batch 333/903 - Train loss: 2427091.1066616764\n",
            "Iteration 3946 - Batch 334/903 - Train loss: 2426525.3899253733\n",
            "Iteration 3947 - Batch 335/903 - Train loss: 2425882.519717262\n",
            "Iteration 3948 - Batch 336/903 - Train loss: 2425609.2889465876\n",
            "Iteration 3949 - Batch 337/903 - Train loss: 2425020.0521449703\n",
            "Iteration 3950 - Batch 338/903 - Train loss: 2424817.513643068\n",
            "Iteration 3951 - Batch 339/903 - Train loss: 2424383.881985294\n",
            "Iteration 3952 - Batch 340/903 - Train loss: 2423906.5113636362\n",
            "Iteration 3953 - Batch 341/903 - Train loss: 2423656.447002924\n",
            "Iteration 3954 - Batch 342/903 - Train loss: 2423538.926020408\n",
            "Iteration 3955 - Batch 343/903 - Train loss: 2422926.265625\n",
            "Iteration 3956 - Batch 344/903 - Train loss: 2421955.456884058\n",
            "Iteration 3957 - Batch 345/903 - Train loss: 2421144.986632948\n",
            "Iteration 3958 - Batch 346/903 - Train loss: 2420656.1797550432\n",
            "Iteration 3959 - Batch 347/903 - Train loss: 2420170.309985632\n",
            "Iteration 3960 - Batch 348/903 - Train loss: 2420152.3062320915\n",
            "Iteration 3961 - Batch 349/903 - Train loss: 2420130.5032142857\n",
            "Iteration 3962 - Batch 350/903 - Train loss: 2419562.392094017\n",
            "Iteration 3963 - Batch 351/903 - Train loss: 2419454.2830255684\n",
            "Iteration 3964 - Batch 352/903 - Train loss: 2418631.4656515582\n",
            "Iteration 3965 - Batch 353/903 - Train loss: 2418340.974223164\n",
            "Iteration 3966 - Batch 354/903 - Train loss: 2417599.845422535\n",
            "Iteration 3967 - Batch 355/903 - Train loss: 2416642.043188202\n",
            "Iteration 3968 - Batch 356/903 - Train loss: 2416424.81127451\n",
            "Iteration 3969 - Batch 357/903 - Train loss: 2416396.2978351954\n",
            "Iteration 3970 - Batch 358/903 - Train loss: 2416937.831128134\n",
            "Iteration 3971 - Batch 359/903 - Train loss: 2417484.617013889\n",
            "Iteration 3972 - Batch 360/903 - Train loss: 2417798.344529086\n",
            "Iteration 3973 - Batch 361/903 - Train loss: 2417448.7199585633\n",
            "Iteration 3974 - Batch 362/903 - Train loss: 2418264.607093664\n",
            "Iteration 3975 - Batch 363/903 - Train loss: 2419142.586881868\n",
            "Iteration 3976 - Batch 364/903 - Train loss: 2419442.145547945\n",
            "Iteration 3977 - Batch 365/903 - Train loss: 2419083.6219262294\n",
            "Iteration 3978 - Batch 366/903 - Train loss: 2419288.7482970026\n",
            "Iteration 3979 - Batch 367/903 - Train loss: 2419696.598845109\n",
            "Iteration 3980 - Batch 368/903 - Train loss: 2421785.6331300815\n",
            "Iteration 3981 - Batch 369/903 - Train loss: 2423816.191554054\n",
            "Iteration 3982 - Batch 370/903 - Train loss: 2425565.720013477\n",
            "Iteration 3983 - Batch 371/903 - Train loss: 2426056.5668682796\n",
            "Iteration 3984 - Batch 372/903 - Train loss: 2425606.017761394\n",
            "Iteration 3985 - Batch 373/903 - Train loss: 2425730.7008689838\n",
            "Iteration 3986 - Batch 374/903 - Train loss: 2425862.3096666667\n",
            "Iteration 3987 - Batch 375/903 - Train loss: 2425848.134640957\n",
            "Iteration 3988 - Batch 376/903 - Train loss: 2425369.441976127\n",
            "Iteration 3989 - Batch 377/903 - Train loss: 2426275.1233465606\n",
            "Iteration 3990 - Batch 378/903 - Train loss: 2425802.338060686\n",
            "Iteration 3991 - Batch 379/903 - Train loss: 2425668.070723684\n",
            "Iteration 3992 - Batch 380/903 - Train loss: 2425336.7076771655\n",
            "Iteration 3993 - Batch 381/903 - Train loss: 2425084.8059554975\n",
            "Iteration 3994 - Batch 382/903 - Train loss: 2424264.9696475198\n",
            "Iteration 3995 - Batch 383/903 - Train loss: 2423582.5322265625\n",
            "Iteration 3996 - Batch 384/903 - Train loss: 2423212.221103896\n",
            "Iteration 3997 - Batch 385/903 - Train loss: 2422880.602007772\n",
            "Iteration 3998 - Batch 386/903 - Train loss: 2422177.713501292\n",
            "Iteration 3999 - Batch 387/903 - Train loss: 2422606.870811856\n",
            "Iteration 4000 - Batch 388/903 - Train loss: 2422212.7426092546\n",
            "Iteration 4001 - Batch 389/903 - Train loss: 2422061.736858974\n",
            "Iteration 4002 - Batch 390/903 - Train loss: 2421750.040601023\n",
            "Iteration 4003 - Batch 391/903 - Train loss: 2420775.0404974488\n",
            "Iteration 4004 - Batch 392/903 - Train loss: 2420757.0900127226\n",
            "Iteration 4005 - Batch 393/903 - Train loss: 2420443.2160532996\n",
            "Iteration 4006 - Batch 394/903 - Train loss: 2419500.2110759495\n",
            "Iteration 4007 - Batch 395/903 - Train loss: 2418501.6631944445\n",
            "Iteration 4008 - Batch 396/903 - Train loss: 2418394.5147984885\n",
            "Iteration 4009 - Batch 397/903 - Train loss: 2417725.012248744\n",
            "Iteration 4010 - Batch 398/903 - Train loss: 2417508.2991854637\n",
            "Iteration 4011 - Batch 399/903 - Train loss: 2417056.3603125\n",
            "Iteration 4012 - Batch 400/903 - Train loss: 2416562.8612842895\n",
            "Iteration 4013 - Batch 401/903 - Train loss: 2415975.458022388\n",
            "Iteration 4014 - Batch 402/903 - Train loss: 2415808.327853598\n",
            "Iteration 4015 - Batch 403/903 - Train loss: 2415397.3876856435\n",
            "Iteration 4016 - Batch 404/903 - Train loss: 2415098.236728395\n",
            "Iteration 4017 - Batch 405/903 - Train loss: 2414179.331896552\n",
            "Iteration 4018 - Batch 406/903 - Train loss: 2413805.3445945946\n",
            "Iteration 4019 - Batch 407/903 - Train loss: 2414265.936887255\n",
            "Iteration 4020 - Batch 408/903 - Train loss: 2413786.5599022005\n",
            "Iteration 4021 - Batch 409/903 - Train loss: 2413399.0390243903\n",
            "Iteration 4022 - Batch 410/903 - Train loss: 2413330.636861314\n",
            "Iteration 4023 - Batch 411/903 - Train loss: 2413550.5776699027\n",
            "Iteration 4024 - Batch 412/903 - Train loss: 2413195.304479419\n",
            "Iteration 4025 - Batch 413/903 - Train loss: 2413177.335748792\n",
            "Iteration 4026 - Batch 414/903 - Train loss: 2413157.546385542\n",
            "Iteration 4027 - Batch 415/903 - Train loss: 2413044.9140625\n",
            "Iteration 4028 - Batch 416/903 - Train loss: 2413015.7152278177\n",
            "Iteration 4029 - Batch 417/903 - Train loss: 2413202.6136363638\n",
            "Iteration 4030 - Batch 418/903 - Train loss: 2413372.2225536993\n",
            "Iteration 4031 - Batch 419/903 - Train loss: 2412933.504761905\n",
            "Iteration 4032 - Batch 420/903 - Train loss: 2412789.6318289787\n",
            "Iteration 4033 - Batch 421/903 - Train loss: 2412567.4573459714\n",
            "Iteration 4034 - Batch 422/903 - Train loss: 2412363.1063829786\n",
            "Iteration 4035 - Batch 423/903 - Train loss: 2411929.129716981\n",
            "Iteration 4036 - Batch 424/903 - Train loss: 2411366.544117647\n",
            "Iteration 4037 - Batch 425/903 - Train loss: 2410961.958920188\n",
            "Iteration 4038 - Batch 426/903 - Train loss: 2410376.8038641685\n",
            "Iteration 4039 - Batch 427/903 - Train loss: 2410357.9667056073\n",
            "Iteration 4040 - Batch 428/903 - Train loss: 2409845.8636363638\n",
            "Iteration 4041 - Batch 429/903 - Train loss: 2409949.787209302\n",
            "Iteration 4042 - Batch 430/903 - Train loss: 2409977.139791183\n",
            "Iteration 4043 - Batch 431/903 - Train loss: 2409961.1927083335\n",
            "Iteration 4044 - Batch 432/903 - Train loss: 2410096.5288683604\n",
            "Iteration 4045 - Batch 433/903 - Train loss: 2409931.7436635946\n",
            "Iteration 4046 - Batch 434/903 - Train loss: 2409066.0761494255\n",
            "Iteration 4047 - Batch 435/903 - Train loss: 2408828.5455848626\n",
            "Iteration 4048 - Batch 436/903 - Train loss: 2409124.595823799\n",
            "Iteration 4049 - Batch 437/903 - Train loss: 2408447.299372146\n",
            "Iteration 4050 - Batch 438/903 - Train loss: 2408218.5885535306\n",
            "Iteration 4051 - Batch 439/903 - Train loss: 2408164.3911931817\n",
            "Iteration 4052 - Batch 440/903 - Train loss: 2408594.753684807\n",
            "Iteration 4053 - Batch 441/903 - Train loss: 2408853.767816742\n",
            "Iteration 4054 - Batch 442/903 - Train loss: 2408883.4224040634\n",
            "Iteration 4055 - Batch 443/903 - Train loss: 2409143.9985923423\n",
            "Iteration 4056 - Batch 444/903 - Train loss: 2409006.843539326\n",
            "Iteration 4057 - Batch 445/903 - Train loss: 2408629.6168721975\n",
            "Iteration 4058 - Batch 446/903 - Train loss: 2408535.288870246\n",
            "Iteration 4059 - Batch 447/903 - Train loss: 2408190.357421875\n",
            "Iteration 4060 - Batch 448/903 - Train loss: 2407797.9434855236\n",
            "Iteration 4061 - Batch 449/903 - Train loss: 2407440.4758333336\n",
            "Iteration 4062 - Batch 450/903 - Train loss: 2407195.2092572064\n",
            "Iteration 4063 - Batch 451/903 - Train loss: 2407326.346515487\n",
            "Iteration 4064 - Batch 452/903 - Train loss: 2407245.2541390727\n",
            "Iteration 4065 - Batch 453/903 - Train loss: 2407111.727147577\n",
            "Iteration 4066 - Batch 454/903 - Train loss: 2407023.8343406594\n",
            "Iteration 4067 - Batch 455/903 - Train loss: 2407164.6384320175\n",
            "Iteration 4068 - Batch 456/903 - Train loss: 2406795.8487417945\n",
            "Iteration 4069 - Batch 457/903 - Train loss: 2406892.2142467247\n",
            "Iteration 4070 - Batch 458/903 - Train loss: 2406757.266067538\n",
            "Iteration 4071 - Batch 459/903 - Train loss: 2406424.222554348\n",
            "Iteration 4072 - Batch 460/903 - Train loss: 2405969.4997288506\n",
            "Iteration 4073 - Batch 461/903 - Train loss: 2405967.4780844157\n",
            "Iteration 4074 - Batch 462/903 - Train loss: 2405617.6023218143\n",
            "Iteration 4075 - Batch 463/903 - Train loss: 2405282.637122845\n",
            "Iteration 4076 - Batch 464/903 - Train loss: 2405327.6400537635\n",
            "Iteration 4077 - Batch 465/903 - Train loss: 2405647.8548819744\n",
            "Iteration 4078 - Batch 466/903 - Train loss: 2405411.168897216\n",
            "Iteration 4079 - Batch 467/903 - Train loss: 2405089.736378205\n",
            "Iteration 4080 - Batch 468/903 - Train loss: 2405064.607942431\n",
            "Iteration 4081 - Batch 469/903 - Train loss: 2404449.257180851\n",
            "Iteration 4082 - Batch 470/903 - Train loss: 2404096.604299363\n",
            "Iteration 4083 - Batch 471/903 - Train loss: 2403923.953654661\n",
            "Iteration 4084 - Batch 472/903 - Train loss: 2403759.2201374206\n",
            "Iteration 4085 - Batch 473/903 - Train loss: 2403447.8251582277\n",
            "Iteration 4086 - Batch 474/903 - Train loss: 2403084.055\n",
            "Iteration 4087 - Batch 475/903 - Train loss: 2402443.063287815\n",
            "Iteration 4088 - Batch 476/903 - Train loss: 2402027.8147274633\n",
            "Iteration 4089 - Batch 477/903 - Train loss: 2402010.092834728\n",
            "Iteration 4090 - Batch 478/903 - Train loss: 2402212.571764092\n",
            "Iteration 4091 - Batch 479/903 - Train loss: 2402553.679427083\n",
            "Iteration 4092 - Batch 480/903 - Train loss: 2402684.159823285\n",
            "Iteration 4093 - Batch 481/903 - Train loss: 2403111.0334543567\n",
            "Iteration 4094 - Batch 482/903 - Train loss: 2403367.148809524\n",
            "Iteration 4095 - Batch 483/903 - Train loss: 2403228.3318698346\n",
            "Iteration 4096 - Batch 484/903 - Train loss: 2402903.1069587627\n",
            "Iteration 4097 - Batch 485/903 - Train loss: 2402661.9945987654\n",
            "Iteration 4098 - Batch 486/903 - Train loss: 2402347.8904004106\n",
            "Iteration 4099 - Batch 487/903 - Train loss: 2402534.5079405736\n",
            "Iteration 4100 - Batch 488/903 - Train loss: 2402481.5242842534\n",
            "Iteration 4101 - Batch 489/903 - Train loss: 2402169.7461734693\n",
            "Iteration 4102 - Batch 490/903 - Train loss: 2402320.9182790224\n",
            "Iteration 4103 - Batch 491/903 - Train loss: 2401996.893038618\n",
            "Iteration 4104 - Batch 492/903 - Train loss: 2401892.3582657203\n",
            "Iteration 4105 - Batch 493/903 - Train loss: 2401679.138917004\n",
            "Iteration 4106 - Batch 494/903 - Train loss: 2401591.0896464647\n",
            "Iteration 4107 - Batch 495/903 - Train loss: 2401230.3364415322\n",
            "Iteration 4108 - Batch 496/903 - Train loss: 2400954.7789235413\n",
            "Iteration 4109 - Batch 497/903 - Train loss: 2400575.113704819\n",
            "Iteration 4110 - Batch 498/903 - Train loss: 2400298.747745491\n",
            "Iteration 4111 - Batch 499/903 - Train loss: 2400505.49525\n",
            "Iteration 4112 - Batch 500/903 - Train loss: 2400554.834580838\n",
            "Iteration 4113 - Batch 501/903 - Train loss: 2400631.437001992\n",
            "Iteration 4114 - Batch 502/903 - Train loss: 2401048.79249503\n",
            "Iteration 4115 - Batch 503/903 - Train loss: 2401094.9992559524\n",
            "Iteration 4116 - Batch 504/903 - Train loss: 2401151.1725247526\n",
            "Iteration 4117 - Batch 505/903 - Train loss: 2401103.000741107\n",
            "Iteration 4118 - Batch 506/903 - Train loss: 2400852.9341715975\n",
            "Iteration 4119 - Batch 507/903 - Train loss: 2400794.8033956694\n",
            "Iteration 4120 - Batch 508/903 - Train loss: 2401141.773821218\n",
            "Iteration 4121 - Batch 509/903 - Train loss: 2401353.443382353\n",
            "Iteration 4122 - Batch 510/903 - Train loss: 2401283.2380136987\n",
            "Iteration 4123 - Batch 511/903 - Train loss: 2401154.5256347656\n",
            "Iteration 4124 - Batch 512/903 - Train loss: 2401257.574317739\n",
            "Iteration 4125 - Batch 513/903 - Train loss: 2401576.3382782103\n",
            "Iteration 4126 - Batch 514/903 - Train loss: 2401740.7555825245\n",
            "Iteration 4127 - Batch 515/903 - Train loss: 2401774.597141473\n",
            "Iteration 4128 - Batch 516/903 - Train loss: 2401403.5669729207\n",
            "Iteration 4129 - Batch 517/903 - Train loss: 2401472.030646718\n",
            "Iteration 4130 - Batch 518/903 - Train loss: 2401970.175096339\n",
            "Iteration 4131 - Batch 519/903 - Train loss: 2402817.015144231\n",
            "Iteration 4132 - Batch 520/903 - Train loss: 2403040.3390115164\n",
            "Iteration 4133 - Batch 521/903 - Train loss: 2402985.9604885057\n",
            "Iteration 4134 - Batch 522/903 - Train loss: 2402797.3028202676\n",
            "Iteration 4135 - Batch 523/903 - Train loss: 2402857.932013359\n",
            "Iteration 4136 - Batch 524/903 - Train loss: 2403331.874047619\n",
            "Iteration 4137 - Batch 525/903 - Train loss: 2405362.2226711027\n",
            "Iteration 4138 - Batch 526/903 - Train loss: 2409071.973671727\n",
            "Iteration 4139 - Batch 527/903 - Train loss: 2413037.9746685605\n",
            "Iteration 4140 - Batch 528/903 - Train loss: 2413604.150519849\n",
            "Iteration 4141 - Batch 529/903 - Train loss: 2414226.6818396226\n",
            "Iteration 4142 - Batch 530/903 - Train loss: 2417678.711629002\n",
            "Iteration 4143 - Batch 531/903 - Train loss: 2419042.3672462404\n",
            "Iteration 4144 - Batch 532/903 - Train loss: 2419227.637664165\n",
            "Iteration 4145 - Batch 533/903 - Train loss: 2421432.8494850188\n",
            "Iteration 4146 - Batch 534/903 - Train loss: 2423228.3824766357\n",
            "Iteration 4147 - Batch 535/903 - Train loss: 2423741.0296175373\n",
            "Iteration 4148 - Batch 536/903 - Train loss: 2426129.831238361\n",
            "Iteration 4149 - Batch 537/903 - Train loss: 2428178.8859200743\n",
            "Iteration 4150 - Batch 538/903 - Train loss: 2428971.279916512\n",
            "Iteration 4151 - Batch 539/903 - Train loss: 2430016.144675926\n",
            "Iteration 4152 - Batch 540/903 - Train loss: 2430537.50577634\n",
            "Iteration 4153 - Batch 541/903 - Train loss: 2431108.6787361624\n",
            "Iteration 4154 - Batch 542/903 - Train loss: 2431572.7156998157\n",
            "Iteration 4155 - Batch 543/903 - Train loss: 2432036.228630515\n",
            "Iteration 4156 - Batch 544/903 - Train loss: 2432034.2006880734\n",
            "Iteration 4157 - Batch 545/903 - Train loss: 2432649.431547619\n",
            "Iteration 4158 - Batch 546/903 - Train loss: 2433261.1771023767\n",
            "Iteration 4159 - Batch 547/903 - Train loss: 2433389.8309762776\n",
            "Iteration 4160 - Batch 548/903 - Train loss: 2433696.0507741347\n",
            "Iteration 4161 - Batch 549/903 - Train loss: 2433677.0511363638\n",
            "Iteration 4162 - Batch 550/903 - Train loss: 2433842.9961433755\n",
            "Iteration 4163 - Batch 551/903 - Train loss: 2433683.4010416665\n",
            "Iteration 4164 - Batch 552/903 - Train loss: 2433983.680153707\n",
            "Iteration 4165 - Batch 553/903 - Train loss: 2434158.904106498\n",
            "Iteration 4166 - Batch 554/903 - Train loss: 2434476.9903153153\n",
            "Iteration 4167 - Batch 555/903 - Train loss: 2434934.3082284173\n",
            "Iteration 4168 - Batch 556/903 - Train loss: 2434863.600314183\n",
            "Iteration 4169 - Batch 557/903 - Train loss: 2434844.517249104\n",
            "Iteration 4170 - Batch 558/903 - Train loss: 2434945.678220036\n",
            "Iteration 4171 - Batch 559/903 - Train loss: 2435255.3542410713\n",
            "Iteration 4172 - Batch 560/903 - Train loss: 2434897.0300802137\n",
            "Iteration 4173 - Batch 561/903 - Train loss: 2434711.994439502\n",
            "Iteration 4174 - Batch 562/903 - Train loss: 2434622.299955595\n",
            "Iteration 4175 - Batch 563/903 - Train loss: 2434571.581781915\n",
            "Iteration 4176 - Batch 564/903 - Train loss: 2434989.8882743362\n",
            "Iteration 4177 - Batch 565/903 - Train loss: 2435686.7833480565\n",
            "Iteration 4178 - Batch 566/903 - Train loss: 2435844.509920635\n",
            "Iteration 4179 - Batch 567/903 - Train loss: 2435737.8721390846\n",
            "Iteration 4180 - Batch 568/903 - Train loss: 2435614.4070738135\n",
            "Iteration 4181 - Batch 569/903 - Train loss: 2435652.065131579\n",
            "Iteration 4182 - Batch 570/903 - Train loss: 2435537.4087127848\n",
            "Iteration 4183 - Batch 571/903 - Train loss: 2434746.544143357\n",
            "Iteration 4184 - Batch 572/903 - Train loss: 2434827.072425829\n",
            "Iteration 4185 - Batch 573/903 - Train loss: 2435121.9969512196\n",
            "Iteration 4186 - Batch 574/903 - Train loss: 2434871.7591304346\n",
            "Iteration 4187 - Batch 575/903 - Train loss: 2434889.642361111\n",
            "Iteration 4188 - Batch 576/903 - Train loss: 2434737.578422877\n",
            "Iteration 4189 - Batch 577/903 - Train loss: 2434115.617430796\n",
            "Iteration 4190 - Batch 578/903 - Train loss: 2434108.8180051814\n",
            "Iteration 4191 - Batch 579/903 - Train loss: 2434330.097198276\n",
            "Iteration 4192 - Batch 580/903 - Train loss: 2434249.8126075733\n",
            "Iteration 4193 - Batch 581/903 - Train loss: 2434182.5942869415\n",
            "Iteration 4194 - Batch 582/903 - Train loss: 2434379.4277444254\n",
            "Iteration 4195 - Batch 583/903 - Train loss: 2434892.6902825343\n",
            "Iteration 4196 - Batch 584/903 - Train loss: 2434406.079273504\n",
            "Iteration 4197 - Batch 585/903 - Train loss: 2434402.310366894\n",
            "Iteration 4198 - Batch 586/903 - Train loss: 2434532.263415673\n",
            "Iteration 4199 - Batch 587/903 - Train loss: 2434612.78082483\n",
            "Iteration 4200 - Batch 588/903 - Train loss: 2434126.314303905\n",
            "Iteration 4201 - Batch 589/903 - Train loss: 2434083.6565677966\n",
            "Iteration 4202 - Batch 590/903 - Train loss: 2434593.154187817\n",
            "Iteration 4203 - Batch 591/903 - Train loss: 2434468.764569257\n",
            "Iteration 4204 - Batch 592/903 - Train loss: 2434143.092537943\n",
            "Iteration 4205 - Batch 593/903 - Train loss: 2434065.98253367\n",
            "Iteration 4206 - Batch 594/903 - Train loss: 2434600.5905462187\n",
            "Iteration 4207 - Batch 595/903 - Train loss: 2434637.842491611\n",
            "Iteration 4208 - Batch 596/903 - Train loss: 2434362.360762144\n",
            "Iteration 4209 - Batch 597/903 - Train loss: 2434182.412416388\n",
            "Iteration 4210 - Batch 598/903 - Train loss: 2434183.036936561\n",
            "Iteration 4211 - Batch 599/903 - Train loss: 2433790.3152083335\n",
            "Iteration 4212 - Batch 600/903 - Train loss: 2434077.0131031615\n",
            "Iteration 4213 - Batch 601/903 - Train loss: 2434482.3025332224\n",
            "Iteration 4214 - Batch 602/903 - Train loss: 2434768.4355306798\n",
            "Iteration 4215 - Batch 603/903 - Train loss: 2434618.6943294704\n",
            "Iteration 4216 - Batch 604/903 - Train loss: 2435035.848966942\n",
            "Iteration 4217 - Batch 605/903 - Train loss: 2435527.1351072607\n",
            "Iteration 4218 - Batch 606/903 - Train loss: 2435293.0842257002\n",
            "Iteration 4219 - Batch 607/903 - Train loss: 2435210.154399671\n",
            "Iteration 4220 - Batch 608/903 - Train loss: 2435678.9213875206\n",
            "Iteration 4221 - Batch 609/903 - Train loss: 2435718.038319672\n",
            "Iteration 4222 - Batch 610/903 - Train loss: 2435404.5591243864\n",
            "Iteration 4223 - Batch 611/903 - Train loss: 2435304.717116013\n",
            "Iteration 4224 - Batch 612/903 - Train loss: 2435355.3293230017\n",
            "Iteration 4225 - Batch 613/903 - Train loss: 2434986.7668973943\n",
            "Iteration 4226 - Batch 614/903 - Train loss: 2435224.479878049\n",
            "Iteration 4227 - Batch 615/903 - Train loss: 2435567.2506087665\n",
            "Iteration 4228 - Batch 616/903 - Train loss: 2434923.1936790925\n",
            "Iteration 4229 - Batch 617/903 - Train loss: 2434634.6254045307\n",
            "Iteration 4230 - Batch 618/903 - Train loss: 2434693.3707592892\n",
            "Iteration 4231 - Batch 619/903 - Train loss: 2434570.631048387\n",
            "Iteration 4232 - Batch 620/903 - Train loss: 2434284.6682769726\n",
            "Iteration 4233 - Batch 621/903 - Train loss: 2433940.393890675\n",
            "Iteration 4234 - Batch 622/903 - Train loss: 2433907.488764045\n",
            "Iteration 4235 - Batch 623/903 - Train loss: 2433738.249599359\n",
            "Iteration 4236 - Batch 624/903 - Train loss: 2433883.4012\n",
            "Iteration 4237 - Batch 625/903 - Train loss: 2433808.4013578277\n",
            "Iteration 4238 - Batch 626/903 - Train loss: 2433514.8137958534\n",
            "Iteration 4239 - Batch 627/903 - Train loss: 2433485.3483280255\n",
            "Iteration 4240 - Batch 628/903 - Train loss: 2433315.3521462637\n",
            "Iteration 4241 - Batch 629/903 - Train loss: 2432716.257142857\n",
            "Iteration 4242 - Batch 630/903 - Train loss: 2432454.062599049\n",
            "Iteration 4243 - Batch 631/903 - Train loss: 2432034.6744462023\n",
            "Iteration 4244 - Batch 632/903 - Train loss: 2432131.079778831\n",
            "Iteration 4245 - Batch 633/903 - Train loss: 2431984.199526814\n",
            "Iteration 4246 - Batch 634/903 - Train loss: 2432157.7578740157\n",
            "Iteration 4247 - Batch 635/903 - Train loss: 2431949.9791666665\n",
            "Iteration 4248 - Batch 636/903 - Train loss: 2432061.6664050235\n",
            "Iteration 4249 - Batch 637/903 - Train loss: 2431884.897727273\n",
            "Iteration 4250 - Batch 638/903 - Train loss: 2432050.9417057903\n",
            "Iteration 4251 - Batch 639/903 - Train loss: 2431980.415234375\n",
            "Iteration 4252 - Batch 640/903 - Train loss: 2431587.8646645867\n",
            "Iteration 4253 - Batch 641/903 - Train loss: 2431412.10241433\n",
            "Iteration 4254 - Batch 642/903 - Train loss: 2431571.5066096424\n",
            "Iteration 4255 - Batch 643/903 - Train loss: 2431356.6118012425\n",
            "Iteration 4256 - Batch 644/903 - Train loss: 2430941.803488372\n",
            "Iteration 4257 - Batch 645/903 - Train loss: 2430637.6071981424\n",
            "Iteration 4258 - Batch 646/903 - Train loss: 2430296.819165379\n",
            "Iteration 4259 - Batch 647/903 - Train loss: 2430369.545138889\n",
            "Iteration 4260 - Batch 648/903 - Train loss: 2430073.8620955315\n",
            "Iteration 4261 - Batch 649/903 - Train loss: 2429466.7325\n",
            "Iteration 4262 - Batch 650/903 - Train loss: 2429343.303955453\n",
            "Iteration 4263 - Batch 651/903 - Train loss: 2429200.931940184\n",
            "Iteration 4264 - Batch 652/903 - Train loss: 2428724.585183767\n",
            "Iteration 4265 - Batch 653/903 - Train loss: 2428294.7746559633\n",
            "Iteration 4266 - Batch 654/903 - Train loss: 2428317.7471374045\n",
            "Iteration 4267 - Batch 655/903 - Train loss: 2428179.8294588416\n",
            "Iteration 4268 - Batch 656/903 - Train loss: 2427817.6569634704\n",
            "Iteration 4269 - Batch 657/903 - Train loss: 2427449.9078647415\n",
            "Iteration 4270 - Batch 658/903 - Train loss: 2427099.840477997\n",
            "Iteration 4271 - Batch 659/903 - Train loss: 2426858.8138257577\n",
            "Iteration 4272 - Batch 660/903 - Train loss: 2426650.542170953\n",
            "Iteration 4273 - Batch 661/903 - Train loss: 2426275.0364425983\n",
            "Iteration 4274 - Batch 662/903 - Train loss: 2425829.516402715\n",
            "Iteration 4275 - Batch 663/903 - Train loss: 2425471.1963478914\n",
            "Iteration 4276 - Batch 664/903 - Train loss: 2425215.3712406014\n",
            "Iteration 4277 - Batch 665/903 - Train loss: 2424823.4630255257\n",
            "Iteration 4278 - Batch 666/903 - Train loss: 2424616.5046851574\n",
            "Iteration 4279 - Batch 667/903 - Train loss: 2424430.033495509\n",
            "Iteration 4280 - Batch 668/903 - Train loss: 2424377.1799327354\n",
            "Iteration 4281 - Batch 669/903 - Train loss: 2424260.8427238804\n",
            "Iteration 4282 - Batch 670/903 - Train loss: 2423961.318368107\n",
            "Iteration 4283 - Batch 671/903 - Train loss: 2423687.672061012\n",
            "Iteration 4284 - Batch 672/903 - Train loss: 2423327.626114413\n",
            "Iteration 4285 - Batch 673/903 - Train loss: 2422813.3099035607\n",
            "Iteration 4286 - Batch 674/903 - Train loss: 2422762.743148148\n",
            "Iteration 4287 - Batch 675/903 - Train loss: 2422643.551590237\n",
            "Iteration 4288 - Batch 676/903 - Train loss: 2422205.1892540623\n",
            "Iteration 4289 - Batch 677/903 - Train loss: 2421561.1244469024\n",
            "Iteration 4290 - Batch 678/903 - Train loss: 2421387.1110088364\n",
            "Iteration 4291 - Batch 679/903 - Train loss: 2421625.821875\n",
            "Iteration 4292 - Batch 680/903 - Train loss: 2421879.5299192364\n",
            "Iteration 4293 - Batch 681/903 - Train loss: 2421955.9536290322\n",
            "Iteration 4294 - Batch 682/903 - Train loss: 2421759.6901537334\n",
            "Iteration 4295 - Batch 683/903 - Train loss: 2421267.742141813\n",
            "Iteration 4296 - Batch 684/903 - Train loss: 2421115.7217153283\n",
            "Iteration 4297 - Batch 685/903 - Train loss: 2421058.594205539\n",
            "Iteration 4298 - Batch 686/903 - Train loss: 2420657.1137190685\n",
            "Iteration 4299 - Batch 687/903 - Train loss: 2420331.7621729653\n",
            "Iteration 4300 - Batch 688/903 - Train loss: 2420406.328918723\n",
            "Iteration 4301 - Batch 689/903 - Train loss: 2420720.155253623\n",
            "Iteration 4302 - Batch 690/903 - Train loss: 2420771.2328147613\n",
            "Iteration 4303 - Batch 691/903 - Train loss: 2420597.391799133\n",
            "Iteration 4304 - Batch 692/903 - Train loss: 2420198.557900433\n",
            "Iteration 4305 - Batch 693/903 - Train loss: 2420062.3557276656\n",
            "Iteration 4306 - Batch 694/903 - Train loss: 2419806.0947841727\n",
            "Iteration 4307 - Batch 695/903 - Train loss: 2419537.6898347703\n",
            "Iteration 4308 - Batch 696/903 - Train loss: 2419292.4399210904\n",
            "Iteration 4309 - Batch 697/903 - Train loss: 2419047.551396848\n",
            "Iteration 4310 - Batch 698/903 - Train loss: 2418769.5305793993\n",
            "Iteration 4311 - Batch 699/903 - Train loss: 2418201.7098214286\n",
            "Iteration 4312 - Batch 700/903 - Train loss: 2417913.8653708985\n",
            "Iteration 4313 - Batch 701/903 - Train loss: 2417531.781160969\n",
            "Iteration 4314 - Batch 702/903 - Train loss: 2417211.7814722615\n",
            "Iteration 4315 - Batch 703/903 - Train loss: 2416921.7164417612\n",
            "Iteration 4316 - Batch 704/903 - Train loss: 2416487.8466312056\n",
            "Iteration 4317 - Batch 705/903 - Train loss: 2416327.1287181303\n",
            "Iteration 4318 - Batch 706/903 - Train loss: 2416076.6543493634\n",
            "Iteration 4319 - Batch 707/903 - Train loss: 2415797.2420550846\n",
            "Iteration 4320 - Batch 708/903 - Train loss: 2415564.680007052\n",
            "Iteration 4321 - Batch 709/903 - Train loss: 2415042.926056338\n",
            "Iteration 4322 - Batch 710/903 - Train loss: 2414463.0393811534\n",
            "Iteration 4323 - Batch 711/903 - Train loss: 2414416.0463483147\n",
            "Iteration 4324 - Batch 712/903 - Train loss: 2414348.9880785416\n",
            "Iteration 4325 - Batch 713/903 - Train loss: 2413824.841736695\n",
            "Iteration 4326 - Batch 714/903 - Train loss: 2413808.05979021\n",
            "Iteration 4327 - Batch 715/903 - Train loss: 2413615.568086592\n",
            "Iteration 4328 - Batch 716/903 - Train loss: 2413740.998953975\n",
            "Iteration 4329 - Batch 717/903 - Train loss: 2413667.8746518106\n",
            "Iteration 4330 - Batch 718/903 - Train loss: 2413695.9130737134\n",
            "Iteration 4331 - Batch 719/903 - Train loss: 2413753.792708333\n",
            "Iteration 4332 - Batch 720/903 - Train loss: 2413657.2617891817\n",
            "Iteration 4333 - Batch 721/903 - Train loss: 2413270.093490305\n",
            "Iteration 4334 - Batch 722/903 - Train loss: 2412965.850968188\n",
            "Iteration 4335 - Batch 723/903 - Train loss: 2413199.291436464\n",
            "Iteration 4336 - Batch 724/903 - Train loss: 2413104.607241379\n",
            "Iteration 4337 - Batch 725/903 - Train loss: 2413124.256198347\n",
            "Iteration 4338 - Batch 726/903 - Train loss: 2413025.689477304\n",
            "Iteration 4339 - Batch 727/903 - Train loss: 2412892.238324176\n",
            "Iteration 4340 - Batch 728/903 - Train loss: 2412709.6447187928\n",
            "Iteration 4341 - Batch 729/903 - Train loss: 2412403.3688356164\n",
            "Iteration 4342 - Batch 730/903 - Train loss: 2412110.453146375\n",
            "Iteration 4343 - Batch 731/903 - Train loss: 2411830.887636612\n",
            "Iteration 4344 - Batch 732/903 - Train loss: 2411371.755457026\n",
            "Iteration 4345 - Batch 733/903 - Train loss: 2411233.4591280655\n",
            "Iteration 4346 - Batch 734/903 - Train loss: 2411029.450340136\n",
            "Iteration 4347 - Batch 735/903 - Train loss: 2410763.384171196\n",
            "Iteration 4348 - Batch 736/903 - Train loss: 2410426.6529850746\n",
            "Iteration 4349 - Batch 737/903 - Train loss: 2410166.316395664\n",
            "Iteration 4350 - Batch 738/903 - Train loss: 2409983.3924221923\n",
            "Iteration 4351 - Batch 739/903 - Train loss: 2409966.093581081\n",
            "Iteration 4352 - Batch 740/903 - Train loss: 2410055.601889339\n",
            "Iteration 4353 - Batch 741/903 - Train loss: 2409987.9861859838\n",
            "Iteration 4354 - Batch 742/903 - Train loss: 2409829.9236204578\n",
            "Iteration 4355 - Batch 743/903 - Train loss: 2409835.012768817\n",
            "Iteration 4356 - Batch 744/903 - Train loss: 2409913.617114094\n",
            "Iteration 4357 - Batch 745/903 - Train loss: 2409646.2174932975\n",
            "Iteration 4358 - Batch 746/903 - Train loss: 2409528.8657965194\n",
            "Iteration 4359 - Batch 747/903 - Train loss: 2409447.2152406415\n",
            "Iteration 4360 - Batch 748/903 - Train loss: 2409223.042056075\n",
            "Iteration 4361 - Batch 749/903 - Train loss: 2409198.8193333335\n",
            "Iteration 4362 - Batch 750/903 - Train loss: 2409205.021304927\n",
            "Iteration 4363 - Batch 751/903 - Train loss: 2409009.1875\n",
            "Iteration 4364 - Batch 752/903 - Train loss: 2408672.012948207\n",
            "Iteration 4365 - Batch 753/903 - Train loss: 2408675.8633952253\n",
            "Iteration 4366 - Batch 754/903 - Train loss: 2408700.734437086\n",
            "Iteration 4367 - Batch 755/903 - Train loss: 2408399.779761905\n",
            "Iteration 4368 - Batch 756/903 - Train loss: 2408247.1978203435\n",
            "Iteration 4369 - Batch 757/903 - Train loss: 2407983.0600263854\n",
            "Iteration 4370 - Batch 758/903 - Train loss: 2408102.7127799736\n",
            "Iteration 4371 - Batch 759/903 - Train loss: 2407764.985526316\n",
            "Iteration 4372 - Batch 760/903 - Train loss: 2407456.5834428384\n",
            "Iteration 4373 - Batch 761/903 - Train loss: 2407377.447178478\n",
            "Iteration 4374 - Batch 762/903 - Train loss: 2407274.3804062908\n",
            "Iteration 4375 - Batch 763/903 - Train loss: 2406892.198298429\n",
            "Iteration 4376 - Batch 764/903 - Train loss: 2406664.03627451\n",
            "Iteration 4377 - Batch 765/903 - Train loss: 2406514.282310705\n",
            "Iteration 4378 - Batch 766/903 - Train loss: 2406248.2773794\n",
            "Iteration 4379 - Batch 767/903 - Train loss: 2406094.6025390625\n",
            "Iteration 4380 - Batch 768/903 - Train loss: 2405971.1293888167\n",
            "Iteration 4381 - Batch 769/903 - Train loss: 2405646.033766234\n",
            "Iteration 4382 - Batch 770/903 - Train loss: 2405290.8592736707\n",
            "Iteration 4383 - Batch 771/903 - Train loss: 2405153.777525907\n",
            "Iteration 4384 - Batch 772/903 - Train loss: 2404890.371927555\n",
            "Iteration 4385 - Batch 773/903 - Train loss: 2404439.988856589\n",
            "Iteration 4386 - Batch 774/903 - Train loss: 2404329.256935484\n",
            "Iteration 4387 - Batch 775/903 - Train loss: 2403967.885148196\n",
            "Iteration 4388 - Batch 776/903 - Train loss: 2403685.217985843\n",
            "Iteration 4389 - Batch 777/903 - Train loss: 2403143.4951799484\n",
            "Iteration 4390 - Batch 778/903 - Train loss: 2402762.1370346597\n",
            "Iteration 4391 - Batch 779/903 - Train loss: 2402481.611858974\n",
            "Iteration 4392 - Batch 780/903 - Train loss: 2402202.4526248397\n",
            "Iteration 4393 - Batch 781/903 - Train loss: 2401998.514386189\n",
            "Iteration 4394 - Batch 782/903 - Train loss: 2401948.2439335887\n",
            "Iteration 4395 - Batch 783/903 - Train loss: 2401542.256218112\n",
            "Iteration 4396 - Batch 784/903 - Train loss: 2401519.35111465\n",
            "Iteration 4397 - Batch 785/903 - Train loss: 2401152.044688295\n",
            "Iteration 4398 - Batch 786/903 - Train loss: 2400866.8457750953\n",
            "Iteration 4399 - Batch 787/903 - Train loss: 2400291.1668781727\n",
            "Iteration 4400 - Batch 788/903 - Train loss: 2399920.4746514577\n",
            "Iteration 4401 - Batch 789/903 - Train loss: 2399736.241772152\n",
            "Iteration 4402 - Batch 790/903 - Train loss: 2399825.5006321114\n",
            "Iteration 4403 - Batch 791/903 - Train loss: 2399768.053977273\n",
            "Iteration 4404 - Batch 792/903 - Train loss: 2399650.043820933\n",
            "Iteration 4405 - Batch 793/903 - Train loss: 2399183.3554785894\n",
            "Iteration 4406 - Batch 794/903 - Train loss: 2399203.882389937\n",
            "Iteration 4407 - Batch 795/903 - Train loss: 2399132.846419598\n",
            "Iteration 4408 - Batch 796/903 - Train loss: 2399260.6254705144\n",
            "Iteration 4409 - Batch 797/903 - Train loss: 2398887.543546366\n",
            "Iteration 4410 - Batch 798/903 - Train loss: 2398780.6414267835\n",
            "Iteration 4411 - Batch 799/903 - Train loss: 2398671.876875\n",
            "Iteration 4412 - Batch 800/903 - Train loss: 2398371.270287141\n",
            "Iteration 4413 - Batch 801/903 - Train loss: 2398155.904925187\n",
            "Iteration 4414 - Batch 802/903 - Train loss: 2397922.485678705\n",
            "Iteration 4415 - Batch 803/903 - Train loss: 2397919.5755597013\n",
            "Iteration 4416 - Batch 804/903 - Train loss: 2397956.245341615\n",
            "Iteration 4417 - Batch 805/903 - Train loss: 2398046.9438585606\n",
            "Iteration 4418 - Batch 806/903 - Train loss: 2398027.394361834\n",
            "Iteration 4419 - Batch 807/903 - Train loss: 2398069.1135519804\n",
            "Iteration 4420 - Batch 808/903 - Train loss: 2397576.747991347\n",
            "Iteration 4421 - Batch 809/903 - Train loss: 2397449.6804012344\n",
            "Iteration 4422 - Batch 810/903 - Train loss: 2397332.5950986436\n",
            "Iteration 4423 - Batch 811/903 - Train loss: 2397441.09375\n",
            "Iteration 4424 - Batch 812/903 - Train loss: 2397372.138222632\n",
            "Iteration 4425 - Batch 813/903 - Train loss: 2397089.3349201474\n",
            "Iteration 4426 - Batch 814/903 - Train loss: 2396966.394018405\n",
            "Iteration 4427 - Batch 815/903 - Train loss: 2396642.0353860296\n",
            "Iteration 4428 - Batch 816/903 - Train loss: 2396567.0078029376\n",
            "Iteration 4429 - Batch 817/903 - Train loss: 2396700.538661369\n",
            "Iteration 4430 - Batch 818/903 - Train loss: 2396666.20528083\n",
            "Iteration 4431 - Batch 819/903 - Train loss: 2396614.874847561\n",
            "Iteration 4432 - Batch 820/903 - Train loss: 2396744.838459196\n",
            "Iteration 4433 - Batch 821/903 - Train loss: 2396460.3289233577\n",
            "Iteration 4434 - Batch 822/903 - Train loss: 2396520.8828979344\n",
            "Iteration 4435 - Batch 823/903 - Train loss: 2396296.4664745145\n",
            "Iteration 4436 - Batch 824/903 - Train loss: 2396181.602878788\n",
            "Iteration 4437 - Batch 825/903 - Train loss: 2396300.622730024\n",
            "Iteration 4438 - Batch 826/903 - Train loss: 2396331.016475212\n",
            "Iteration 4439 - Batch 827/903 - Train loss: 2396473.982638889\n",
            "Iteration 4440 - Batch 828/903 - Train loss: 2396588.7818154404\n",
            "Iteration 4441 - Batch 829/903 - Train loss: 2396506.0748493974\n",
            "Iteration 4442 - Batch 830/903 - Train loss: 2396653.180655836\n",
            "Iteration 4443 - Batch 831/903 - Train loss: 2397357.907001202\n",
            "Iteration 4444 - Batch 832/903 - Train loss: 2397712.4020108045\n",
            "Iteration 4445 - Batch 833/903 - Train loss: 2397899.9818645082\n",
            "Iteration 4446 - Batch 834/903 - Train loss: 2397771.6211077846\n",
            "Iteration 4447 - Batch 835/903 - Train loss: 2397724.103020335\n",
            "Iteration 4448 - Batch 836/903 - Train loss: 2398005.234617682\n",
            "Iteration 4449 - Batch 837/903 - Train loss: 2398128.833681384\n",
            "Iteration 4450 - Batch 838/903 - Train loss: 2398036.0221990463\n",
            "Iteration 4451 - Batch 839/903 - Train loss: 2397700.2436011904\n",
            "Iteration 4452 - Batch 840/903 - Train loss: 2397861.997473246\n",
            "Iteration 4453 - Batch 841/903 - Train loss: 2397906.882274347\n",
            "Iteration 4454 - Batch 842/903 - Train loss: 2397779.9515124555\n",
            "Iteration 4455 - Batch 843/903 - Train loss: 2397648.9311315166\n",
            "Iteration 4456 - Batch 844/903 - Train loss: 2397408.399260355\n",
            "Iteration 4457 - Batch 845/903 - Train loss: 2397259.325206856\n",
            "Iteration 4458 - Batch 846/903 - Train loss: 2397407.1276564347\n",
            "Iteration 4459 - Batch 847/903 - Train loss: 2397291.0257959906\n",
            "Iteration 4460 - Batch 848/903 - Train loss: 2397187.391489988\n",
            "Iteration 4461 - Batch 849/903 - Train loss: 2397085.8130882354\n",
            "Iteration 4462 - Batch 850/903 - Train loss: 2397261.4390423032\n",
            "Iteration 4463 - Batch 851/903 - Train loss: 2397371.929137324\n",
            "Iteration 4464 - Batch 852/903 - Train loss: 2397262.860345838\n",
            "Iteration 4465 - Batch 853/903 - Train loss: 2397626.2103337236\n",
            "Iteration 4466 - Batch 854/903 - Train loss: 2397548.4864035086\n",
            "Iteration 4467 - Batch 855/903 - Train loss: 2397322.9198306073\n",
            "Iteration 4468 - Batch 856/903 - Train loss: 2396958.650816803\n",
            "Iteration 4469 - Batch 857/903 - Train loss: 2396603.288607226\n",
            "Iteration 4470 - Batch 858/903 - Train loss: 2396317.138096624\n",
            "Iteration 4471 - Batch 859/903 - Train loss: 2396102.769331395\n",
            "Iteration 4472 - Batch 860/903 - Train loss: 2395928.794570267\n",
            "Iteration 4473 - Batch 861/903 - Train loss: 2395995.878335267\n",
            "Iteration 4474 - Batch 862/903 - Train loss: 2395881.742323291\n",
            "Iteration 4475 - Batch 863/903 - Train loss: 2395874.039496528\n",
            "Iteration 4476 - Batch 864/903 - Train loss: 2395746.679046243\n",
            "Iteration 4477 - Batch 865/903 - Train loss: 2395631.8220265587\n",
            "Iteration 4478 - Batch 866/903 - Train loss: 2395159.7507208767\n",
            "Iteration 4479 - Batch 867/903 - Train loss: 2394843.412298387\n",
            "Iteration 4480 - Batch 868/903 - Train loss: 2394451.690304948\n",
            "Iteration 4481 - Batch 869/903 - Train loss: 2394273.3728448274\n",
            "Iteration 4482 - Batch 870/903 - Train loss: 2394118.108065442\n",
            "Iteration 4483 - Batch 871/903 - Train loss: 2393920.7097190367\n",
            "Iteration 4484 - Batch 872/903 - Train loss: 2393800.3890320733\n",
            "Iteration 4485 - Batch 873/903 - Train loss: 2393675.0210240274\n",
            "Iteration 4486 - Batch 874/903 - Train loss: 2393458.092714286\n",
            "Iteration 4487 - Batch 875/903 - Train loss: 2393285.9205194064\n",
            "Iteration 4488 - Batch 876/903 - Train loss: 2393176.746721779\n",
            "Iteration 4489 - Batch 877/903 - Train loss: 2393255.1026480636\n",
            "Iteration 4490 - Batch 878/903 - Train loss: 2393085.5098122866\n",
            "Iteration 4491 - Batch 879/903 - Train loss: 2392803.5776988636\n",
            "Iteration 4492 - Batch 880/903 - Train loss: 2392708.1156356414\n",
            "Iteration 4493 - Batch 881/903 - Train loss: 2392663.9692460317\n",
            "Iteration 4494 - Batch 882/903 - Train loss: 2392652.0717723668\n",
            "Iteration 4495 - Batch 883/903 - Train loss: 2392464.8881504526\n",
            "Iteration 4496 - Batch 884/903 - Train loss: 2392217.6122881356\n",
            "Iteration 4497 - Batch 885/903 - Train loss: 2392147.7806151244\n",
            "Iteration 4498 - Batch 886/903 - Train loss: 2392160.662204059\n",
            "Iteration 4499 - Batch 887/903 - Train loss: 2392099.2416948196\n",
            "Iteration 4500 - Batch 888/903 - Train loss: 2392335.376406074\n",
            "Iteration 4501 - Batch 889/903 - Train loss: 2392189.043960674\n",
            "Iteration 4502 - Batch 890/903 - Train loss: 2391972.648849607\n",
            "Iteration 4503 - Batch 891/903 - Train loss: 2391846.150924888\n",
            "Iteration 4504 - Batch 892/903 - Train loss: 2391937.548852184\n",
            "Iteration 4505 - Batch 893/903 - Train loss: 2391995.335989933\n",
            "Iteration 4506 - Batch 894/903 - Train loss: 2392030.0532122906\n",
            "Iteration 4507 - Batch 895/903 - Train loss: 2391762.97140067\n",
            "Iteration 4508 - Batch 896/903 - Train loss: 2391701.736204013\n",
            "Iteration 4509 - Batch 897/903 - Train loss: 2391908.9138363027\n",
            "Iteration 4510 - Batch 898/903 - Train loss: 2391819.120272525\n",
            "Iteration 4511 - Batch 899/903 - Train loss: 2391694.6456944444\n",
            "Iteration 4512 - Batch 900/903 - Train loss: 2391419.9060765817\n",
            "Iteration 4513 - Batch 901/903 - Train loss: 2391384.195260532\n",
            "Iteration 4514 - Batch 902/903 - Train loss: 2389372.653654485\n",
            "Val loss: 2250349.75\n",
            "Epoch 6/6\n",
            "Iteration 4516 - Batch 1/903 - Train loss: 3608819.5\n",
            "Iteration 4517 - Batch 2/903 - Train loss: 3715889.5833333335\n",
            "Iteration 4518 - Batch 3/903 - Train loss: 3498783.1875\n",
            "Iteration 4519 - Batch 4/903 - Train loss: 3412551.0\n",
            "Iteration 4520 - Batch 5/903 - Train loss: 3489693.7083333335\n",
            "Iteration 4521 - Batch 6/903 - Train loss: 3521368.4285714286\n",
            "Iteration 4522 - Batch 7/903 - Train loss: 3418199.375\n",
            "Iteration 4523 - Batch 8/903 - Train loss: 3401265.1666666665\n",
            "Iteration 4524 - Batch 9/903 - Train loss: 3394036.4\n",
            "Iteration 4525 - Batch 10/903 - Train loss: 3304611.977272727\n",
            "Iteration 4526 - Batch 11/903 - Train loss: 3281018.3125\n",
            "Iteration 4527 - Batch 12/903 - Train loss: 3274410.5576923075\n",
            "Iteration 4528 - Batch 13/903 - Train loss: 3217999.0178571427\n",
            "Iteration 4529 - Batch 14/903 - Train loss: 3186464.0166666666\n",
            "Iteration 4530 - Batch 15/903 - Train loss: 3189869.3125\n",
            "Iteration 4531 - Batch 16/903 - Train loss: 3145319.8088235296\n",
            "Iteration 4532 - Batch 17/903 - Train loss: 3108010.236111111\n",
            "Iteration 4533 - Batch 18/903 - Train loss: 3093775.8289473685\n",
            "Iteration 4534 - Batch 19/903 - Train loss: 3050135.0875\n",
            "Iteration 4535 - Batch 20/903 - Train loss: 3016863.4285714286\n",
            "Iteration 4536 - Batch 21/903 - Train loss: 2988731.9545454546\n",
            "Iteration 4537 - Batch 22/903 - Train loss: 2950632.6086956523\n",
            "Iteration 4538 - Batch 23/903 - Train loss: 2926230.375\n",
            "Iteration 4539 - Batch 24/903 - Train loss: 2908424.14\n",
            "Iteration 4540 - Batch 25/903 - Train loss: 2875158.2884615385\n",
            "Iteration 4541 - Batch 26/903 - Train loss: 2850215.5555555555\n",
            "Iteration 4542 - Batch 27/903 - Train loss: 2831570.7053571427\n",
            "Iteration 4543 - Batch 28/903 - Train loss: 2807854.5172413792\n",
            "Iteration 4544 - Batch 29/903 - Train loss: 2783645.8333333335\n",
            "Iteration 4545 - Batch 30/903 - Train loss: 2765583.6774193547\n",
            "Iteration 4546 - Batch 31/903 - Train loss: 2747363.0625\n",
            "Iteration 4547 - Batch 32/903 - Train loss: 2737297.106060606\n",
            "Iteration 4548 - Batch 33/903 - Train loss: 2726535.544117647\n",
            "Iteration 4549 - Batch 34/903 - Train loss: 2712528.0857142857\n",
            "Iteration 4550 - Batch 35/903 - Train loss: 2695795.111111111\n",
            "Iteration 4551 - Batch 36/903 - Train loss: 2685152.7094594594\n",
            "Iteration 4552 - Batch 37/903 - Train loss: 2671416.7171052634\n",
            "Iteration 4553 - Batch 38/903 - Train loss: 2657080.4487179485\n",
            "Iteration 4554 - Batch 39/903 - Train loss: 2647688.43125\n",
            "Iteration 4555 - Batch 40/903 - Train loss: 2642208.31097561\n",
            "Iteration 4556 - Batch 41/903 - Train loss: 2629450.273809524\n",
            "Iteration 4557 - Batch 42/903 - Train loss: 2617676.761627907\n",
            "Iteration 4558 - Batch 43/903 - Train loss: 2611998.039772727\n",
            "Iteration 4559 - Batch 44/903 - Train loss: 2598448.152777778\n",
            "Iteration 4560 - Batch 45/903 - Train loss: 2590402.622282609\n",
            "Iteration 4561 - Batch 46/903 - Train loss: 2582741.369680851\n",
            "Iteration 4562 - Batch 47/903 - Train loss: 2571515.2890625\n",
            "Iteration 4563 - Batch 48/903 - Train loss: 2565589.349489796\n",
            "Iteration 4564 - Batch 49/903 - Train loss: 2558334.3875\n",
            "Iteration 4565 - Batch 50/903 - Train loss: 2551439.3210784313\n",
            "Iteration 4566 - Batch 51/903 - Train loss: 2545786.449519231\n",
            "Iteration 4567 - Batch 52/903 - Train loss: 2538706.7806603773\n",
            "Iteration 4568 - Batch 53/903 - Train loss: 2531895.076388889\n",
            "Iteration 4569 - Batch 54/903 - Train loss: 2525726.0613636365\n",
            "Iteration 4570 - Batch 55/903 - Train loss: 2519979.895089286\n",
            "Iteration 4571 - Batch 56/903 - Train loss: 2509912.5635964912\n",
            "Iteration 4572 - Batch 57/903 - Train loss: 2502274.495689655\n",
            "Iteration 4573 - Batch 58/903 - Train loss: 2499659.940677966\n",
            "Iteration 4574 - Batch 59/903 - Train loss: 2492060.1333333333\n",
            "Iteration 4575 - Batch 60/903 - Train loss: 2487632.5040983604\n",
            "Iteration 4576 - Batch 61/903 - Train loss: 2484622.0766129033\n",
            "Iteration 4577 - Batch 62/903 - Train loss: 2477570.9464285714\n",
            "Iteration 4578 - Batch 63/903 - Train loss: 2471495.5390625\n",
            "Iteration 4579 - Batch 64/903 - Train loss: 2471703.953846154\n",
            "Iteration 4580 - Batch 65/903 - Train loss: 2469242.484848485\n",
            "Iteration 4581 - Batch 66/903 - Train loss: 2463917.0597014925\n",
            "Iteration 4582 - Batch 67/903 - Train loss: 2461332.2536764704\n",
            "Iteration 4583 - Batch 68/903 - Train loss: 2459889.6231884058\n",
            "Iteration 4584 - Batch 69/903 - Train loss: 2458196.507142857\n",
            "Iteration 4585 - Batch 70/903 - Train loss: 2456738.045774648\n",
            "Iteration 4586 - Batch 71/903 - Train loss: 2449505.9305555555\n",
            "Iteration 4587 - Batch 72/903 - Train loss: 2445326.9897260275\n",
            "Iteration 4588 - Batch 73/903 - Train loss: 2442936.631756757\n",
            "Iteration 4589 - Batch 74/903 - Train loss: 2438553.6933333334\n",
            "Iteration 4590 - Batch 75/903 - Train loss: 2438417.6809210526\n",
            "Iteration 4591 - Batch 76/903 - Train loss: 2435667.772727273\n",
            "Iteration 4592 - Batch 77/903 - Train loss: 2435001.2564102565\n",
            "Iteration 4593 - Batch 78/903 - Train loss: 2433425.731012658\n",
            "Iteration 4594 - Batch 79/903 - Train loss: 2430692.8875\n",
            "Iteration 4595 - Batch 80/903 - Train loss: 2425947.220679012\n",
            "Iteration 4596 - Batch 81/903 - Train loss: 2423687.3399390243\n",
            "Iteration 4597 - Batch 82/903 - Train loss: 2423061.3057228914\n",
            "Iteration 4598 - Batch 83/903 - Train loss: 2421382.933035714\n",
            "Iteration 4599 - Batch 84/903 - Train loss: 2419282.863235294\n",
            "Iteration 4600 - Batch 85/903 - Train loss: 2417552.6264534886\n",
            "Iteration 4601 - Batch 86/903 - Train loss: 2414908.932471264\n",
            "Iteration 4602 - Batch 87/903 - Train loss: 2412801.132102273\n",
            "Iteration 4603 - Batch 88/903 - Train loss: 2410020.425561798\n",
            "Iteration 4604 - Batch 89/903 - Train loss: 2407180.5680555557\n",
            "Iteration 4605 - Batch 90/903 - Train loss: 2406590.193681319\n",
            "Iteration 4606 - Batch 91/903 - Train loss: 2404367.846467391\n",
            "Iteration 4607 - Batch 92/903 - Train loss: 2402234.380376344\n",
            "Iteration 4608 - Batch 93/903 - Train loss: 2399485.3656914895\n",
            "Iteration 4609 - Batch 94/903 - Train loss: 2398997.5381578947\n",
            "Iteration 4610 - Batch 95/903 - Train loss: 2397635.28515625\n",
            "Iteration 4611 - Batch 96/903 - Train loss: 2394824.442010309\n",
            "Iteration 4612 - Batch 97/903 - Train loss: 2394021.0191326533\n",
            "Iteration 4613 - Batch 98/903 - Train loss: 2391304.347222222\n",
            "Iteration 4614 - Batch 99/903 - Train loss: 2391115.89625\n",
            "Iteration 4615 - Batch 100/903 - Train loss: 2389167.503712871\n",
            "Iteration 4616 - Batch 101/903 - Train loss: 2387749.094362745\n",
            "Iteration 4617 - Batch 102/903 - Train loss: 2386482.4041262134\n",
            "Iteration 4618 - Batch 103/903 - Train loss: 2388413.5564903845\n",
            "Iteration 4619 - Batch 104/903 - Train loss: 2391627.355952381\n",
            "Iteration 4620 - Batch 105/903 - Train loss: 2390850.3478773586\n",
            "Iteration 4621 - Batch 106/903 - Train loss: 2389203.8142523365\n",
            "Iteration 4622 - Batch 107/903 - Train loss: 2386504.1469907407\n",
            "Iteration 4623 - Batch 108/903 - Train loss: 2388164.143348624\n",
            "Iteration 4624 - Batch 109/903 - Train loss: 2388709.503409091\n",
            "Iteration 4625 - Batch 110/903 - Train loss: 2389447.6475225226\n",
            "Iteration 4626 - Batch 111/903 - Train loss: 2388119.3069196427\n",
            "Iteration 4627 - Batch 112/903 - Train loss: 2385643.257743363\n",
            "Iteration 4628 - Batch 113/903 - Train loss: 2384513.2576754387\n",
            "Iteration 4629 - Batch 114/903 - Train loss: 2383652.5684782607\n",
            "Iteration 4630 - Batch 115/903 - Train loss: 2382616.425646552\n",
            "Iteration 4631 - Batch 116/903 - Train loss: 2383069.4326923075\n",
            "Iteration 4632 - Batch 117/903 - Train loss: 2382754.221398305\n",
            "Iteration 4633 - Batch 118/903 - Train loss: 2383270.2594537814\n",
            "Iteration 4634 - Batch 119/903 - Train loss: 2385281.2885416667\n",
            "Iteration 4635 - Batch 120/903 - Train loss: 2384102.120867769\n",
            "Iteration 4636 - Batch 121/903 - Train loss: 2382405.3084016396\n",
            "Iteration 4637 - Batch 122/903 - Train loss: 2381893.43597561\n",
            "Iteration 4638 - Batch 123/903 - Train loss: 2380207.3054435486\n",
            "Iteration 4639 - Batch 124/903 - Train loss: 2379199.963\n",
            "Iteration 4640 - Batch 125/903 - Train loss: 2377938.848214286\n",
            "Iteration 4641 - Batch 126/903 - Train loss: 2378256.441929134\n",
            "Iteration 4642 - Batch 127/903 - Train loss: 2378323.0654296875\n",
            "Iteration 4643 - Batch 128/903 - Train loss: 2378386.770348837\n",
            "Iteration 4644 - Batch 129/903 - Train loss: 2375655.8259615386\n",
            "Iteration 4645 - Batch 130/903 - Train loss: 2374555.453244275\n",
            "Iteration 4646 - Batch 131/903 - Train loss: 2373611.9422348486\n",
            "Iteration 4647 - Batch 132/903 - Train loss: 2372548.5460526315\n",
            "Iteration 4648 - Batch 133/903 - Train loss: 2371358.0289179105\n",
            "Iteration 4649 - Batch 134/903 - Train loss: 2370938.263888889\n",
            "Iteration 4650 - Batch 135/903 - Train loss: 2369882.5064338236\n",
            "Iteration 4651 - Batch 136/903 - Train loss: 2368450.86770073\n",
            "Iteration 4652 - Batch 137/903 - Train loss: 2366279.417572464\n",
            "Iteration 4653 - Batch 138/903 - Train loss: 2364308.409172662\n",
            "Iteration 4654 - Batch 139/903 - Train loss: 2362882.9223214285\n",
            "Iteration 4655 - Batch 140/903 - Train loss: 2361048.499113475\n",
            "Iteration 4656 - Batch 141/903 - Train loss: 2359777.720950704\n",
            "Iteration 4657 - Batch 142/903 - Train loss: 2359182.499125874\n",
            "Iteration 4658 - Batch 143/903 - Train loss: 2358111.962673611\n",
            "Iteration 4659 - Batch 144/903 - Train loss: 2355708.0077586207\n",
            "Iteration 4660 - Batch 145/903 - Train loss: 2354085.774828767\n",
            "Iteration 4661 - Batch 146/903 - Train loss: 2353921.5773809524\n",
            "Iteration 4662 - Batch 147/903 - Train loss: 2353092.154560811\n",
            "Iteration 4663 - Batch 148/903 - Train loss: 2350442.1837248323\n",
            "Iteration 4664 - Batch 149/903 - Train loss: 2350772.9025\n",
            "Iteration 4665 - Batch 150/903 - Train loss: 2350705.161423841\n",
            "Iteration 4666 - Batch 151/903 - Train loss: 2352710.7771381577\n",
            "Iteration 4667 - Batch 152/903 - Train loss: 2352894.1919934643\n",
            "Iteration 4668 - Batch 153/903 - Train loss: 2351121.864448052\n",
            "Iteration 4669 - Batch 154/903 - Train loss: 2351047.976612903\n",
            "Iteration 4670 - Batch 155/903 - Train loss: 2352393.21875\n",
            "Iteration 4671 - Batch 156/903 - Train loss: 2352467.661624204\n",
            "Iteration 4672 - Batch 157/903 - Train loss: 2352112.022943038\n",
            "Iteration 4673 - Batch 158/903 - Train loss: 2352523.1831761007\n",
            "Iteration 4674 - Batch 159/903 - Train loss: 2352094.55078125\n",
            "Iteration 4675 - Batch 160/903 - Train loss: 2352035.1156832296\n",
            "Iteration 4676 - Batch 161/903 - Train loss: 2351381.8356481483\n",
            "Iteration 4677 - Batch 162/903 - Train loss: 2350900.686349693\n",
            "Iteration 4678 - Batch 163/903 - Train loss: 2351238.7583841463\n",
            "Iteration 4679 - Batch 164/903 - Train loss: 2351248.999242424\n",
            "Iteration 4680 - Batch 165/903 - Train loss: 2351279.3426204817\n",
            "Iteration 4681 - Batch 166/903 - Train loss: 2349680.2163173654\n",
            "Iteration 4682 - Batch 167/903 - Train loss: 2349079.402529762\n",
            "Iteration 4683 - Batch 168/903 - Train loss: 2348924.176775148\n",
            "Iteration 4684 - Batch 169/903 - Train loss: 2349298.4227941176\n",
            "Iteration 4685 - Batch 170/903 - Train loss: 2348828.408625731\n",
            "Iteration 4686 - Batch 171/903 - Train loss: 2349079.042877907\n",
            "Iteration 4687 - Batch 172/903 - Train loss: 2349018.325867052\n",
            "Iteration 4688 - Batch 173/903 - Train loss: 2348885.7176724137\n",
            "Iteration 4689 - Batch 174/903 - Train loss: 2348101.972142857\n",
            "Iteration 4690 - Batch 175/903 - Train loss: 2348520.1384943184\n",
            "Iteration 4691 - Batch 176/903 - Train loss: 2348665.554378531\n",
            "Iteration 4692 - Batch 177/903 - Train loss: 2348649.6973314607\n",
            "Iteration 4693 - Batch 178/903 - Train loss: 2347536.6208100556\n",
            "Iteration 4694 - Batch 179/903 - Train loss: 2348000.7118055555\n",
            "Iteration 4695 - Batch 180/903 - Train loss: 2348162.3625690606\n",
            "Iteration 4696 - Batch 181/903 - Train loss: 2349562.5350274723\n",
            "Iteration 4697 - Batch 182/903 - Train loss: 2349025.043032787\n",
            "Iteration 4698 - Batch 183/903 - Train loss: 2349142.2533967393\n",
            "Iteration 4699 - Batch 184/903 - Train loss: 2348807.343918919\n",
            "Iteration 4700 - Batch 185/903 - Train loss: 2348877.249327957\n",
            "Iteration 4701 - Batch 186/903 - Train loss: 2348925.9338235296\n",
            "Iteration 4702 - Batch 187/903 - Train loss: 2348978.004654255\n",
            "Iteration 4703 - Batch 188/903 - Train loss: 2348512.35515873\n",
            "Iteration 4704 - Batch 189/903 - Train loss: 2347913.7243421054\n",
            "Iteration 4705 - Batch 190/903 - Train loss: 2347109.2061518324\n",
            "Iteration 4706 - Batch 191/903 - Train loss: 2346187.2076822915\n",
            "Iteration 4707 - Batch 192/903 - Train loss: 2345677.623704663\n",
            "Iteration 4708 - Batch 193/903 - Train loss: 2346481.120489691\n",
            "Iteration 4709 - Batch 194/903 - Train loss: 2345439.691666667\n",
            "Iteration 4710 - Batch 195/903 - Train loss: 2343564.25127551\n",
            "Iteration 4711 - Batch 196/903 - Train loss: 2342445.255076142\n",
            "Iteration 4712 - Batch 197/903 - Train loss: 2342245.26010101\n",
            "Iteration 4713 - Batch 198/903 - Train loss: 2341987.449748744\n",
            "Iteration 4714 - Batch 199/903 - Train loss: 2341803.70125\n",
            "Iteration 4715 - Batch 200/903 - Train loss: 2340839.8768656715\n",
            "Iteration 4716 - Batch 201/903 - Train loss: 2339797.9777227724\n",
            "Iteration 4717 - Batch 202/903 - Train loss: 2338631.9544334975\n",
            "Iteration 4718 - Batch 203/903 - Train loss: 2338571.4901960786\n",
            "Iteration 4719 - Batch 204/903 - Train loss: 2337768.930487805\n",
            "Iteration 4720 - Batch 205/903 - Train loss: 2336690.8531553396\n",
            "Iteration 4721 - Batch 206/903 - Train loss: 2335553.716183575\n",
            "Iteration 4722 - Batch 207/903 - Train loss: 2335072.2860576925\n",
            "Iteration 4723 - Batch 208/903 - Train loss: 2334721.484449761\n",
            "Iteration 4724 - Batch 209/903 - Train loss: 2332778.6380952382\n",
            "Iteration 4725 - Batch 210/903 - Train loss: 2331408.585900474\n",
            "Iteration 4726 - Batch 211/903 - Train loss: 2330699.9557783017\n",
            "Iteration 4727 - Batch 212/903 - Train loss: 2331044.578051643\n",
            "Iteration 4728 - Batch 213/903 - Train loss: 2331009.0683411215\n",
            "Iteration 4729 - Batch 214/903 - Train loss: 2328871.1104651163\n",
            "Iteration 4730 - Batch 215/903 - Train loss: 2328742.431712963\n",
            "Iteration 4731 - Batch 216/903 - Train loss: 2327718.0668202764\n",
            "Iteration 4732 - Batch 217/903 - Train loss: 2327357.591743119\n",
            "Iteration 4733 - Batch 218/903 - Train loss: 2326871.154109589\n",
            "Iteration 4734 - Batch 219/903 - Train loss: 2326088.4272727272\n",
            "Iteration 4735 - Batch 220/903 - Train loss: 2326245.9321266967\n",
            "Iteration 4736 - Batch 221/903 - Train loss: 2324847.1390765766\n",
            "Iteration 4737 - Batch 222/903 - Train loss: 2324644.994955157\n",
            "Iteration 4738 - Batch 223/903 - Train loss: 2324074.3331473214\n",
            "Iteration 4739 - Batch 224/903 - Train loss: 2324491.5516666668\n",
            "Iteration 4740 - Batch 225/903 - Train loss: 2324748.9928097343\n",
            "Iteration 4741 - Batch 226/903 - Train loss: 2324429.314427313\n",
            "Iteration 4742 - Batch 227/903 - Train loss: 2324303.7033991227\n",
            "Iteration 4743 - Batch 228/903 - Train loss: 2323859.643558952\n",
            "Iteration 4744 - Batch 229/903 - Train loss: 2322844.030978261\n",
            "Iteration 4745 - Batch 230/903 - Train loss: 2321876.3425324676\n",
            "Iteration 4746 - Batch 231/903 - Train loss: 2320709.2510775863\n",
            "Iteration 4747 - Batch 232/903 - Train loss: 2320493.35944206\n",
            "Iteration 4748 - Batch 233/903 - Train loss: 2321118.2542735045\n",
            "Iteration 4749 - Batch 234/903 - Train loss: 2321561.7574468087\n",
            "Iteration 4750 - Batch 235/903 - Train loss: 2322561.532838983\n",
            "Iteration 4751 - Batch 236/903 - Train loss: 2321816.6856540083\n",
            "Iteration 4752 - Batch 237/903 - Train loss: 2321927.2006302522\n",
            "Iteration 4753 - Batch 238/903 - Train loss: 2321638.8995815897\n",
            "Iteration 4754 - Batch 239/903 - Train loss: 2323759.955208333\n",
            "Iteration 4755 - Batch 240/903 - Train loss: 2323795.5871369294\n",
            "Iteration 4756 - Batch 241/903 - Train loss: 2323607.1353305783\n",
            "Iteration 4757 - Batch 242/903 - Train loss: 2323848.8868312757\n",
            "Iteration 4758 - Batch 243/903 - Train loss: 2324298.093237705\n",
            "Iteration 4759 - Batch 244/903 - Train loss: 2324772.089795918\n",
            "Iteration 4760 - Batch 245/903 - Train loss: 2324963.4278455283\n",
            "Iteration 4761 - Batch 246/903 - Train loss: 2324141.136639676\n",
            "Iteration 4762 - Batch 247/903 - Train loss: 2324832.6723790322\n",
            "Iteration 4763 - Batch 248/903 - Train loss: 2326961.013052209\n",
            "Iteration 4764 - Batch 249/903 - Train loss: 2328846.963\n",
            "Iteration 4765 - Batch 250/903 - Train loss: 2328532.55876494\n",
            "Iteration 4766 - Batch 251/903 - Train loss: 2328523.0962301586\n",
            "Iteration 4767 - Batch 252/903 - Train loss: 2329117.9407114624\n",
            "Iteration 4768 - Batch 253/903 - Train loss: 2329885.380905512\n",
            "Iteration 4769 - Batch 254/903 - Train loss: 2330268.6539215688\n",
            "Iteration 4770 - Batch 255/903 - Train loss: 2330111.423828125\n",
            "Iteration 4771 - Batch 256/903 - Train loss: 2330469.8249027235\n",
            "Iteration 4772 - Batch 257/903 - Train loss: 2330322.020348837\n",
            "Iteration 4773 - Batch 258/903 - Train loss: 2329790.743243243\n",
            "Iteration 4774 - Batch 259/903 - Train loss: 2329521.516346154\n",
            "Iteration 4775 - Batch 260/903 - Train loss: 2329797.8831417626\n",
            "Iteration 4776 - Batch 261/903 - Train loss: 2331005.9112595418\n",
            "Iteration 4777 - Batch 262/903 - Train loss: 2331419.923003802\n",
            "Iteration 4778 - Batch 263/903 - Train loss: 2331013.4005681816\n",
            "Iteration 4779 - Batch 264/903 - Train loss: 2330833.4386792453\n",
            "Iteration 4780 - Batch 265/903 - Train loss: 2330898.10056391\n",
            "Iteration 4781 - Batch 266/903 - Train loss: 2331229.5852059927\n",
            "Iteration 4782 - Batch 267/903 - Train loss: 2331721.682835821\n",
            "Iteration 4783 - Batch 268/903 - Train loss: 2331759.942379182\n",
            "Iteration 4784 - Batch 269/903 - Train loss: 2331539.5759259257\n",
            "Iteration 4785 - Batch 270/903 - Train loss: 2331839.2961254613\n",
            "Iteration 4786 - Batch 271/903 - Train loss: 2331704.7536764704\n",
            "Iteration 4787 - Batch 272/903 - Train loss: 2331657.652014652\n",
            "Iteration 4788 - Batch 273/903 - Train loss: 2332047.848540146\n",
            "Iteration 4789 - Batch 274/903 - Train loss: 2331744.958181818\n",
            "Iteration 4790 - Batch 275/903 - Train loss: 2332525.571557971\n",
            "Iteration 4791 - Batch 276/903 - Train loss: 2332474.9214801444\n",
            "Iteration 4792 - Batch 277/903 - Train loss: 2332446.0242805756\n",
            "Iteration 4793 - Batch 278/903 - Train loss: 2331893.4919354836\n",
            "Iteration 4794 - Batch 279/903 - Train loss: 2332265.3348214286\n",
            "Iteration 4795 - Batch 280/903 - Train loss: 2331845.259786477\n",
            "Iteration 4796 - Batch 281/903 - Train loss: 2332157.9583333335\n",
            "Iteration 4797 - Batch 282/903 - Train loss: 2331935.9355123676\n",
            "Iteration 4798 - Batch 283/903 - Train loss: 2331855.9639084507\n",
            "Iteration 4799 - Batch 284/903 - Train loss: 2332173.5307017546\n",
            "Iteration 4800 - Batch 285/903 - Train loss: 2333080.819055944\n",
            "Iteration 4801 - Batch 286/903 - Train loss: 2333726.99912892\n",
            "Iteration 4802 - Batch 287/903 - Train loss: 2333755.0147569445\n",
            "Iteration 4803 - Batch 288/903 - Train loss: 2333115.528546713\n",
            "Iteration 4804 - Batch 289/903 - Train loss: 2333361.7198275863\n",
            "Iteration 4805 - Batch 290/903 - Train loss: 2333346.5850515463\n",
            "Iteration 4806 - Batch 291/903 - Train loss: 2332887.4503424657\n",
            "Iteration 4807 - Batch 292/903 - Train loss: 2332772.353242321\n",
            "Iteration 4808 - Batch 293/903 - Train loss: 2332916.393707483\n",
            "Iteration 4809 - Batch 294/903 - Train loss: 2333620.8254237287\n",
            "Iteration 4810 - Batch 295/903 - Train loss: 2333199.0050675673\n",
            "Iteration 4811 - Batch 296/903 - Train loss: 2332135.523989899\n",
            "Iteration 4812 - Batch 297/903 - Train loss: 2332222.7227348993\n",
            "Iteration 4813 - Batch 298/903 - Train loss: 2333129.6977424747\n",
            "Iteration 4814 - Batch 299/903 - Train loss: 2333670.044583333\n",
            "Iteration 4815 - Batch 300/903 - Train loss: 2333137.5261627906\n",
            "Iteration 4816 - Batch 301/903 - Train loss: 2333000.114652318\n",
            "Iteration 4817 - Batch 302/903 - Train loss: 2332835.408828383\n",
            "Iteration 4818 - Batch 303/903 - Train loss: 2333552.2685032897\n",
            "Iteration 4819 - Batch 304/903 - Train loss: 2335284.098770492\n",
            "Iteration 4820 - Batch 305/903 - Train loss: 2335551.0053104577\n",
            "Iteration 4821 - Batch 306/903 - Train loss: 2334819.332654723\n",
            "Iteration 4822 - Batch 307/903 - Train loss: 2335498.2723214286\n",
            "Iteration 4823 - Batch 308/903 - Train loss: 2337808.2172330096\n",
            "Iteration 4824 - Batch 309/903 - Train loss: 2339308.20766129\n",
            "Iteration 4825 - Batch 310/903 - Train loss: 2338916.1925241156\n",
            "Iteration 4826 - Batch 311/903 - Train loss: 2338766.0356570515\n",
            "Iteration 4827 - Batch 312/903 - Train loss: 2339761.8103035144\n",
            "Iteration 4828 - Batch 313/903 - Train loss: 2339802.9478503186\n",
            "Iteration 4829 - Batch 314/903 - Train loss: 2339895.529761905\n",
            "Iteration 4830 - Batch 315/903 - Train loss: 2340235.3975474685\n",
            "Iteration 4831 - Batch 316/903 - Train loss: 2340662.6242113565\n",
            "Iteration 4832 - Batch 317/903 - Train loss: 2340735.366745283\n",
            "Iteration 4833 - Batch 318/903 - Train loss: 2340492.60854232\n",
            "Iteration 4834 - Batch 319/903 - Train loss: 2341098.375390625\n",
            "Iteration 4835 - Batch 320/903 - Train loss: 2341791.406931464\n",
            "Iteration 4836 - Batch 321/903 - Train loss: 2341455.7651397516\n",
            "Iteration 4837 - Batch 322/903 - Train loss: 2341127.51122291\n",
            "Iteration 4838 - Batch 323/903 - Train loss: 2340660.1971450616\n",
            "Iteration 4839 - Batch 324/903 - Train loss: 2339341.955\n",
            "Iteration 4840 - Batch 325/903 - Train loss: 2338828.021088957\n",
            "Iteration 4841 - Batch 326/903 - Train loss: 2338718.1815749235\n",
            "Iteration 4842 - Batch 327/903 - Train loss: 2338607.895960366\n",
            "Iteration 4843 - Batch 328/903 - Train loss: 2337861.1804711246\n",
            "Iteration 4844 - Batch 329/903 - Train loss: 2336975.646969697\n",
            "Iteration 4845 - Batch 330/903 - Train loss: 2336528.245468278\n",
            "Iteration 4846 - Batch 331/903 - Train loss: 2336118.175451807\n",
            "Iteration 4847 - Batch 332/903 - Train loss: 2335664.575075075\n",
            "Iteration 4848 - Batch 333/903 - Train loss: 2334366.5546407187\n",
            "Iteration 4849 - Batch 334/903 - Train loss: 2334009.2373134326\n",
            "Iteration 4850 - Batch 335/903 - Train loss: 2333475.0625\n",
            "Iteration 4851 - Batch 336/903 - Train loss: 2332785.8397626113\n",
            "Iteration 4852 - Batch 337/903 - Train loss: 2332588.6693786983\n",
            "Iteration 4853 - Batch 338/903 - Train loss: 2331970.0833333335\n",
            "Iteration 4854 - Batch 339/903 - Train loss: 2331868.8022058825\n",
            "Iteration 4855 - Batch 340/903 - Train loss: 2331146.8493401757\n",
            "Iteration 4856 - Batch 341/903 - Train loss: 2331218.7701023393\n",
            "Iteration 4857 - Batch 342/903 - Train loss: 2331260.537536443\n",
            "Iteration 4858 - Batch 343/903 - Train loss: 2331310.503270349\n",
            "Iteration 4859 - Batch 344/903 - Train loss: 2330874.3264492755\n",
            "Iteration 4860 - Batch 345/903 - Train loss: 2330108.297687861\n",
            "Iteration 4861 - Batch 346/903 - Train loss: 2329615.917867435\n",
            "Iteration 4862 - Batch 347/903 - Train loss: 2328614.7363505745\n",
            "Iteration 4863 - Batch 348/903 - Train loss: 2328446.191977077\n",
            "Iteration 4864 - Batch 349/903 - Train loss: 2328480.982857143\n",
            "Iteration 4865 - Batch 350/903 - Train loss: 2327695.845797721\n",
            "Iteration 4866 - Batch 351/903 - Train loss: 2327819.037997159\n",
            "Iteration 4867 - Batch 352/903 - Train loss: 2327180.448654391\n",
            "Iteration 4868 - Batch 353/903 - Train loss: 2326865.1966807907\n",
            "Iteration 4869 - Batch 354/903 - Train loss: 2326600.0679577463\n",
            "Iteration 4870 - Batch 355/903 - Train loss: 2326327.268609551\n",
            "Iteration 4871 - Batch 356/903 - Train loss: 2325623.6362044816\n",
            "Iteration 4872 - Batch 357/903 - Train loss: 2325959.151885475\n",
            "Iteration 4873 - Batch 358/903 - Train loss: 2325723.603412256\n",
            "Iteration 4874 - Batch 359/903 - Train loss: 2325277.3739583334\n",
            "Iteration 4875 - Batch 360/903 - Train loss: 2324613.6658587256\n",
            "Iteration 4876 - Batch 361/903 - Train loss: 2324065.5583563535\n",
            "Iteration 4877 - Batch 362/903 - Train loss: 2323635.8088842975\n",
            "Iteration 4878 - Batch 363/903 - Train loss: 2322850.5209478023\n",
            "Iteration 4879 - Batch 364/903 - Train loss: 2322647.7688356163\n",
            "Iteration 4880 - Batch 365/903 - Train loss: 2322542.0782103827\n",
            "Iteration 4881 - Batch 366/903 - Train loss: 2322249.8641008176\n",
            "Iteration 4882 - Batch 367/903 - Train loss: 2323000.9921875\n",
            "Iteration 4883 - Batch 368/903 - Train loss: 2323256.1046747966\n",
            "Iteration 4884 - Batch 369/903 - Train loss: 2323119.213175676\n",
            "Iteration 4885 - Batch 370/903 - Train loss: 2322379.7638140162\n",
            "Iteration 4886 - Batch 371/903 - Train loss: 2322432.4445564514\n",
            "Iteration 4887 - Batch 372/903 - Train loss: 2322729.423257373\n",
            "Iteration 4888 - Batch 373/903 - Train loss: 2322867.3018048126\n",
            "Iteration 4889 - Batch 374/903 - Train loss: 2322697.039\n",
            "Iteration 4890 - Batch 375/903 - Train loss: 2322108.052194149\n",
            "Iteration 4891 - Batch 376/903 - Train loss: 2321873.87831565\n",
            "Iteration 4892 - Batch 377/903 - Train loss: 2321712.2318121693\n",
            "Iteration 4893 - Batch 378/903 - Train loss: 2320866.3885224275\n",
            "Iteration 4894 - Batch 379/903 - Train loss: 2320084.8575657895\n",
            "Iteration 4895 - Batch 380/903 - Train loss: 2319262.723753281\n",
            "Iteration 4896 - Batch 381/903 - Train loss: 2318484.6940445025\n",
            "Iteration 4897 - Batch 382/903 - Train loss: 2317603.5097911227\n",
            "Iteration 4898 - Batch 383/903 - Train loss: 2316997.2571614585\n",
            "Iteration 4899 - Batch 384/903 - Train loss: 2316821.7207792206\n",
            "Iteration 4900 - Batch 385/903 - Train loss: 2316481.9676165804\n",
            "Iteration 4901 - Batch 386/903 - Train loss: 2315717.2254521963\n",
            "Iteration 4902 - Batch 387/903 - Train loss: 2315205.4864690723\n",
            "Iteration 4903 - Batch 388/903 - Train loss: 2314799.140102828\n",
            "Iteration 4904 - Batch 389/903 - Train loss: 2314330.3397435895\n",
            "Iteration 4905 - Batch 390/903 - Train loss: 2314443.480818414\n",
            "Iteration 4906 - Batch 391/903 - Train loss: 2314015.3616071427\n",
            "Iteration 4907 - Batch 392/903 - Train loss: 2314084.8454198474\n",
            "Iteration 4908 - Batch 393/903 - Train loss: 2313933.9777918784\n",
            "Iteration 4909 - Batch 394/903 - Train loss: 2313613.9120253166\n",
            "Iteration 4910 - Batch 395/903 - Train loss: 2313147.1085858587\n",
            "Iteration 4911 - Batch 396/903 - Train loss: 2312889.7852644837\n",
            "Iteration 4912 - Batch 397/903 - Train loss: 2312628.5087939696\n",
            "Iteration 4913 - Batch 398/903 - Train loss: 2312512.8377192984\n",
            "Iteration 4914 - Batch 399/903 - Train loss: 2312367.56625\n",
            "Iteration 4915 - Batch 400/903 - Train loss: 2312373.22319202\n",
            "Iteration 4916 - Batch 401/903 - Train loss: 2312183.1666666665\n",
            "Iteration 4917 - Batch 402/903 - Train loss: 2312060.928039702\n",
            "Iteration 4918 - Batch 403/903 - Train loss: 2311438.7271039602\n",
            "Iteration 4919 - Batch 404/903 - Train loss: 2310936.183333333\n",
            "Iteration 4920 - Batch 405/903 - Train loss: 2310862.3362068967\n",
            "Iteration 4921 - Batch 406/903 - Train loss: 2310386.020884521\n",
            "Iteration 4922 - Batch 407/903 - Train loss: 2309797.699754902\n",
            "Iteration 4923 - Batch 408/903 - Train loss: 2309382.8661369192\n",
            "Iteration 4924 - Batch 409/903 - Train loss: 2309429.605487805\n",
            "Iteration 4925 - Batch 410/903 - Train loss: 2308914.812652068\n",
            "Iteration 4926 - Batch 411/903 - Train loss: 2309012.2348300973\n",
            "Iteration 4927 - Batch 412/903 - Train loss: 2308635.015738499\n",
            "Iteration 4928 - Batch 413/903 - Train loss: 2308469.611111111\n",
            "Iteration 4929 - Batch 414/903 - Train loss: 2308322.3240963854\n",
            "Iteration 4930 - Batch 415/903 - Train loss: 2307804.190204327\n",
            "Iteration 4931 - Batch 416/903 - Train loss: 2307717.669964029\n",
            "Iteration 4932 - Batch 417/903 - Train loss: 2307332.7317583733\n",
            "Iteration 4933 - Batch 418/903 - Train loss: 2307590.9048329354\n",
            "Iteration 4934 - Batch 419/903 - Train loss: 2307580.5544642857\n",
            "Iteration 4935 - Batch 420/903 - Train loss: 2307502.9676365796\n",
            "Iteration 4936 - Batch 421/903 - Train loss: 2307460.214751185\n",
            "Iteration 4937 - Batch 422/903 - Train loss: 2307418.1439125296\n",
            "Iteration 4938 - Batch 423/903 - Train loss: 2307946.4607900945\n",
            "Iteration 4939 - Batch 424/903 - Train loss: 2308732.060294118\n",
            "Iteration 4940 - Batch 425/903 - Train loss: 2309665.4521713615\n",
            "Iteration 4941 - Batch 426/903 - Train loss: 2310133.00204918\n",
            "Iteration 4942 - Batch 427/903 - Train loss: 2310401.9389602803\n",
            "Iteration 4943 - Batch 428/903 - Train loss: 2310313.958916084\n",
            "Iteration 4944 - Batch 429/903 - Train loss: 2310224.5781976744\n",
            "Iteration 4945 - Batch 430/903 - Train loss: 2310634.6215197216\n",
            "Iteration 4946 - Batch 431/903 - Train loss: 2310541.1999421297\n",
            "Iteration 4947 - Batch 432/903 - Train loss: 2310730.9864318706\n",
            "Iteration 4948 - Batch 433/903 - Train loss: 2311608.4058179725\n",
            "Iteration 4949 - Batch 434/903 - Train loss: 2311466.8727011494\n",
            "Iteration 4950 - Batch 435/903 - Train loss: 2310842.0819954127\n",
            "Iteration 4951 - Batch 436/903 - Train loss: 2311567.3295194507\n",
            "Iteration 4952 - Batch 437/903 - Train loss: 2311728.267694064\n",
            "Iteration 4953 - Batch 438/903 - Train loss: 2311883.9322323464\n",
            "Iteration 4954 - Batch 439/903 - Train loss: 2312488.4005681816\n",
            "Iteration 4955 - Batch 440/903 - Train loss: 2312493.5385487527\n",
            "Iteration 4956 - Batch 441/903 - Train loss: 2312148.6917420817\n",
            "Iteration 4957 - Batch 442/903 - Train loss: 2311648.5220090295\n",
            "Iteration 4958 - Batch 443/903 - Train loss: 2311264.363738739\n",
            "Iteration 4959 - Batch 444/903 - Train loss: 2310986.930337079\n",
            "Iteration 4960 - Batch 445/903 - Train loss: 2310846.764013453\n",
            "Iteration 4961 - Batch 446/903 - Train loss: 2310781.4300894854\n",
            "Iteration 4962 - Batch 447/903 - Train loss: 2310372.6785714286\n",
            "Iteration 4963 - Batch 448/903 - Train loss: 2309833.614699332\n",
            "Iteration 4964 - Batch 449/903 - Train loss: 2309954.8905555555\n",
            "Iteration 4965 - Batch 450/903 - Train loss: 2309804.7494456763\n",
            "Iteration 4966 - Batch 451/903 - Train loss: 2309867.3136061947\n",
            "Iteration 4967 - Batch 452/903 - Train loss: 2309725.995584989\n",
            "Iteration 4968 - Batch 453/903 - Train loss: 2309877.883259912\n",
            "Iteration 4969 - Batch 454/903 - Train loss: 2310421.0428571426\n",
            "Iteration 4970 - Batch 455/903 - Train loss: 2310673.0701754387\n",
            "Iteration 4971 - Batch 456/903 - Train loss: 2310643.818380744\n",
            "Iteration 4972 - Batch 457/903 - Train loss: 2310499.71069869\n",
            "Iteration 4973 - Batch 458/903 - Train loss: 2310968.9635076253\n",
            "Iteration 4974 - Batch 459/903 - Train loss: 2311739.1646739133\n",
            "Iteration 4975 - Batch 460/903 - Train loss: 2312650.204989154\n",
            "Iteration 4976 - Batch 461/903 - Train loss: 2312461.545995671\n",
            "Iteration 4977 - Batch 462/903 - Train loss: 2311835.360961123\n",
            "Iteration 4978 - Batch 463/903 - Train loss: 2311827.966325431\n",
            "Iteration 4979 - Batch 464/903 - Train loss: 2311684.0524193547\n",
            "Iteration 4980 - Batch 465/903 - Train loss: 2311421.867757511\n",
            "Iteration 4981 - Batch 466/903 - Train loss: 2311405.455299786\n",
            "Iteration 4982 - Batch 467/903 - Train loss: 2311060.2913995725\n",
            "Iteration 4983 - Batch 468/903 - Train loss: 2310646.1441897657\n",
            "Iteration 4984 - Batch 469/903 - Train loss: 2310284.2539893617\n",
            "Iteration 4985 - Batch 470/903 - Train loss: 2310090.6764861997\n",
            "Iteration 4986 - Batch 471/903 - Train loss: 2310270.0209216103\n",
            "Iteration 4987 - Batch 472/903 - Train loss: 2310541.1397991544\n",
            "Iteration 4988 - Batch 473/903 - Train loss: 2310268.683280591\n",
            "Iteration 4989 - Batch 474/903 - Train loss: 2309995.3939473685\n",
            "Iteration 4990 - Batch 475/903 - Train loss: 2309910.308560924\n",
            "Iteration 4991 - Batch 476/903 - Train loss: 2310063.377096436\n",
            "Iteration 4992 - Batch 477/903 - Train loss: 2310116.4129184103\n",
            "Iteration 4993 - Batch 478/903 - Train loss: 2310859.0237473906\n",
            "Iteration 4994 - Batch 479/903 - Train loss: 2310891.65859375\n",
            "Iteration 4995 - Batch 480/903 - Train loss: 2311260.7045218297\n",
            "Iteration 4996 - Batch 481/903 - Train loss: 2311400.051607884\n",
            "Iteration 4997 - Batch 482/903 - Train loss: 2311987.9086438925\n",
            "Iteration 4998 - Batch 483/903 - Train loss: 2312199.747159091\n",
            "Iteration 4999 - Batch 484/903 - Train loss: 2311959.743041237\n",
            "Iteration 5000 - Batch 485/903 - Train loss: 2312342.088734568\n",
            "Iteration 5001 - Batch 486/903 - Train loss: 2312511.0521047227\n",
            "Iteration 5002 - Batch 487/903 - Train loss: 2312261.6308913934\n",
            "Iteration 5003 - Batch 488/903 - Train loss: 2312751.4092535786\n",
            "Iteration 5004 - Batch 489/903 - Train loss: 2313042.296173469\n",
            "Iteration 5005 - Batch 490/903 - Train loss: 2312847.690173116\n",
            "Iteration 5006 - Batch 491/903 - Train loss: 2312894.8788109757\n",
            "Iteration 5007 - Batch 492/903 - Train loss: 2312707.400862069\n",
            "Iteration 5008 - Batch 493/903 - Train loss: 2312900.262904858\n",
            "Iteration 5009 - Batch 494/903 - Train loss: 2313051.322979798\n",
            "Iteration 5010 - Batch 495/903 - Train loss: 2312744.8036794355\n",
            "Iteration 5011 - Batch 496/903 - Train loss: 2312425.96805835\n",
            "Iteration 5012 - Batch 497/903 - Train loss: 2312069.109688755\n",
            "Iteration 5013 - Batch 498/903 - Train loss: 2311908.748246493\n",
            "Iteration 5014 - Batch 499/903 - Train loss: 2311246.312\n",
            "Iteration 5015 - Batch 500/903 - Train loss: 2311449.632235529\n",
            "Iteration 5016 - Batch 501/903 - Train loss: 2311393.533864542\n",
            "Iteration 5017 - Batch 502/903 - Train loss: 2311052.819085487\n",
            "Iteration 5018 - Batch 503/903 - Train loss: 2310814.105654762\n",
            "Iteration 5019 - Batch 504/903 - Train loss: 2310900.955940594\n",
            "Iteration 5020 - Batch 505/903 - Train loss: 2310523.2302371543\n",
            "Iteration 5021 - Batch 506/903 - Train loss: 2310287.6612426033\n",
            "Iteration 5022 - Batch 507/903 - Train loss: 2310151.1983267716\n",
            "Iteration 5023 - Batch 508/903 - Train loss: 2310254.988212181\n",
            "Iteration 5024 - Batch 509/903 - Train loss: 2309493.2078431374\n",
            "Iteration 5025 - Batch 510/903 - Train loss: 2309100.086105675\n",
            "Iteration 5026 - Batch 511/903 - Train loss: 2308795.642578125\n",
            "Iteration 5027 - Batch 512/903 - Train loss: 2308738.5911306045\n",
            "Iteration 5028 - Batch 513/903 - Train loss: 2308314.1057879375\n",
            "Iteration 5029 - Batch 514/903 - Train loss: 2308163.9779126216\n",
            "Iteration 5030 - Batch 515/903 - Train loss: 2308014.992005814\n",
            "Iteration 5031 - Batch 516/903 - Train loss: 2307614.700918762\n",
            "Iteration 5032 - Batch 517/903 - Train loss: 2307257.5359555986\n",
            "Iteration 5033 - Batch 518/903 - Train loss: 2307041.848025048\n",
            "Iteration 5034 - Batch 519/903 - Train loss: 2306283.355769231\n",
            "Iteration 5035 - Batch 520/903 - Train loss: 2306200.865642994\n",
            "Iteration 5036 - Batch 521/903 - Train loss: 2306119.530651341\n",
            "Iteration 5037 - Batch 522/903 - Train loss: 2305971.930210325\n",
            "Iteration 5038 - Batch 523/903 - Train loss: 2305841.7705152673\n",
            "Iteration 5039 - Batch 524/903 - Train loss: 2305355.835952381\n",
            "Iteration 5040 - Batch 525/903 - Train loss: 2304987.461739544\n",
            "Iteration 5041 - Batch 526/903 - Train loss: 2304803.2260436434\n",
            "Iteration 5042 - Batch 527/903 - Train loss: 2305021.2597064395\n",
            "Iteration 5043 - Batch 528/903 - Train loss: 2304937.504962193\n",
            "Iteration 5044 - Batch 529/903 - Train loss: 2304992.367688679\n",
            "Iteration 5045 - Batch 530/903 - Train loss: 2305397.713983051\n",
            "Iteration 5046 - Batch 531/903 - Train loss: 2305904.1346334587\n",
            "Iteration 5047 - Batch 532/903 - Train loss: 2306153.529315197\n",
            "Iteration 5048 - Batch 533/903 - Train loss: 2305681.3628277155\n",
            "Iteration 5049 - Batch 534/903 - Train loss: 2305629.946728972\n",
            "Iteration 5050 - Batch 535/903 - Train loss: 2305934.9668843285\n",
            "Iteration 5051 - Batch 536/903 - Train loss: 2306965.3845437616\n",
            "Iteration 5052 - Batch 537/903 - Train loss: 2308652.959107807\n",
            "Iteration 5053 - Batch 538/903 - Train loss: 2309657.279684601\n",
            "Iteration 5054 - Batch 539/903 - Train loss: 2309863.975\n",
            "Iteration 5055 - Batch 540/903 - Train loss: 2309815.0799445473\n",
            "Iteration 5056 - Batch 541/903 - Train loss: 2310405.0350553505\n",
            "Iteration 5057 - Batch 542/903 - Train loss: 2310732.2099447516\n",
            "Iteration 5058 - Batch 543/903 - Train loss: 2310642.5119485296\n",
            "Iteration 5059 - Batch 544/903 - Train loss: 2310716.780733945\n",
            "Iteration 5060 - Batch 545/903 - Train loss: 2310625.419871795\n",
            "Iteration 5061 - Batch 546/903 - Train loss: 2310606.7202925044\n",
            "Iteration 5062 - Batch 547/903 - Train loss: 2310627.34169708\n",
            "Iteration 5063 - Batch 548/903 - Train loss: 2310558.97996357\n",
            "Iteration 5064 - Batch 549/903 - Train loss: 2310432.0704545453\n",
            "Iteration 5065 - Batch 550/903 - Train loss: 2309919.578266788\n",
            "Iteration 5066 - Batch 551/903 - Train loss: 2310081.036911232\n",
            "Iteration 5067 - Batch 552/903 - Train loss: 2310644.544981917\n",
            "Iteration 5068 - Batch 553/903 - Train loss: 2311394.0945397113\n",
            "Iteration 5069 - Batch 554/903 - Train loss: 2311660.320045045\n",
            "Iteration 5070 - Batch 555/903 - Train loss: 2311749.2070593527\n",
            "Iteration 5071 - Batch 556/903 - Train loss: 2312485.51997307\n",
            "Iteration 5072 - Batch 557/903 - Train loss: 2313333.654345878\n",
            "Iteration 5073 - Batch 558/903 - Train loss: 2313332.3016547407\n",
            "Iteration 5074 - Batch 559/903 - Train loss: 2313284.266294643\n",
            "Iteration 5075 - Batch 560/903 - Train loss: 2313614.4685828877\n",
            "Iteration 5076 - Batch 561/903 - Train loss: 2314150.502001779\n",
            "Iteration 5077 - Batch 562/903 - Train loss: 2313978.0774866785\n",
            "Iteration 5078 - Batch 563/903 - Train loss: 2313956.3273492907\n",
            "Iteration 5079 - Batch 564/903 - Train loss: 2313602.3303097347\n",
            "Iteration 5080 - Batch 565/903 - Train loss: 2313313.838560071\n",
            "Iteration 5081 - Batch 566/903 - Train loss: 2313017.238315697\n",
            "Iteration 5082 - Batch 567/903 - Train loss: 2313105.641505282\n",
            "Iteration 5083 - Batch 568/903 - Train loss: 2313461.7655975395\n",
            "Iteration 5084 - Batch 569/903 - Train loss: 2313279.0923245614\n",
            "Iteration 5085 - Batch 570/903 - Train loss: 2313101.4708844135\n",
            "Iteration 5086 - Batch 571/903 - Train loss: 2313241.8677884615\n",
            "Iteration 5087 - Batch 572/903 - Train loss: 2313509.2436736473\n",
            "Iteration 5088 - Batch 573/903 - Train loss: 2313521.691419861\n",
            "Iteration 5089 - Batch 574/903 - Train loss: 2313579.8867391306\n",
            "Iteration 5090 - Batch 575/903 - Train loss: 2313744.1182725695\n",
            "Iteration 5091 - Batch 576/903 - Train loss: 2313794.954289428\n",
            "Iteration 5092 - Batch 577/903 - Train loss: 2313758.576773356\n",
            "Iteration 5093 - Batch 578/903 - Train loss: 2314180.058506045\n",
            "Iteration 5094 - Batch 579/903 - Train loss: 2314378.0842672414\n",
            "Iteration 5095 - Batch 580/903 - Train loss: 2313925.338425129\n",
            "Iteration 5096 - Batch 581/903 - Train loss: 2313994.1535652922\n",
            "Iteration 5097 - Batch 582/903 - Train loss: 2313911.9397512865\n",
            "Iteration 5098 - Batch 583/903 - Train loss: 2314083.370505137\n",
            "Iteration 5099 - Batch 584/903 - Train loss: 2313866.2138888887\n",
            "Iteration 5100 - Batch 585/903 - Train loss: 2313896.2566126278\n",
            "Iteration 5101 - Batch 586/903 - Train loss: 2314183.6364991483\n",
            "Iteration 5102 - Batch 587/903 - Train loss: 2313914.870535714\n",
            "Iteration 5103 - Batch 588/903 - Train loss: 2313629.176358234\n",
            "Iteration 5104 - Batch 589/903 - Train loss: 2313203.8377118646\n",
            "Iteration 5105 - Batch 590/903 - Train loss: 2313462.1450930624\n",
            "Iteration 5106 - Batch 591/903 - Train loss: 2313793.051097973\n",
            "Iteration 5107 - Batch 592/903 - Train loss: 2313428.4430860034\n",
            "Iteration 5108 - Batch 593/903 - Train loss: 2312987.57996633\n",
            "Iteration 5109 - Batch 594/903 - Train loss: 2312924.692016807\n",
            "Iteration 5110 - Batch 595/903 - Train loss: 2312758.3125\n",
            "Iteration 5111 - Batch 596/903 - Train loss: 2312167.0854271357\n",
            "Iteration 5112 - Batch 597/903 - Train loss: 2312120.4084448162\n",
            "Iteration 5113 - Batch 598/903 - Train loss: 2312162.810100167\n",
            "Iteration 5114 - Batch 599/903 - Train loss: 2312061.8358333334\n",
            "Iteration 5115 - Batch 600/903 - Train loss: 2311870.7308652247\n",
            "Iteration 5116 - Batch 601/903 - Train loss: 2312023.047757475\n",
            "Iteration 5117 - Batch 602/903 - Train loss: 2311973.220978441\n",
            "Iteration 5118 - Batch 603/903 - Train loss: 2311834.627897351\n",
            "Iteration 5119 - Batch 604/903 - Train loss: 2312054.6309917355\n",
            "Iteration 5120 - Batch 605/903 - Train loss: 2311753.959570957\n",
            "Iteration 5121 - Batch 606/903 - Train loss: 2311243.7971581547\n",
            "Iteration 5122 - Batch 607/903 - Train loss: 2311304.409745066\n",
            "Iteration 5123 - Batch 608/903 - Train loss: 2311117.1418308704\n",
            "Iteration 5124 - Batch 609/903 - Train loss: 2311025.480122951\n",
            "Iteration 5125 - Batch 610/903 - Train loss: 2310938.9744271687\n",
            "Iteration 5126 - Batch 611/903 - Train loss: 2310562.344362745\n",
            "Iteration 5127 - Batch 612/903 - Train loss: 2309948.4406606853\n",
            "Iteration 5128 - Batch 613/903 - Train loss: 2309920.204193811\n",
            "Iteration 5129 - Batch 614/903 - Train loss: 2309761.7815040653\n",
            "Iteration 5130 - Batch 615/903 - Train loss: 2309561.718547078\n",
            "Iteration 5131 - Batch 616/903 - Train loss: 2309667.1278363047\n",
            "Iteration 5132 - Batch 617/903 - Train loss: 2309443.0835355986\n",
            "Iteration 5133 - Batch 618/903 - Train loss: 2309225.0607835217\n",
            "Iteration 5134 - Batch 619/903 - Train loss: 2309017.1925403224\n",
            "Iteration 5135 - Batch 620/903 - Train loss: 2308644.8967391304\n",
            "Iteration 5136 - Batch 621/903 - Train loss: 2308593.589027331\n",
            "Iteration 5137 - Batch 622/903 - Train loss: 2308117.897672552\n",
            "Iteration 5138 - Batch 623/903 - Train loss: 2307660.2580128205\n",
            "Iteration 5139 - Batch 624/903 - Train loss: 2307102.7536\n",
            "Iteration 5140 - Batch 625/903 - Train loss: 2306643.989816294\n",
            "Iteration 5141 - Batch 626/903 - Train loss: 2306382.606658692\n",
            "Iteration 5142 - Batch 627/903 - Train loss: 2305928.794785032\n",
            "Iteration 5143 - Batch 628/903 - Train loss: 2306080.7279411764\n",
            "Iteration 5144 - Batch 629/903 - Train loss: 2306018.4648809526\n",
            "Iteration 5145 - Batch 630/903 - Train loss: 2305980.0929080825\n",
            "Iteration 5146 - Batch 631/903 - Train loss: 2305918.811511076\n",
            "Iteration 5147 - Batch 632/903 - Train loss: 2305549.350513428\n",
            "Iteration 5148 - Batch 633/903 - Train loss: 2305167.478509464\n",
            "Iteration 5149 - Batch 634/903 - Train loss: 2304975.9053149605\n",
            "Iteration 5150 - Batch 635/903 - Train loss: 2305029.008058176\n",
            "Iteration 5151 - Batch 636/903 - Train loss: 2304956.0551412874\n",
            "Iteration 5152 - Batch 637/903 - Train loss: 2305129.830133229\n",
            "Iteration 5153 - Batch 638/903 - Train loss: 2304654.2799295774\n",
            "Iteration 5154 - Batch 639/903 - Train loss: 2304475.5376953124\n",
            "Iteration 5155 - Batch 640/903 - Train loss: 2304174.8137675505\n",
            "Iteration 5156 - Batch 641/903 - Train loss: 2303575.495716511\n",
            "Iteration 5157 - Batch 642/903 - Train loss: 2303581.7663297043\n",
            "Iteration 5158 - Batch 643/903 - Train loss: 2303641.2406832296\n",
            "Iteration 5159 - Batch 644/903 - Train loss: 2303544.9639534885\n",
            "Iteration 5160 - Batch 645/903 - Train loss: 2302787.306114551\n",
            "Iteration 5161 - Batch 646/903 - Train loss: 2302720.9687017\n",
            "Iteration 5162 - Batch 647/903 - Train loss: 2302490.494212963\n",
            "Iteration 5163 - Batch 648/903 - Train loss: 2302285.7969953776\n",
            "Iteration 5164 - Batch 649/903 - Train loss: 2302128.2773076924\n",
            "Iteration 5165 - Batch 650/903 - Train loss: 2301849.8225806453\n",
            "Iteration 5166 - Batch 651/903 - Train loss: 2301713.098542945\n",
            "Iteration 5167 - Batch 652/903 - Train loss: 2301341.6889356812\n",
            "Iteration 5168 - Batch 653/903 - Train loss: 2301279.9619648317\n",
            "Iteration 5169 - Batch 654/903 - Train loss: 2301209.921183206\n",
            "Iteration 5170 - Batch 655/903 - Train loss: 2300921.063452744\n",
            "Iteration 5171 - Batch 656/903 - Train loss: 2300702.9381659054\n",
            "Iteration 5172 - Batch 657/903 - Train loss: 2300642.7205547113\n",
            "Iteration 5173 - Batch 658/903 - Train loss: 2300580.2209787555\n",
            "Iteration 5174 - Batch 659/903 - Train loss: 2300398.464583333\n",
            "Iteration 5175 - Batch 660/903 - Train loss: 2300281.6408850225\n",
            "Iteration 5176 - Batch 661/903 - Train loss: 2299949.174471299\n",
            "Iteration 5177 - Batch 662/903 - Train loss: 2299652.569004525\n",
            "Iteration 5178 - Batch 663/903 - Train loss: 2299259.6558734938\n",
            "Iteration 5179 - Batch 664/903 - Train loss: 2298807.827819549\n",
            "Iteration 5180 - Batch 665/903 - Train loss: 2298998.209834835\n",
            "Iteration 5181 - Batch 666/903 - Train loss: 2298787.2275112444\n",
            "Iteration 5182 - Batch 667/903 - Train loss: 2298617.2732035927\n",
            "Iteration 5183 - Batch 668/903 - Train loss: 2298283.9067638265\n",
            "Iteration 5184 - Batch 669/903 - Train loss: 2298069.1949626864\n",
            "Iteration 5185 - Batch 670/903 - Train loss: 2297484.5426602084\n",
            "Iteration 5186 - Batch 671/903 - Train loss: 2297128.379464286\n",
            "Iteration 5187 - Batch 672/903 - Train loss: 2296763.3863298665\n",
            "Iteration 5188 - Batch 673/903 - Train loss: 2296746.169139466\n",
            "Iteration 5189 - Batch 674/903 - Train loss: 2296278.7955555557\n",
            "Iteration 5190 - Batch 675/903 - Train loss: 2295982.509800296\n",
            "Iteration 5191 - Batch 676/903 - Train loss: 2295801.1039512553\n",
            "Iteration 5192 - Batch 677/903 - Train loss: 2295509.77710177\n",
            "Iteration 5193 - Batch 678/903 - Train loss: 2295277.311671576\n",
            "Iteration 5194 - Batch 679/903 - Train loss: 2295047.5193014704\n",
            "Iteration 5195 - Batch 680/903 - Train loss: 2295153.617657856\n",
            "Iteration 5196 - Batch 681/903 - Train loss: 2295214.767778592\n",
            "Iteration 5197 - Batch 682/903 - Train loss: 2295005.7161420207\n",
            "Iteration 5198 - Batch 683/903 - Train loss: 2295188.1229897663\n",
            "Iteration 5199 - Batch 684/903 - Train loss: 2294895.1\n",
            "Iteration 5200 - Batch 685/903 - Train loss: 2294978.336734694\n",
            "Iteration 5201 - Batch 686/903 - Train loss: 2295078.076419214\n",
            "Iteration 5202 - Batch 687/903 - Train loss: 2294821.258357558\n",
            "Iteration 5203 - Batch 688/903 - Train loss: 2294439.8343613935\n",
            "Iteration 5204 - Batch 689/903 - Train loss: 2294082.6596014495\n",
            "Iteration 5205 - Batch 690/903 - Train loss: 2293838.185781476\n",
            "Iteration 5206 - Batch 691/903 - Train loss: 2293687.0370303467\n",
            "Iteration 5207 - Batch 692/903 - Train loss: 2293550.8760822513\n",
            "Iteration 5208 - Batch 693/903 - Train loss: 2293902.9061599425\n",
            "Iteration 5209 - Batch 694/903 - Train loss: 2293684.0196043164\n",
            "Iteration 5210 - Batch 695/903 - Train loss: 2293469.536817529\n",
            "Iteration 5211 - Batch 696/903 - Train loss: 2293279.030308465\n",
            "Iteration 5212 - Batch 697/903 - Train loss: 2293325.2978151864\n",
            "Iteration 5213 - Batch 698/903 - Train loss: 2293741.874642346\n",
            "Iteration 5214 - Batch 699/903 - Train loss: 2294061.62375\n",
            "Iteration 5215 - Batch 700/903 - Train loss: 2294017.651747504\n",
            "Iteration 5216 - Batch 701/903 - Train loss: 2293549.4389245017\n",
            "Iteration 5217 - Batch 702/903 - Train loss: 2293396.0890825037\n",
            "Iteration 5218 - Batch 703/903 - Train loss: 2293921.799538352\n",
            "Iteration 5219 - Batch 704/903 - Train loss: 2294074.7760638297\n",
            "Iteration 5220 - Batch 705/903 - Train loss: 2294115.230701133\n",
            "Iteration 5221 - Batch 706/903 - Train loss: 2293729.3636845825\n",
            "Iteration 5222 - Batch 707/903 - Train loss: 2293579.2900776835\n",
            "Iteration 5223 - Batch 708/903 - Train loss: 2293744.2977785612\n",
            "Iteration 5224 - Batch 709/903 - Train loss: 2293739.577640845\n",
            "Iteration 5225 - Batch 710/903 - Train loss: 2293551.1380098453\n",
            "Iteration 5226 - Batch 711/903 - Train loss: 2293559.282127809\n",
            "Iteration 5227 - Batch 712/903 - Train loss: 2293528.5366409537\n",
            "Iteration 5228 - Batch 713/903 - Train loss: 2293815.9851190476\n",
            "Iteration 5229 - Batch 714/903 - Train loss: 2293992.0697552445\n",
            "Iteration 5230 - Batch 715/903 - Train loss: 2293588.778107542\n",
            "Iteration 5231 - Batch 716/903 - Train loss: 2293452.6888075313\n",
            "Iteration 5232 - Batch 717/903 - Train loss: 2293433.873781337\n",
            "Iteration 5233 - Batch 718/903 - Train loss: 2293345.2453059806\n",
            "Iteration 5234 - Batch 719/903 - Train loss: 2293271.9505208335\n",
            "Iteration 5235 - Batch 720/903 - Train loss: 2293221.4752080445\n",
            "Iteration 5236 - Batch 721/903 - Train loss: 2293019.442001385\n",
            "Iteration 5237 - Batch 722/903 - Train loss: 2292660.0522130015\n",
            "Iteration 5238 - Batch 723/903 - Train loss: 2292318.2486187844\n",
            "Iteration 5239 - Batch 724/903 - Train loss: 2292249.7834482756\n",
            "Iteration 5240 - Batch 725/903 - Train loss: 2292095.1649449035\n",
            "Iteration 5241 - Batch 726/903 - Train loss: 2291948.1715955986\n",
            "Iteration 5242 - Batch 727/903 - Train loss: 2291501.2850274723\n",
            "Iteration 5243 - Batch 728/903 - Train loss: 2291509.2709190673\n",
            "Iteration 5244 - Batch 729/903 - Train loss: 2291262.5852739727\n",
            "Iteration 5245 - Batch 730/903 - Train loss: 2291563.3348153215\n",
            "Iteration 5246 - Batch 731/903 - Train loss: 2291788.855191257\n",
            "Iteration 5247 - Batch 732/903 - Train loss: 2291991.2684174627\n",
            "Iteration 5248 - Batch 733/903 - Train loss: 2291822.745572207\n",
            "Iteration 5249 - Batch 734/903 - Train loss: 2291479.305782313\n",
            "Iteration 5250 - Batch 735/903 - Train loss: 2291382.555027174\n",
            "Iteration 5251 - Batch 736/903 - Train loss: 2291602.5301899593\n",
            "Iteration 5252 - Batch 737/903 - Train loss: 2291748.148712737\n",
            "Iteration 5253 - Batch 738/903 - Train loss: 2291892.111975643\n",
            "Iteration 5254 - Batch 739/903 - Train loss: 2291751.704391892\n",
            "Iteration 5255 - Batch 740/903 - Train loss: 2291555.5829959516\n",
            "Iteration 5256 - Batch 741/903 - Train loss: 2291169.2476415094\n",
            "Iteration 5257 - Batch 742/903 - Train loss: 2290942.438088829\n",
            "Iteration 5258 - Batch 743/903 - Train loss: 2290578.8126680106\n",
            "Iteration 5259 - Batch 744/903 - Train loss: 2290721.970637584\n",
            "Iteration 5260 - Batch 745/903 - Train loss: 2290632.7317359247\n",
            "Iteration 5261 - Batch 746/903 - Train loss: 2290573.486445783\n",
            "Iteration 5262 - Batch 747/903 - Train loss: 2290375.4657419785\n",
            "Iteration 5263 - Batch 748/903 - Train loss: 2290141.3833444593\n",
            "Iteration 5264 - Batch 749/903 - Train loss: 2289948.1585\n",
            "Iteration 5265 - Batch 750/903 - Train loss: 2289747.4732023966\n",
            "Iteration 5266 - Batch 751/903 - Train loss: 2289793.517453457\n",
            "Iteration 5267 - Batch 752/903 - Train loss: 2289643.5436586984\n",
            "Iteration 5268 - Batch 753/903 - Train loss: 2289306.327917772\n",
            "Iteration 5269 - Batch 754/903 - Train loss: 2289085.4314569538\n",
            "Iteration 5270 - Batch 755/903 - Train loss: 2288770.5310846562\n",
            "Iteration 5271 - Batch 756/903 - Train loss: 2288366.992239102\n",
            "Iteration 5272 - Batch 757/903 - Train loss: 2288299.5380936675\n",
            "Iteration 5273 - Batch 758/903 - Train loss: 2287985.424077734\n",
            "Iteration 5274 - Batch 759/903 - Train loss: 2287988.489638158\n",
            "Iteration 5275 - Batch 760/903 - Train loss: 2287769.6979303546\n",
            "Iteration 5276 - Batch 761/903 - Train loss: 2287384.927493438\n",
            "Iteration 5277 - Batch 762/903 - Train loss: 2287182.9226736566\n",
            "Iteration 5278 - Batch 763/903 - Train loss: 2287386.892342932\n",
            "Iteration 5279 - Batch 764/903 - Train loss: 2286949.930882353\n",
            "Iteration 5280 - Batch 765/903 - Train loss: 2286758.375489556\n",
            "Iteration 5281 - Batch 766/903 - Train loss: 2286756.719198175\n",
            "Iteration 5282 - Batch 767/903 - Train loss: 2286522.8374023438\n",
            "Iteration 5283 - Batch 768/903 - Train loss: 2286500.7885240573\n",
            "Iteration 5284 - Batch 769/903 - Train loss: 2286493.8670454547\n",
            "Iteration 5285 - Batch 770/903 - Train loss: 2286291.904831388\n",
            "Iteration 5286 - Batch 771/903 - Train loss: 2286031.5488989637\n",
            "Iteration 5287 - Batch 772/903 - Train loss: 2285725.3500970243\n",
            "Iteration 5288 - Batch 773/903 - Train loss: 2285686.196543928\n",
            "Iteration 5289 - Batch 774/903 - Train loss: 2285434.7777419356\n",
            "Iteration 5290 - Batch 775/903 - Train loss: 2285225.312177835\n",
            "Iteration 5291 - Batch 776/903 - Train loss: 2284737.8046975546\n",
            "Iteration 5292 - Batch 777/903 - Train loss: 2284327.1230719793\n",
            "Iteration 5293 - Batch 778/903 - Train loss: 2284219.685494223\n",
            "Iteration 5294 - Batch 779/903 - Train loss: 2284386.63525641\n",
            "Iteration 5295 - Batch 780/903 - Train loss: 2284268.5038412292\n",
            "Iteration 5296 - Batch 781/903 - Train loss: 2284032.594948849\n",
            "Iteration 5297 - Batch 782/903 - Train loss: 2283733.86302682\n",
            "Iteration 5298 - Batch 783/903 - Train loss: 2283412.383290816\n",
            "Iteration 5299 - Batch 784/903 - Train loss: 2283059.188375796\n",
            "Iteration 5300 - Batch 785/903 - Train loss: 2282831.2189885494\n",
            "Iteration 5301 - Batch 786/903 - Train loss: 2282895.963310038\n",
            "Iteration 5302 - Batch 787/903 - Train loss: 2282592.4141814723\n",
            "Iteration 5303 - Batch 788/903 - Train loss: 2282324.10234474\n",
            "Iteration 5304 - Batch 789/903 - Train loss: 2282002.0338607593\n",
            "Iteration 5305 - Batch 790/903 - Train loss: 2281821.4854614413\n",
            "Iteration 5306 - Batch 791/903 - Train loss: 2281632.0293560605\n",
            "Iteration 5307 - Batch 792/903 - Train loss: 2281184.0926860026\n",
            "Iteration 5308 - Batch 793/903 - Train loss: 2280962.9108942067\n",
            "Iteration 5309 - Batch 794/903 - Train loss: 2280688.1157232705\n",
            "Iteration 5310 - Batch 795/903 - Train loss: 2280587.2412060304\n",
            "Iteration 5311 - Batch 796/903 - Train loss: 2280355.6008469258\n",
            "Iteration 5312 - Batch 797/903 - Train loss: 2280159.25485589\n",
            "Iteration 5313 - Batch 798/903 - Train loss: 2279848.4658948686\n",
            "Iteration 5314 - Batch 799/903 - Train loss: 2279435.52328125\n",
            "Iteration 5315 - Batch 800/903 - Train loss: 2279350.6871098625\n",
            "Iteration 5316 - Batch 801/903 - Train loss: 2279004.399937656\n",
            "Iteration 5317 - Batch 802/903 - Train loss: 2278684.082347447\n",
            "Iteration 5318 - Batch 803/903 - Train loss: 2278647.67335199\n",
            "Iteration 5319 - Batch 804/903 - Train loss: 2278447.429037267\n",
            "Iteration 5320 - Batch 805/903 - Train loss: 2278224.9297456574\n",
            "Iteration 5321 - Batch 806/903 - Train loss: 2278258.1906753406\n",
            "Iteration 5322 - Batch 807/903 - Train loss: 2278123.8309096536\n",
            "Iteration 5323 - Batch 808/903 - Train loss: 2278035.423207664\n",
            "Iteration 5324 - Batch 809/903 - Train loss: 2278175.133179012\n",
            "Iteration 5325 - Batch 810/903 - Train loss: 2278506.8503390877\n",
            "Iteration 5326 - Batch 811/903 - Train loss: 2278720.848060345\n",
            "Iteration 5327 - Batch 812/903 - Train loss: 2279217.907287823\n",
            "Iteration 5328 - Batch 813/903 - Train loss: 2279678.126382064\n",
            "Iteration 5329 - Batch 814/903 - Train loss: 2279782.3394171777\n",
            "Iteration 5330 - Batch 815/903 - Train loss: 2279656.6772365198\n",
            "Iteration 5331 - Batch 816/903 - Train loss: 2279792.983017136\n",
            "Iteration 5332 - Batch 817/903 - Train loss: 2279846.8724022005\n",
            "Iteration 5333 - Batch 818/903 - Train loss: 2280087.120115995\n",
            "Iteration 5334 - Batch 819/903 - Train loss: 2280124.0407012193\n",
            "Iteration 5335 - Batch 820/903 - Train loss: 2280401.8521619975\n",
            "Iteration 5336 - Batch 821/903 - Train loss: 2280538.1023418494\n",
            "Iteration 5337 - Batch 822/903 - Train loss: 2280678.8506986634\n",
            "Iteration 5338 - Batch 823/903 - Train loss: 2280719.6488167476\n",
            "Iteration 5339 - Batch 824/903 - Train loss: 2280719.2046969696\n",
            "Iteration 5340 - Batch 825/903 - Train loss: 2280501.021035109\n",
            "Iteration 5341 - Batch 826/903 - Train loss: 2280757.5451934705\n",
            "Iteration 5342 - Batch 827/903 - Train loss: 2281129.162892512\n",
            "Iteration 5343 - Batch 828/903 - Train loss: 2281066.0517189386\n",
            "Iteration 5344 - Batch 829/903 - Train loss: 2281261.590813253\n",
            "Iteration 5345 - Batch 830/903 - Train loss: 2281009.182009627\n",
            "Iteration 5346 - Batch 831/903 - Train loss: 2281065.1283052885\n",
            "Iteration 5347 - Batch 832/903 - Train loss: 2281278.6353541417\n",
            "Iteration 5348 - Batch 833/903 - Train loss: 2281459.9355515586\n",
            "Iteration 5349 - Batch 834/903 - Train loss: 2281637.3110778444\n",
            "Iteration 5350 - Batch 835/903 - Train loss: 2281646.355861244\n",
            "Iteration 5351 - Batch 836/903 - Train loss: 2281658.123655914\n",
            "Iteration 5352 - Batch 837/903 - Train loss: 2281419.3781324583\n",
            "Iteration 5353 - Batch 838/903 - Train loss: 2281230.9390643626\n",
            "Iteration 5354 - Batch 839/903 - Train loss: 2281471.5858630952\n",
            "Iteration 5355 - Batch 840/903 - Train loss: 2281422.7019916764\n",
            "Iteration 5356 - Batch 841/903 - Train loss: 2281216.1394002377\n",
            "Iteration 5357 - Batch 842/903 - Train loss: 2281160.4390569394\n",
            "Iteration 5358 - Batch 843/903 - Train loss: 2281348.7975414693\n",
            "Iteration 5359 - Batch 844/903 - Train loss: 2281161.582988166\n",
            "Iteration 5360 - Batch 845/903 - Train loss: 2281069.167996454\n",
            "Iteration 5361 - Batch 846/903 - Train loss: 2280727.538961039\n",
            "Iteration 5362 - Batch 847/903 - Train loss: 2280325.8265035376\n",
            "Iteration 5363 - Batch 848/903 - Train loss: 2280225.680359246\n",
            "Iteration 5364 - Batch 849/903 - Train loss: 2280343.4804411763\n",
            "Iteration 5365 - Batch 850/903 - Train loss: 2280384.0060223266\n",
            "Iteration 5366 - Batch 851/903 - Train loss: 2280358.144806338\n",
            "Iteration 5367 - Batch 852/903 - Train loss: 2280096.008059789\n",
            "Iteration 5368 - Batch 853/903 - Train loss: 2279979.4975117096\n",
            "Iteration 5369 - Batch 854/903 - Train loss: 2279559.476608187\n",
            "Iteration 5370 - Batch 855/903 - Train loss: 2279412.275116822\n",
            "Iteration 5371 - Batch 856/903 - Train loss: 2279043.4797257874\n",
            "Iteration 5372 - Batch 857/903 - Train loss: 2278987.3547494174\n",
            "Iteration 5373 - Batch 858/903 - Train loss: 2278909.869470314\n",
            "Iteration 5374 - Batch 859/903 - Train loss: 2278729.7780523254\n",
            "Iteration 5375 - Batch 860/903 - Train loss: 2278477.675667828\n",
            "Iteration 5376 - Batch 861/903 - Train loss: 2278227.7288283063\n",
            "Iteration 5377 - Batch 862/903 - Train loss: 2278151.537949015\n",
            "Iteration 5378 - Batch 863/903 - Train loss: 2278013.3020833335\n",
            "Iteration 5379 - Batch 864/903 - Train loss: 2277755.266618497\n",
            "Iteration 5380 - Batch 865/903 - Train loss: 2277745.5437355656\n",
            "Iteration 5381 - Batch 866/903 - Train loss: 2277615.112312572\n",
            "Iteration 5382 - Batch 867/903 - Train loss: 2277410.184187788\n",
            "Iteration 5383 - Batch 868/903 - Train loss: 2277469.4932393557\n",
            "Iteration 5384 - Batch 869/903 - Train loss: 2277371.390948276\n",
            "Iteration 5385 - Batch 870/903 - Train loss: 2277275.9085820895\n",
            "Iteration 5386 - Batch 871/903 - Train loss: 2277224.2406823393\n",
            "Iteration 5387 - Batch 872/903 - Train loss: 2277033.5634306986\n",
            "Iteration 5388 - Batch 873/903 - Train loss: 2276812.082522883\n",
            "Iteration 5389 - Batch 874/903 - Train loss: 2276768.9775714288\n",
            "Iteration 5390 - Batch 875/903 - Train loss: 2277026.1202910957\n",
            "Iteration 5391 - Batch 876/903 - Train loss: 2277584.594213227\n",
            "Iteration 5392 - Batch 877/903 - Train loss: 2277908.4150056946\n",
            "Iteration 5393 - Batch 878/903 - Train loss: 2278510.2276734924\n",
            "Iteration 5394 - Batch 879/903 - Train loss: 2278560.1126420456\n",
            "Iteration 5395 - Batch 880/903 - Train loss: 2278324.525113507\n",
            "Iteration 5396 - Batch 881/903 - Train loss: 2278624.969812925\n",
            "Iteration 5397 - Batch 882/903 - Train loss: 2279251.829416761\n",
            "Iteration 5398 - Batch 883/903 - Train loss: 2279756.35675905\n",
            "Iteration 5399 - Batch 884/903 - Train loss: 2280029.109745763\n",
            "Iteration 5400 - Batch 885/903 - Train loss: 2280314.6273984197\n",
            "Iteration 5401 - Batch 886/903 - Train loss: 2280047.280439684\n",
            "Iteration 5402 - Batch 887/903 - Train loss: 2280247.248873874\n",
            "Iteration 5403 - Batch 888/903 - Train loss: 2280370.327896513\n",
            "Iteration 5404 - Batch 889/903 - Train loss: 2280266.224438202\n",
            "Iteration 5405 - Batch 890/903 - Train loss: 2280418.9034792366\n",
            "Iteration 5406 - Batch 891/903 - Train loss: 2280618.168441704\n",
            "Iteration 5407 - Batch 892/903 - Train loss: 2280848.422732363\n",
            "Iteration 5408 - Batch 893/903 - Train loss: 2280732.177852349\n",
            "Iteration 5409 - Batch 894/903 - Train loss: 2280795.2452513967\n",
            "Iteration 5410 - Batch 895/903 - Train loss: 2280988.1174665177\n",
            "Iteration 5411 - Batch 896/903 - Train loss: 2280870.304347826\n",
            "Iteration 5412 - Batch 897/903 - Train loss: 2280974.6628619153\n",
            "Iteration 5413 - Batch 898/903 - Train loss: 2281481.6565628476\n",
            "Iteration 5414 - Batch 899/903 - Train loss: 2281601.569166667\n",
            "Iteration 5415 - Batch 900/903 - Train loss: 2281548.5496670366\n",
            "Iteration 5416 - Batch 901/903 - Train loss: 2281573.5249445676\n",
            "Iteration 5417 - Batch 902/903 - Train loss: 2279649.897217608\n",
            "Val loss: 2058055.4375\n",
            "\n",
            "Tiempo total de entrenamiento: 10758.8254 [s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 936x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "epochs = 6\n",
        "criterion = DecoderLoss()\n",
        "batch_size = 45\n",
        "lr = 1 * 10**(-3)\n",
        "use_gpu = True\n",
        "\n",
        "train_model(\n",
        "    model = Decode,\n",
        "    train_dataset = Data, val_dataset = Val,\n",
        "    epochs = epochs, \n",
        "    criterion = criterion,\n",
        "    batch_size = batch_size, lr = lr,\n",
        "    use_gpu = use_gpu,\n",
        "    n_evaluations_per_epoch = 500, #Cada cuantos batches te muestra el progreso de loss de entrenamiento\n",
        "    name='Deco1FULL',               #Nombre de como queremos que se guarde el modelo resultante de cada época\n",
        "    Losses = 'Loss1L'               #Nombre de los outputs de loss por época\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "52d5eea5c96ee1a51b9158777603f18b7256b812db0971735d706c20e472b1b0"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a62291f408204382a5a902c9ae2d266b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_911e526ca929470bbd49cf471ae1ddc6",
              "IPY_MODEL_10a27d542d13425c8a3141ccf9a65210",
              "IPY_MODEL_02991449ce9b48428a3d19f550da96c9"
            ],
            "layout": "IPY_MODEL_ccb0f3ea7c1c46fea228a7ccda5dba43"
          }
        },
        "911e526ca929470bbd49cf471ae1ddc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c3427fb9b5843ef8233cf056ba9860a",
            "placeholder": "​",
            "style": "IPY_MODEL_a550587140d24fe39a6aa6581bdd3ea8",
            "value": "100%"
          }
        },
        "10a27d542d13425c8a3141ccf9a65210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec1c982aa9dc49ffac2ed79d26b898e6",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_645904cd74304435bfc797c67ac572c5",
            "value": 553433881
          }
        },
        "02991449ce9b48428a3d19f550da96c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e43c28f81d4c416ca318b81f20de7266",
            "placeholder": "​",
            "style": "IPY_MODEL_c7f5b073fdee49b3abd6049ded7dc1c7",
            "value": " 528M/528M [00:51&lt;00:00, 16.6MB/s]"
          }
        },
        "ccb0f3ea7c1c46fea228a7ccda5dba43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c3427fb9b5843ef8233cf056ba9860a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a550587140d24fe39a6aa6581bdd3ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec1c982aa9dc49ffac2ed79d26b898e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "645904cd74304435bfc797c67ac572c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e43c28f81d4c416ca318b81f20de7266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f5b073fdee49b3abd6049ded7dc1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}